# Standard library imports
from foundational_ssm.utils import h5_to_dict 
import os
import multiprocessing as mp
import logging
import hydra
from dotenv import load_dotenv

# Third-party imports
import numpy as np
import wandb
from jax import random as jr
import equinox as eqx

# Foundational SSM imports
from omegaconf import OmegaConf

from foundational_ssm.loaders import get_brainset_train_val_loaders

from foundational_ssm.utils.downstream_utils import (
    mse_loss_downstream,
    train_one_epoch_brainsets,
    validate_one_epoch_brainsets,
    load_training_state
)

from foundational_ssm.utils.wandb_utils_jax import (
    save_checkpoint_wandb,
    add_alias_to_checkpoint,
    count_parameters
)
import multiprocessing as mp

load_dotenv()
logger = logging.getLogger(__name__)

@hydra.main(config_path="../../configs", config_name="pm_transfer", version_base="1.3")
def main(cfg: OmegaConf):
    mp.set_start_method("spawn", force=True)
    logging.basicConfig(filename='downstream_decoding_rtt.log', level=logging.INFO)
    
    best_r2_score = 0
    
    key, train_key, val_key = jr.split(jr.PRNGKey(cfg.rng_seed), 3)
    prepend_history = cfg.prepend_history
    skip_timesteps = int(prepend_history * cfg.sampling_rate)
    _, train_loader, _, val_loader, max_neural_units = get_brainset_train_val_loaders(
        cfg.dataset_args,
        cfg.train_loader,
        cfg.val_loader,
        prepend_history
    )
    if hasattr(cfg.dataset_args,'recording_id'):
        run_postfix = '_'+cfg.dataset_args.recording_id.split('/')[1]
    cfg, model, state, opt, opt_state, lr_scheduler = load_training_state(cfg, max_neural_units, run_postfix=run_postfix)

    model_num_params = count_parameters(model)
    wandb.log({"model/num_params": model_num_params}, step=0)
    
    current_step = 0
    epoch_loss = 1e10
    for epoch in range(0, cfg.training.epochs):
        if cfg.training.save_checkpoints and epoch % cfg.training.checkpoint_every == 0:
            metadata = {
                'train_loss': epoch_loss,
                'epoch': epoch,
                'step': current_step
            }
            checkpoint_artifact = save_checkpoint_wandb(model, state, opt_state, metadata)

        if epoch % cfg.training.log_val_every == 0:
            if 'checkpoint_artifact' in locals():
                add_alias_to_checkpoint(checkpoint_artifact, f'epoch_{epoch}')

            logger.info(f"Running validation for epoch {epoch}")
            metrics = validate_one_epoch_brainsets(val_loader, model, state, skip_timesteps)
            metrics['epoch'] = epoch
            wandb.log(metrics, step=current_step)
            current_r2 = metrics.get(f'val/r2', 0.0)
            
            if current_r2 > best_r2_score:
                best_r2_score = current_r2
                logger.info(f"New best RÂ² score: {best_r2_score:.4f} at epoch {epoch}")
                if 'checkpoint_artifact' in locals():
                    add_alias_to_checkpoint(checkpoint_artifact, 'best', metrics)
                    
        train_key, subkey = jr.split(train_key)
        model, state, opt_state, current_step, epoch_loss = train_one_epoch_brainsets(
            train_loader, model, state, mse_loss_downstream, opt, opt_state, subkey, lr_scheduler, current_step, epoch, skip_timesteps
        )

    wandb.log({f"final/r2/{cfg.dataset_args.recording_id.split('/')[-1]}/mean": best_r2_score})
    wandb.finish()


if __name__ == "__main__":
    main()