wandb:
  project: foundational_ssm_pretrain_decoding
  run_prefix: train_batch-1024   
  tags: [neural, behavior, masking]
  log_freq: 1

train_dataset:
  name: perich_miller_population_2018
  subjects: [c,m,t,j]
  batch_size: 1024
  root: /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/
  lazy: true

val_dataset:
  name: perich_miller_population_2018
  subjects: [c,m,t,j]
  batch_size: 1024
  root: /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/


model:
  ssm_io_dim: 128
  ssm_dim: 128
  ssm_init_diag_blocks: 4
  ssm_num_layers: 1
  output_dim: 2
  rng_seed: 42

optimizer:
  lr: 0.001
  weight_decay: 0.001
  use_cosine_scheduler: false
  min_lr: 0.00001  # Minimum learning rate for cosine annealing
  warmup_steps: 100  # Number of warmup steps (0 for no warmup)

training:
  mask_prob: 0.5
  epochs: 1000
  neural_weight: 10.0
  behavior_weight: 1.0
  validate_every: 20
  freeze_ssm: false
  freeze_mlp: false
  num_workers: 16


rng_seed: 42

device: cuda

