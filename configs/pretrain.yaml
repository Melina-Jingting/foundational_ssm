wandb:
  project: foundational_ssm_pretrain_decoding
  run_prefix: inverse_variance  
  tags: [neural, behavior, masking]
  entity: melinajingting-ucl
  resume_run_id: null #cr6zuzfw (l1-d128)

activations_dataset_config: /cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/activations.yaml

train_loader:
  dataset_args:
    config: /cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/pretrain_train_and_val.yaml
    keep_files_open: false
    lazy: true
    split: train
  sampler: RandomFixedWindowSampler
  sampler_args:
    drop_short: true
    window_length: 1
  dataloader_args:
    batch_size: 512
    num_workers: 12
    persistent_workers: true
  window_length: 1
  sampling_rate: 200

val_loader:
  dataset_args:
    config: /cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/pretrain_train_and_val.yaml
    keep_files_open: false
    lazy: true
    split: val
  sampler: TrialSampler
  sampler_args: null
  dataloader_args:
    batch_size: 512
    num_workers: 0
    persistent_workers: false
  window_length: 6
  sampling_rate: 200


model:
  ssm_io_dim: 128
  ssm_dim: 128
  ssm_init_diag_blocks: 4
  ssm_num_layers: 2
  output_dim: 2
  rng_seed: 42

optimizer:
  lr: 0.001
  weight_decay: 0.001
  use_cosine_scheduler: false
  min_lr: 0.00001  # Minimum learning rate for cosine annealing
  warmup_steps: 100  # Number of warmup steps (0 for no warmup)

training:
  epochs: 1000
  checkpoint_every: 1
  log_val_every: 50
  log_pred_and_activations_every: 50

filter_spec:
  freeze_ssm: false
  freeze_mlp: false

rng_seed: 42

device: cuda

