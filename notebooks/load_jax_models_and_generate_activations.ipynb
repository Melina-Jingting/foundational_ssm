{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed3b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import equinox as eqx\n",
    "import os \n",
    "\n",
    "# Foundational SSM imports\n",
    "from omegaconf import OmegaConf\n",
    "import tempfile \n",
    "from foundational_ssm.models import SSMDownstreamDecoder, SSMFoundationalDecoder\n",
    "from foundational_ssm.utils import h5_to_dict\n",
    "from foundational_ssm.transform import smooth_spikes\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import Any, BinaryIO\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def default_deserialise_filter_spec(f: BinaryIO, x: Any) -> Any:\n",
    "    \"\"\"Default filter specification for deserialising saved data.\n",
    "\n",
    "    **Arguments**\n",
    "\n",
    "    -   `f`: file-like object\n",
    "    -   `x`: The leaf for which the data needs to be loaded.\n",
    "\n",
    "    **Returns**\n",
    "\n",
    "    The new value for datatype `x`.\n",
    "\n",
    "    !!! info\n",
    "\n",
    "        This function can be extended to customise the deserialisation behaviour for\n",
    "        leaves.\n",
    "\n",
    "    !!! example\n",
    "\n",
    "        Skipping loading of jax.Array.\n",
    "\n",
    "        ```python\n",
    "        import jax.numpy as jnp\n",
    "        import equinox as eqx\n",
    "\n",
    "        tree = (jnp.array([4,5,6]), [1,2,3])\n",
    "        new_filter_spec = lambda f,x: (\n",
    "            x if isinstance(x, jax.Array) else eqx.default_deserialise_filter_spec(f, x)\n",
    "        )\n",
    "        new_tree = eqx.tree_deserialise_leaves(\"some_filename.eqx\", tree, filter_spec=new_filter_spec)\n",
    "        ```\n",
    "    \"\"\"  # noqa: E501\n",
    "    try:\n",
    "        if isinstance(x, (jax.Array, jax.ShapeDtypeStruct)):\n",
    "            return jnp.load(f)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            # Important to use `np` here to avoid promoting NumPy arrays to JAX.\n",
    "            return np.load(f)\n",
    "        elif eqx.is_array_like(x):\n",
    "            # np.generic gets deserialised directly as an array, so convert back to a scalar\n",
    "            # type here.\n",
    "            # See also https://github.com/google/jax/issues/17858\n",
    "            out = np.load(f)\n",
    "            if isinstance(x, jax.dtypes.bfloat16):\n",
    "                out = out.view(jax.dtypes.bfloat16)\n",
    "            if np.size(out) == 1:\n",
    "                return type(x)(out.item())\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        print(\"Failed to load data for leaf with shape/ value:\", x.shape if hasattr(x, 'shape') else x)\n",
    "        return x \n",
    "\n",
    "def load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMDownstreamDecoder, model_cfg=None):\n",
    "    \"\"\"Load model, optimizer state, epoch, and step from a checkpoint file.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        artifact = api.artifact(artifact_full_name, type=\"checkpoint\")\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Could not find checkpoint artifact: {artifact_full_name}\")\n",
    "    \n",
    "    if model_cfg is None:\n",
    "        run = artifact.logged_by()\n",
    "        run_cfg = OmegaConf.create(run.config)\n",
    "        print(run_cfg)\n",
    "        model_cfg = OmegaConf.create(run_cfg.model)\n",
    "    \n",
    "    model_template, state_template = eqx.nn.make_with_state(model_cls)(\n",
    "        **model_cfg\n",
    "    )\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        artifact.download(temp_dir)\n",
    "        model = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"model.ckpt\"), model_template, default_deserialise_filter_spec)\n",
    "        state = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"state.ckpt\"), state_template, default_deserialise_filter_spec)\n",
    "\n",
    "    meta = artifact.metadata\n",
    "    return model, state, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ab55e",
   "metadata": {},
   "source": [
    "# Loading the Downstream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15f7799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 32, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.03, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1500, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "layer = \"2\"\n",
    "pretrain_mode = \"scratch\"\n",
    "train_mode = \"all\"\n",
    "alias = \"best\" # can be latest/best/ epoch_{any value in range(0,1000,100)}\n",
    "# epoch 0 now stores a fresh model.\n",
    "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_rtt/l{layer}_{pretrain_mode}_{train_mode}_checkpoint:{alias}\"\n",
    "model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c1b41",
   "metadata": {},
   "source": [
    "# Calling with activations (Downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c66262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
    "# Edit dataset_dir to your directory\n",
    "dataset_dir = \"../../data/foundational_ssm/processed/nlb\" \n",
    "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
    "data = h5_to_dict(dataset_path)\n",
    "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
    "input = data[\"neural_input\"]\n",
    "target_vel = data[\"behavior_input\"]\n",
    "\n",
    "# Specify the layers you want to generate the activations of. \n",
    "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
    "layer_keys = [\"ssm_pre_activation\"] \n",
    "inf_model = eqx.nn.inference_mode(model) # Switches off dropout\n",
    "pred_vel, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state, layer_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ae424",
   "metadata": {},
   "source": [
    "# Example: Plotting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from foundational_ssm.plotting import aggregate_bin_label_results, plot_pred_vs_targets_by_angle_bin\n",
    "\n",
    "\n",
    "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
    "# Edit dataset_dir to your directory\n",
    "dataset_dir = \"../../data/foundational_ssm/processed/nlb\" \n",
    "trial_info = pd.read_csv(os.path.join(dataset_dir, \"mc_rtt_trialized.csv\"))\n",
    "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
    "data = h5_to_dict(dataset_path)\n",
    "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
    "input = data[\"neural_input\"]\n",
    "target_vel = data[\"behavior_input\"]\n",
    "\n",
    "# Specify the layers you want to generate the activations of. \n",
    "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
    "layer_keys = [\"ssm_pre_activation\"] \n",
    "inf_model = eqx.nn.inference_mode(model) # Switches off dropout\n",
    "pred_vel, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state, layer_keys)\n",
    "\n",
    "results_df = aggregate_bin_label_results(trial_info, target_vel, pred_vel)\n",
    "fig = plot_pred_vs_targets_by_angle_bin(results_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e3254",
   "metadata": {},
   "source": [
    "# Loading the Foundational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb84781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': True, 'window_length': 3.279}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': True, 'window_length': 3.279}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"l2\"\n",
    "dataset = \"reaching\"\n",
    "alias = \"best\"\n",
    "\n",
    "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_pretrain/{model}_{dataset}_checkpoint:{alias}\"\n",
    "model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMFoundationalDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97ddf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# Foundational SSM core imports\n",
    "from foundational_ssm.loaders import get_brainset_data_loader\n",
    "from foundational_ssm.constants import DATA_ROOT\n",
    "from foundational_ssm.samplers import TrialSampler\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2357034",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_brainset_data_loader() missing 1 required positional argument: 'sampling_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m dataset_args = {\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mkeep_files_open\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlaze\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m }\n\u001b[32m      9\u001b[39m dataloader_args = {\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m128\u001b[39m, \u001b[38;5;66;03m# Adjust per your system capacity\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_workers\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m4\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpersistent_workers\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     13\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m trial_dataset, trial_loader = \u001b[43mget_brainset_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrialSampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreaching_analysis.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_root\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m sessions = trial_dataset.get_session_ids() \u001b[38;5;66;03m# list of sessions in your dataset\u001b[39;00m\n\u001b[32m     25\u001b[39m sampling_intervals = trial_dataset.get_sampling_intervals() \u001b[38;5;66;03m# list of sampling intervals for each session\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: get_brainset_data_loader() missing 1 required positional argument: 'sampling_rate'"
     ]
    }
   ],
   "source": [
    "mp.set_start_method(\"spawn\", force=True) # otherwise causes deadlock on jax.\n",
    "\n",
    "data_root = '../' + DATA_ROOT # change to the folder holding the brainsets\n",
    "config_dir = '../config' # change\n",
    "dataset_args = {\n",
    "    'keep_files_open': False,\n",
    "    'laze': True\n",
    "}\n",
    "dataloader_args = {\n",
    "    'batch_size': 128, # Adjust per your system capacity\n",
    "    'num_workers': 4,\n",
    "    'persistent_workers': False\n",
    "}\n",
    "\n",
    "trial_dataset, trial_loader = get_brainset_data_loader(\n",
    "    dataset_args=dataset_args,\n",
    "    sampler = TrialSampler,\n",
    "    sampler_args = {},\n",
    "    dataloader_args = dataloader_args,\n",
    "    sampling_rate = 200,\n",
    "    dataset_cfg = os.path.join(config_dir, 'reaching_analysis.yaml'),\n",
    "    data_root = data_root\n",
    ")\n",
    "\n",
    "sessions = trial_dataset.get_session_ids() # list of sessions in your dataset\n",
    "sampling_intervals = trial_dataset.get_sampling_intervals() # list of sampling intervals for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundational_ssm.utils.pretrain_utils import validate_one_epoch \n",
    "\n",
    "metrics = validate_one_epoch(\n",
    "    trial_loader, model, state, skip_timesteps=56 # only when computing R2, we would keep this for analysis\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
