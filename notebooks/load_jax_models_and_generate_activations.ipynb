{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed3b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import equinox as eqx\n",
    "import os \n",
    "\n",
    "# Foundational SSM imports\n",
    "from omegaconf import OmegaConf\n",
    "import tempfile \n",
    "from foundational_ssm.models import SSMDownstreamDecoder, SSMFoundationalDecoder\n",
    "from foundational_ssm.utils import h5_to_dict\n",
    "from foundational_ssm.transform import smooth_spikes\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import Any, BinaryIO\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def default_deserialise_filter_spec(f: BinaryIO, x: Any) -> Any:\n",
    "    \"\"\"Default filter specification for deserialising saved data.\n",
    "\n",
    "    **Arguments**\n",
    "\n",
    "    -   `f`: file-like object\n",
    "    -   `x`: The leaf for which the data needs to be loaded.\n",
    "\n",
    "    **Returns**\n",
    "\n",
    "    The new value for datatype `x`.\n",
    "\n",
    "    !!! info\n",
    "\n",
    "        This function can be extended to customise the deserialisation behaviour for\n",
    "        leaves.\n",
    "\n",
    "    !!! example\n",
    "\n",
    "        Skipping loading of jax.Array.\n",
    "\n",
    "        ```python\n",
    "        import jax.numpy as jnp\n",
    "        import equinox as eqx\n",
    "\n",
    "        tree = (jnp.array([4,5,6]), [1,2,3])\n",
    "        new_filter_spec = lambda f,x: (\n",
    "            x if isinstance(x, jax.Array) else eqx.default_deserialise_filter_spec(f, x)\n",
    "        )\n",
    "        new_tree = eqx.tree_deserialise_leaves(\"some_filename.eqx\", tree, filter_spec=new_filter_spec)\n",
    "        ```\n",
    "    \"\"\"  # noqa: E501\n",
    "    try:\n",
    "        if isinstance(x, (jax.Array, jax.ShapeDtypeStruct)):\n",
    "            return jnp.load(f)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            # Important to use `np` here to avoid promoting NumPy arrays to JAX.\n",
    "            return np.load(f)\n",
    "        elif eqx.is_array_like(x):\n",
    "            # np.generic gets deserialised directly as an array, so convert back to a scalar\n",
    "            # type here.\n",
    "            # See also https://github.com/google/jax/issues/17858\n",
    "            out = np.load(f)\n",
    "            if isinstance(x, jax.dtypes.bfloat16):\n",
    "                out = out.view(jax.dtypes.bfloat16)\n",
    "            if np.size(out) == 1:\n",
    "                return type(x)(out.item())\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        print(\"Failed to load data for leaf with shape/ value:\", x.shape if hasattr(x, 'shape') else x)\n",
    "        return x \n",
    "\n",
    "def load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMDownstreamDecoder, model_cfg=None):\n",
    "    \"\"\"Load model, optimizer state, epoch, and step from a checkpoint file.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        artifact = api.artifact(artifact_full_name, type=\"checkpoint\")\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Could not find checkpoint artifact: {artifact_full_name}\")\n",
    "    \n",
    "    if model_cfg is None:\n",
    "        run = artifact.logged_by()\n",
    "        run_cfg = OmegaConf.create(run.config)\n",
    "        print(run_cfg)\n",
    "        model_cfg = OmegaConf.create(run_cfg.model)\n",
    "    \n",
    "    model_template, state_template = eqx.nn.make_with_state(model_cls)(\n",
    "        **model_cfg\n",
    "    )\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        artifact.download(temp_dir)\n",
    "        model = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"model.ckpt\"), model_template, default_deserialise_filter_spec)\n",
    "        state = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"state.ckpt\"), state_template, default_deserialise_filter_spec)\n",
    "\n",
    "    meta = artifact.metadata\n",
    "    return model, state, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ab55e",
   "metadata": {},
   "source": [
    "# Loading the Downstream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15f7799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 32, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.03, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1500, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "layer = \"2\"\n",
    "pretrain_mode = \"scratch\"\n",
    "train_mode = \"all\"\n",
    "alias = \"best\" # can be latest/best/ epoch_{any value in range(0,1000,100)}\n",
    "# epoch 0 now stores a fresh model.\n",
    "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_rtt/l{layer}_{pretrain_mode}_{train_mode}_checkpoint:{alias}\"\n",
    "model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e3254",
   "metadata": {},
   "source": [
    "# Loading the Foundational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb84781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "model = \"l2\"\n",
    "dataset = \"similar_task\"\n",
    "alias = \"best\"\n",
    "\n",
    "# Bit of a hack to manually feed in model params for now. Future renditions wont need this. \n",
    "model_cfg = {\n",
    "    \"ssm_io_dim\": 256,\n",
    "    \"ssm_dim\": 256,\n",
    "    \"ssm_init_diag_blocks\": 4,\n",
    "    \"ssm_num_layers\": 2,\n",
    "    \"output_dim\": 2,\n",
    "    \"rng_seed\": 42,\n",
    "    \"dt_min\": 0.001,\n",
    "    \"dt_max\": 0.01,\n",
    "    \"dropout_p\": 0.03,\n",
    "    \"ssm_dropout_p\": 0.03\n",
    "}\n",
    "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_pretrain/{model}_{dataset}_checkpoint:{alias}\"\n",
    "model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMFoundationalDecoder, model_cfg=model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c1b41",
   "metadata": {},
   "source": [
    "# Calling with activations (Downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c66262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
    "# Edit dataset_dir to your directory\n",
    "dataset_dir = \"../../data/foundational_ssm/processed/nlb\" \n",
    "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
    "data = h5_to_dict(dataset_path)\n",
    "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
    "input = data[\"neural_input\"]\n",
    "target_vel = data[\"behavior_input\"]\n",
    "\n",
    "# Specify the layers you want to generate the activations of. \n",
    "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
    "layer_keys = [\"ssm_pre_activation\"] \n",
    "inf_model = eqx.nn.inference_mode(model) # Switches off dropout\n",
    "pred_vel, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state, layer_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ae424",
   "metadata": {},
   "source": [
    "# Example: Plotting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from foundational_ssm.plotting import aggregate_bin_label_results, plot_pred_vs_targets_by_angle_bin\n",
    "\n",
    "\n",
    "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
    "# Edit dataset_dir to your directory\n",
    "dataset_dir = \"../../data/foundational_ssm/processed/nlb\" \n",
    "trial_info = pd.read_csv(os.path.join(dataset_dir, \"mc_rtt_trialized.csv\"))\n",
    "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
    "data = h5_to_dict(dataset_path)\n",
    "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
    "input = data[\"neural_input\"]\n",
    "target_vel = data[\"behavior_input\"]\n",
    "\n",
    "# Specify the layers you want to generate the activations of. \n",
    "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
    "layer_keys = [\"ssm_pre_activation\"] \n",
    "inf_model = eqx.nn.inference_mode(model) # Switches off dropout\n",
    "pred_vel, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state, layer_keys)\n",
    "\n",
    "results_df = aggregate_bin_label_results(trial_info, target_vel, pred_vel)\n",
    "fig = plot_pred_vs_targets_by_angle_bin(results_df)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
