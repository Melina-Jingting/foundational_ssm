{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c168df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, default_collate\n",
    "\n",
    "from jax import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from foundational_ssm.models import S5\n",
    "from foundational_ssm.utils import h5_to_dict\n",
    "from foundational_ssm.data_preprocessing import smooth_spikes\n",
    "\n",
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = random.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input, state, key):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = jax.lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias), state\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, state, inputs, key):\n",
    "    \"\"\"Predict on a batch of inputs using JAX's vmap\"\"\"\n",
    "    batch_keys = random.split(key, inputs.shape[0])\n",
    "    preds, _ = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0))(inputs, state, batch_keys)\n",
    "    return preds\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  \"\"\"To allow us to use torch dataloaders with JAX models\"\"\"\n",
    "  return tree_map(np.asarray, default_collate(batch))\n",
    "\n",
    "def compute_r2_single_dim(pred_single_dim, target_single_dim):\n",
    "    \"\"\"Compute R² for a single dimension\"\"\"\n",
    "    corr_matrix = jnp.corrcoef(pred_single_dim, target_single_dim)\n",
    "    return corr_matrix[0, 1] ** 2 \n",
    "\n",
    "def compute_r2(preds, targets):\n",
    "    r2 = jax.vmap(\n",
    "        compute_r2_single_dim,\n",
    "        in_axes=(0, 0),\n",
    "        out_axes=0\n",
    "    )(preds, targets).mean()\n",
    "    return r2\n",
    "\n",
    "def compute_mse(preds, targets):\n",
    "    return jnp.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(preds, targets):\n",
    "    \"\"\"Compute comprehensive R² metrics\"\"\"\n",
    "    \n",
    "    # Flatten: (batch_size, time_steps, output_dim) -> (batch_size*time_steps, output_dim)\n",
    "    preds_flat = preds.reshape(-1, preds.shape[-1]) \n",
    "    targets_flat = targets.reshape(-1, targets.shape[-1])\n",
    "    \n",
    "    r2 = compute_r2(preds_flat, targets_flat)\n",
    "    mse = compute_mse(preds_flat, targets_flat)\n",
    "    \n",
    "    return {\n",
    "        \"r2\": r2,\n",
    "        \"mse\": mse\n",
    "    }\n",
    "\n",
    "def log_model_params_and_grads(model, grads=None):\n",
    "    \"\"\"Extract model parameters and gradients for logging\"\"\"\n",
    "    params_dict = {}\n",
    "    grads_dict = {}\n",
    "    \n",
    "    # Convert model parameters to numpy for logging\n",
    "    flat_params = eqx.filter(model, eqx.is_inexact_array)\n",
    "    for path, param in eqx.filter_eval_shape(lambda x: x, flat_params).items():\n",
    "        # Create hierarchical parameter name\n",
    "        param_name = '.'.join(str(p) for p in path if p is not None)\n",
    "        if not param_name:\n",
    "            continue\n",
    "            \n",
    "        # Get parameter value\n",
    "        param_value = eqx.filter_get(flat_params, path)\n",
    "        \n",
    "        # For logging scalar statistics of parameters\n",
    "        if hasattr(param_value, 'shape'):\n",
    "            params_dict[f\"params/{param_name}/mean\"] = np.array(param_value).mean()\n",
    "            params_dict[f\"params/{param_name}/std\"] = np.array(param_value).std()\n",
    "            params_dict[f\"params/{param_name}/l2_norm\"] = np.sqrt(np.sum(np.array(param_value)**2))\n",
    "            \n",
    "            # Record gradient statistics if provided\n",
    "            if grads is not None:\n",
    "                grad_value = eqx.filter_get(grads, path)\n",
    "                if grad_value is not None:\n",
    "                    grads_dict[f\"grads/{param_name}/mean\"] = np.array(grad_value).mean()\n",
    "                    grads_dict[f\"grads/{param_name}/std\"] = np.array(grad_value).std()\n",
    "                    grads_dict[f\"grads/{param_name}/l2_norm\"] = np.sqrt(np.sum(np.array(grad_value)**2))\n",
    "    \n",
    "    return {**params_dict, **grads_dict}\n",
    "\n",
    "# @eqx.filter_value_and_grad\n",
    "@eqx.filter_jit\n",
    "def mse_loss(model, state, inputs, targets, key):\n",
    "    # model = eqx.combine(model_params, model_static)\n",
    "    # batch_keys = random.split(key, inputs.shape[0])\n",
    "    preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, None), out_axes=(0, None))(inputs, state, key)\n",
    "    mse = jnp.mean((preds - targets) ** 2)\n",
    "    return (mse, state)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, inputs, targets, state, opt, opt_state, key):\n",
    "    # filter_spec = jax.tree_util.tree_map(lambda _: True, model)\n",
    "    # model_params, model_static = eqx.partition(model, filter_spec)\n",
    "    (value, state), grads = eqx.filter_value_and_grad(mse_loss, has_aux=True)(model, state, inputs, targets, key)\n",
    "    updates, opt_state = opt.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state, value, grads\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    train_loader, \n",
    "    train_tensors,\n",
    "    val_tensors,\n",
    "    loss_fn, \n",
    "    state,\n",
    "    opt,\n",
    "    epochs,\n",
    "    log_every,\n",
    "    key=random.PRNGKey(0),\n",
    "    wandb_run_name=None\n",
    "):\n",
    "    if wandb_run_name is not None:\n",
    "        wandb.init(\n",
    "            project=\"foundational_ssm_nlb\",\n",
    "            name=wandb_run_name,\n",
    "            config=conf\n",
    "        )\n",
    "    \n",
    "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            key, subkey = random.split(key)\n",
    "            \n",
    "            model, state, opt_state, loss_value, grads = make_step(\n",
    "                model,\n",
    "                inputs, \n",
    "                targets,\n",
    "                state,\n",
    "                opt,\n",
    "                opt_state,  \n",
    "                subkey\n",
    "            )\n",
    "            \n",
    "            if wandb_run_name is not None:\n",
    "                wandb.log({\n",
    "                    \"train/loss\": float(loss_value)\n",
    "                })\n",
    "            \n",
    "        if epoch % log_every == 0:\n",
    "            # Generate keys for evaluation\n",
    "            key, train_key, val_key = random.split(key, 3)\n",
    "            \n",
    "            # Get inputs and targets for train and val sets\n",
    "            train_inputs, train_targets = train_tensors\n",
    "            train_inputs = jnp.array(train_inputs.numpy())\n",
    "            train_targets = jnp.array(train_targets.numpy())\n",
    "            train_preds = predict_batch(model, state, train_inputs, train_key)\n",
    "            \n",
    "            val_inputs, val_targets = val_tensors\n",
    "            val_inputs = jnp.array(val_inputs.numpy())\n",
    "            val_targets = jnp.array(val_targets.numpy())\n",
    "            val_preds = predict_batch(model, state, val_inputs, val_key)\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_mse = compute_mse(train_preds, train_targets)\n",
    "            train_r2 = compute_r2(train_preds, train_targets)\n",
    "            val_r2 = compute_r2(val_preds, val_targets)\n",
    "            \n",
    "            # Log to console\n",
    "            print(f\"Epoch {epoch}/{epochs}, Train Loss: {train_mse:.4f}\")\n",
    "            print(f\"  Train R²: {train_r2:.4f}, Val R²: {val_r2:.4f}\")\n",
    "            print(f\"  Train MSE: {train_mse:.4f}\")\n",
    "            \n",
    "            # Extract and prepare parameter/gradient statistics for logging\n",
    "            if wandb_run_name is not None:\n",
    "                wandb_log_dict = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"metrics/train/mse\": float(train_mse),\n",
    "                    \"metrics/train/r2\": float(train_r2),\n",
    "                    \"metrics/val/r2\": float(val_r2),\n",
    "                }\n",
    "                # param_grad_dict = log_model_params_and_grads(model, grads)\n",
    "                # wandb_log_dict.update(param_grad_dict)\n",
    "                \n",
    "                # Log everything to WandB\n",
    "                wandb.log(wandb_log_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================ \n",
    "# Main script starts here\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processed_data_folder = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb' \n",
    "dataset_name = \"mc_maze\"\n",
    "processed_data_path = os.path.join(processed_data_folder,dataset_name + \".h5\")\n",
    "trial_info_path = os.path.join(processed_data_folder,dataset_name + \".csv\")\n",
    "\n",
    "conf = {\n",
    "    'task':'decoding',\n",
    "    'dataset': {\n",
    "        'dataset': dataset_name\n",
    "    },\n",
    "    'model': {\n",
    "        'input_dim': 182,\n",
    "        'output_dim': 2,\n",
    "        'd_state': 64,\n",
    "        'num_layers': 1,\n",
    "        'hidden_dim': 64,\n",
    "        'dropout': 0.1,\n",
    "        'ssm_core':'s5'\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'lr': 0.004,\n",
    "        'weight_decay': 0.01  # Added common parameter\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 64,\n",
    "        'epochs': 2000\n",
    "    },\n",
    "    'device': 'cuda',\n",
    "    'framework': 'jax'\n",
    "}\n",
    "\n",
    "args = OmegaConf.create(conf)\n",
    "\n",
    "with h5py.File(processed_data_path, 'r') as h5file:\n",
    "    dataset_dict = h5_to_dict(h5file)\n",
    "\n",
    "trial_info = pd.read_csv(trial_info_path)\n",
    "trial_info = trial_info[trial_info['split'].isin(['train','val'])]\n",
    "min_idx = trial_info['trial_id'].min()\n",
    "trial_info['trial_id'] = trial_info['trial_id'] - min_idx\n",
    "\n",
    "train_ids = trial_info[trial_info['split']=='train']['trial_id'].tolist()\n",
    "val_ids = trial_info[trial_info['split']=='val']['trial_id'].tolist()\n",
    "\n",
    "# Concatenate both heldin and heldout spikes since we're using spikes to predict behavior\n",
    "spikes = np.concat([\n",
    "    dataset_dict['train_spikes_heldin'], \n",
    "    dataset_dict['train_spikes_heldout']],axis=2) \n",
    "smoothed_spikes = torch.tensor(\n",
    "    smooth_spikes(spikes, kern_sd_ms=40, bin_width=5), \n",
    "    dtype=torch.float64)\n",
    "behavior = torch.tensor(\n",
    "    dataset_dict['train_behavior'],\n",
    "    dtype=torch.float64)\n",
    "\n",
    "input_dim = smoothed_spikes.shape[2]\n",
    "output_dim = behavior.shape[2]\n",
    "\n",
    "# Split train and val based on splits from nlb\n",
    "train_dataset = TensorDataset(smoothed_spikes[train_ids], behavior[train_ids])\n",
    "val_dataset = TensorDataset(smoothed_spikes[val_ids], behavior[val_ids])\n",
    "full_dataset = TensorDataset(smoothed_spikes, behavior)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=args.training.batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=numpy_collate\n",
    ")\n",
    "\n",
    "wandb_run_name = f\"nlb_{args.task}_{args.model.ssm_core}_l{args.model.num_layers}_d{args.model.d_state}\"\n",
    "\n",
    "model_key = random.PRNGKey(0)\n",
    "model = S5(\n",
    "    key= model_key,\n",
    "    num_blocks=args.model.num_layers,\n",
    "    N=args.model.input_dim,\n",
    "    ssm_size=args.model.d_state,\n",
    "    ssm_blocks=1,\n",
    "    H=args.model.hidden_dim,\n",
    "    output_dim=args.model.output_dim,\n",
    ")\n",
    "# model = RNN(in_size=182, out_size=2, hidden_size=128, key=model_key)\n",
    "\n",
    "# Initialize the state\n",
    "key = random.PRNGKey(0)\n",
    "state = eqx.nn.State(model)\n",
    "batch_size = smoothed_spikes.shape[0]\n",
    "keys = random.split(random.PRNGKey(0), batch_size)\n",
    "\n",
    "opt = optax.adamw(\n",
    "    learning_rate=args.optimizer.lr, \n",
    "    weight_decay=args.optimizer.weight_decay\n",
    ")\n",
    "opt_state = opt.init(eqx.filter(model, eqx.is_array))\n",
    "loss_fn = mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a152e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/jax/_src/lax/lax.py:5377: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    key, subkey = random.split(key)\n",
    "    model, state, opt_state, loss_value, grads = make_step(\n",
    "        model,\n",
    "        inputs, \n",
    "        targets,\n",
    "        state,\n",
    "        opt,\n",
    "        opt_state,  \n",
    "        subkey\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aeaa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmelinajingting\u001b[0m (\u001b[33mmelinajingting-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/notebooks/wandb/run-20250622_150204-hcuykieq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb/runs/hcuykieq' target=\"_blank\">nlb_decoding_s5_l1_d64</a></strong> to <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb/runs/hcuykieq' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb/runs/hcuykieq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/melinajingting-ucl/foundational_ssm_nlb/runs/hcuykieq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fee9423cec0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"foundational_ssm_nlb\",\n",
    "    name=wandb_run_name,\n",
    "    config=conf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3c13164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu\n",
    "\n",
    "def log_model_params_and_grads_wandb(model, grads=None):\n",
    "    model_params = jtu.tree_flatten_with_path(model)[0] \n",
    "    grads = jtu.tree_flatten_with_path(grads)[0] if grads is not None else []\n",
    "    for path, value in model_params:\n",
    "        if eqx.is_array(value):\n",
    "            full_path = \"\".join(str(p) for p in path)\n",
    "            hist = wandb.Histogram(value.flatten())\n",
    "            wandb.log({\n",
    "                f\"params/{full_path}\": hist\n",
    "            })\n",
    "    for path, value in grads:\n",
    "        if eqx.is_array(value):\n",
    "            full_path = \"\".join(str(p) for p in path)\n",
    "            hist = wandb.Histogram(value.flatten())\n",
    "            wandb.log({\n",
    "                f\"grads/{full_path}\": hist\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62f3a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/wandb/sdk/data_types/histogram.py:77: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "  histogram, bins = np.histogram(sequence, bins=num_bins)\n"
     ]
    }
   ],
   "source": [
    "log_model_params_and_grads_wandb(model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e4b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
