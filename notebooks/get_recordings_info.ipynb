{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4656a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from foundational_ssm.data_utils.dataset import TorchBrainDataset\n",
    "from foundational_ssm.constants import DATA_ROOT\n",
    "\n",
    "def parse_session_id(session_id: str) -> Tuple[str, str, str]:\n",
    "    patterns = {\n",
    "        \"churchland_shenoy_neural_2012\": re.compile(r\"([^/]+)/([^_]+)_[0-9]+_(.+)\"),\n",
    "        \"flint_slutzky_accurate_2012\": re.compile(r\"([^/]+)/monkey_([^_]+)_e1_(.+)\"),\n",
    "        \"odoherty_sabes_nonhuman_2017\": re.compile(r\"([^/]+)/([^_]+)_[0-9]{8}_[0-9]+\"),\n",
    "        \"pei_pandarinath_nlb_2021\": re.compile(r\"([^/]+)/([^_]+)_(.+)\"),\n",
    "        \"perich_miller_population_2018\": re.compile(r\"([^/]+)/([^_]+)_[0-9]+_(.+)\"),\n",
    "    }\n",
    "\n",
    "    dataset = session_id.split('/')[0]\n",
    "    if dataset not in patterns:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "    match = patterns[dataset].match(session_id)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not parse session_id: {session_id!r}\")\n",
    "\n",
    "    if dataset == \"odoherty_sabes_nonhuman_2017\":\n",
    "        # Always assign task as 'random_target_reaching'\n",
    "        _, subject = match.groups()\n",
    "        return dataset, subject, \"random_target_reaching\"\n",
    "    elif dataset == \"flint_slutzky_accurate_2012\":\n",
    "        # task is always 'center_out_reaching'\n",
    "        _, subject, _ = match.groups()\n",
    "        return dataset, subject, \"center_out_reaching\"\n",
    "    else:\n",
    "        return match.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8844691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [25:58, 10.53s/it] \n"
     ]
    }
   ],
   "source": [
    "pretrain_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/pretrain_train_and_val.yaml\"\n",
    "\n",
    "pretrain_dataset = TorchBrainDataset(\n",
    "        root=\"../\"+DATA_ROOT,                # root directory where .h5 files are found\n",
    "        # recording_id=recording_id,  # you either specify a single recording ID\n",
    "        config=pretrain_config_path,                 # or a config for multi-session training / more complex configs\n",
    "        keep_files_open=True,\n",
    "        lazy=True,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "pretrain_sampling_intervals = pretrain_dataset.get_sampling_intervals()        \n",
    "DATASET_GROUP_INFO: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "\n",
    "for i, (recording_id, train_intervals) in tqdm(enumerate(pretrain_sampling_intervals.items())):\n",
    "    recording_data = pretrain_dataset.get_recording_data(recording_id)\n",
    "    num_units = int(np.max(recording_data.spikes.unit_index))\n",
    "    dataset, subject, task = parse_session_id(recording_id)\n",
    "    train_duration = np.sum(train_intervals.end - train_intervals.start)\n",
    "    # print(type(recording_data.cursor.vel))\n",
    "    if recording_id.startswith(\"pei_pandarinath_nlb_2021\"):\n",
    "        behavior_sampling_rate = np.min(recording_data.hand.timestamps[1:] - recording_data.hand.timestamps[:-1])\n",
    "    else:    \n",
    "        behavior_sampling_rate = np.min(recording_data.cursor.timestamps[1:] - recording_data.cursor.timestamps[:-1])\n",
    "    if (dataset, subject, task) not in DATASET_GROUP_INFO:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)] = {\n",
    "            \"max_num_units\": num_units,\n",
    "            \"behavior_dim\": 2,\n",
    "            \"train_duration\": float(round(train_duration, 2)),\n",
    "            \"min_behavior_sampling_rate\": behavior_sampling_rate,\n",
    "        }\n",
    "    else:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"] = max(DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"], num_units)\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] = DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] + train_duration\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"] = min(DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"], behavior_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa654cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "downstream_t_rt_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/downstream_t_rt.yaml\"\n",
    "downstream_t_co_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/downstream_t_co.yaml\"\n",
    "\n",
    "downstream_t_co_dataset = TorchBrainDataset(\n",
    "        root=\"../\"+DATA_ROOT,                # root directory where .h5 files are found\n",
    "        # recording_id=recording_id,  # you either specify a single recording ID\n",
    "        config=downstream_t_co_config_path,                 # or a config for multi-session training / more complex configs\n",
    "        keep_files_open=True,\n",
    "        lazy=True,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "downstream_t_co_sampling_intervals = downstream_t_co_dataset.get_sampling_intervals()        \n",
    "DATASET_GROUP_INFO: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "\n",
    "for i, (recording_id, train_intervals) in tqdm(enumerate(downstream_t_co_sampling_intervals.items())):\n",
    "    recording_data = downstream_t_co_dataset.get_recording_data(recording_id)\n",
    "    num_units = int(np.max(recording_data.spikes.unit_index))\n",
    "    dataset, subject, task = parse_session_id(recording_id)\n",
    "    train_duration = np.sum(train_intervals.end - train_intervals.start)\n",
    "    # print(type(recording_data.cursor.vel))\n",
    "    if recording_id.startswith(\"pei_pandarinath_nlb_2021\"):\n",
    "        behavior_sampling_rate = np.min(recording_data.hand.timestamps[1:] - recording_data.hand.timestamps[:-1])\n",
    "    else:    \n",
    "        behavior_sampling_rate = np.min(recording_data.cursor.timestamps[1:] - recording_data.cursor.timestamps[:-1])\n",
    "    if (dataset, subject, task) not in DATASET_GROUP_INFO:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)] = {\n",
    "            \"max_num_units\": num_units,\n",
    "            \"behavior_dim\": 2,\n",
    "            \"train_duration\": float(round(train_duration, 2)),\n",
    "            \"min_behavior_sampling_rate\": behavior_sampling_rate,\n",
    "        }\n",
    "    else:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"] = max(DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"], num_units)\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] = DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] + train_duration\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"] = min(DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"], behavior_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7979aef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('perich_miller_population_2018',\n",
       "  't',\n",
       "  'center_out_reaching'): {'max_num_units': 58, 'behavior_dim': 2, 'train_duration': np.float64(2193.2977666666666), 'min_behavior_sampling_rate': np.float64(0.009999999999990905)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_GROUP_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfc08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
