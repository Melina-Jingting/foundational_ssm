{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# Foundational SSM core imports\n",
    "from foundational_ssm.loaders import get_brainset_data_loader\n",
    "from foundational_ssm.constants import DATA_ROOT\n",
    "from foundational_ssm.samplers import TrialSampler\n",
    "import os \n",
    "import equinox as eqx\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True) # otherwise causes deadlock on jax.\n",
    "\n",
    "data_root = '../' + DATA_ROOT # change to the folder holding the brainsets\n",
    "config_dir = '../configs/dataset' # change\n",
    "dataset_args = {\n",
    "    'keep_files_open': False,\n",
    "    'lazy': True,\n",
    "    'split': 'train' # or 'train' \n",
    "    'min_window_length': 3.280\n",
    "}\n",
    "dataloader_args = {\n",
    "    'batch_size': 128, # Adjust per your system capacity\n",
    "    'num_workers': 10,\n",
    "    'persistent_workers': False\n",
    "}\n",
    "sampler = 'SequentialFixedWindowSampler'\n",
    "sampler_args = { \n",
    "                'window_length': 3.279,\n",
    "                'drop_short': False \n",
    "                }\n",
    "\n",
    "dataset, data_loader = get_brainset_data_loader(\n",
    "    dataset_args=dataset_args,\n",
    "    sampler = sampler,\n",
    "    sampler_args = sampler_args,\n",
    "    dataloader_args = dataloader_args,\n",
    "    sampling_rate = 200,\n",
    "    dataset_cfg = os.path.join(config_dir, 'reaching.yaml'),\n",
    "    data_root = data_root\n",
    ")\n",
    "\n",
    "sessions = dataset.get_session_ids() # list of sessions in your dataset\n",
    "sampling_intervals = dataset.get_sampling_intervals() # list of sampling intervals for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5099094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_834439/1314508992.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
      "123it [00:54,  4.01it/s]/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "127it [00:56,  2.46it/s]/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "129it [00:56,  3.80it/s]/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: divide by zero encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "560it [05:28,  1.71it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 619.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'variance': Array(1., dtype=float32)},\n",
       " 1: {'variance': Array(1.0000001, dtype=float32)},\n",
       " 2: {'variance': Array(0.9999999, dtype=float32)},\n",
       " 3: {'variance': Array(1., dtype=float32)},\n",
       " 5: {'variance': Array(0.99999994, dtype=float32)},\n",
       " 6: {'variance': Array(1., dtype=float32)},\n",
       " 7: {'variance': Array(0.9999999, dtype=float32)},\n",
       " 8: {'variance': Array(0.9999999, dtype=float32)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "metrics = {}  # New: store metrics per group\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_dataset_group_idxs = []\n",
    "for batch_idx, batch in tqdm(enumerate(data_loader)):\n",
    "    batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
    "    dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
    "    inputs = batch[\"neural_input\"]\n",
    "    targets = batch[\"behavior_input\"]\n",
    "\n",
    "    all_targets.append(targets)\n",
    "    all_dataset_group_idxs.append(dataset_group_idxs)\n",
    "all_targets = jnp.concatenate(all_targets, axis=0)\n",
    "all_dataset_group_idxs = jnp.concatenate(all_dataset_group_idxs, axis=0)\n",
    "\n",
    "for group_idx in tqdm(jnp.unique(all_dataset_group_idxs)):\n",
    "    group_targets = all_targets[all_dataset_group_idxs == group_idx]\n",
    "    variance = jnp.var(group_targets.reshape(-1, 2), axis=0)\n",
    "    metrics[int(group_idx)] = {\n",
    "        \"variance\": variance.mean()\n",
    "    }\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8844691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [25:58, 10.53s/it] \n"
     ]
    }
   ],
   "source": [
    "pretrain_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/pretrain_train_and_val.yaml\"\n",
    "\n",
    "pretrain_dataset = TorchBrainDataset(\n",
    "        root=\"../\"+DATA_ROOT,                # root directory where .h5 files are found\n",
    "        # recording_id=recording_id,  # you either specify a single recording ID\n",
    "        config=pretrain_config_path,                 # or a config for multi-session training / more complex configs\n",
    "        keep_files_open=True,\n",
    "        lazy=True,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "pretrain_sampling_intervals = pretrain_dataset.get_sampling_intervals()        \n",
    "DATASET_GROUP_INFO: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "\n",
    "for i, (recording_id, train_intervals) in tqdm(enumerate(pretrain_sampling_intervals.items())):\n",
    "    recording_data = pretrain_dataset.get_recording_data(recording_id)\n",
    "    num_units = int(np.max(recording_data.spikes.unit_index))\n",
    "    dataset, subject, task = parse_session_id(recording_id)\n",
    "    train_duration = np.sum(train_intervals.end - train_intervals.start)\n",
    "    # print(type(recording_data.cursor.vel))\n",
    "    if recording_id.startswith(\"pei_pandarinath_nlb_2021\"):\n",
    "        behavior_sampling_rate = np.min(recording_data.hand.timestamps[1:] - recording_data.hand.timestamps[:-1])\n",
    "    else:    \n",
    "        behavior_sampling_rate = np.min(recording_data.cursor.timestamps[1:] - recording_data.cursor.timestamps[:-1])\n",
    "    if (dataset, subject, task) not in DATASET_GROUP_INFO:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)] = {\n",
    "            \"max_num_units\": num_units,\n",
    "            \"behavior_dim\": 2,\n",
    "            \"train_duration\": float(round(train_duration, 2)),\n",
    "            \"min_behavior_sampling_rate\": behavior_sampling_rate,\n",
    "        }\n",
    "    else:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"] = max(DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"], num_units)\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] = DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] + train_duration\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"] = min(DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"], behavior_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa654cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "downstream_t_rt_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/downstream_t_rt.yaml\"\n",
    "downstream_t_co_config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/downstream_t_co.yaml\"\n",
    "\n",
    "downstream_t_co_dataset = TorchBrainDataset(\n",
    "        root=\"../\"+DATA_ROOT,                # root directory where .h5 files are found\n",
    "        # recording_id=recording_id,  # you either specify a single recording ID\n",
    "        config=downstream_t_co_config_path,                 # or a config for multi-session training / more complex configs\n",
    "        keep_files_open=True,\n",
    "        lazy=True,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "downstream_t_co_sampling_intervals = downstream_t_co_dataset.get_sampling_intervals()        \n",
    "DATASET_GROUP_INFO: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
    "\n",
    "for i, (recording_id, train_intervals) in tqdm(enumerate(downstream_t_co_sampling_intervals.items())):\n",
    "    recording_data = downstream_t_co_dataset.get_recording_data(recording_id)\n",
    "    num_units = int(np.max(recording_data.spikes.unit_index))\n",
    "    dataset, subject, task = parse_session_id(recording_id)\n",
    "    train_duration = np.sum(train_intervals.end - train_intervals.start)\n",
    "    # print(type(recording_data.cursor.vel))\n",
    "    if recording_id.startswith(\"pei_pandarinath_nlb_2021\"):\n",
    "        behavior_sampling_rate = np.min(recording_data.hand.timestamps[1:] - recording_data.hand.timestamps[:-1])\n",
    "    else:    \n",
    "        behavior_sampling_rate = np.min(recording_data.cursor.timestamps[1:] - recording_data.cursor.timestamps[:-1])\n",
    "    if (dataset, subject, task) not in DATASET_GROUP_INFO:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)] = {\n",
    "            \"max_num_units\": num_units,\n",
    "            \"behavior_dim\": 2,\n",
    "            \"train_duration\": float(round(train_duration, 2)),\n",
    "            \"min_behavior_sampling_rate\": behavior_sampling_rate,\n",
    "        }\n",
    "    else:\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"] = max(DATASET_GROUP_INFO[(dataset, subject, task)][\"max_num_units\"], num_units)\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] = DATASET_GROUP_INFO[(dataset, subject, task)][\"train_duration\"] + train_duration\n",
    "        DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"] = min(DATASET_GROUP_INFO[(dataset, subject, task)][\"min_behavior_sampling_rate\"], behavior_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7979aef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('perich_miller_population_2018',\n",
       "  't',\n",
       "  'center_out_reaching'): {'max_num_units': 58, 'behavior_dim': 2, 'train_duration': np.float64(2193.2977666666666), 'min_behavior_sampling_rate': np.float64(0.009999999999990905)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_GROUP_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfc08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
