{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundational_ssm.data_utils.loaders import get_nlb_train_val_loaders\n",
    "from foundational_ssm.models import SSMFoundational\n",
    "from omegaconf import OmegaConf\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt \n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_map\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from foundational_ssm.constants import DATASET_IDX_TO_GROUP_SHORT\n",
    "from foundational_ssm.metrics import compute_r2_standard\n",
    "from foundational_ssm.utils import save_model_wandb\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, state, inputs, key, dataset_group_idx):\n",
    "    \"\"\"Predict on a batch of inputs using JAX's vmap\"\"\"\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, _ = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "    return preds\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def mse_loss(model_params, model_static, state, inputs, targets, dataset_group_idx, key):\n",
    "    model = eqx.combine(model_params, model_static)\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "    mse = jnp.mean((preds - targets) ** 2)\n",
    "    return (mse, state)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, filter_spec, inputs, targets, dataset_group_idx, loss_fn, opt, opt_state, key):\n",
    "    model_params, model_static = eqx.partition(model, filter_spec)\n",
    "    (value, state), grads = loss_fn(model_params, model_static, state, inputs, targets, dataset_group_idx, key)\n",
    "    updates, opt_state = opt.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state, value, grads\n",
    "\n",
    "def load_model_and_state(wandb_pretrained_model_id, hyperparams):\n",
    "    \"\"\"\n",
    "    either loads a model from wandb or creates a new model from hyperparams\n",
    "    Args:\n",
    "        wandb_pretrained_model_id: wandb artifact id of the model to load\n",
    "        hyperparams: dict of hyperparams to create a new model\n",
    "    Returns:\n",
    "        model (SSMFoundational): Loaded model or None if not specified.\n",
    "    \"\"\"\n",
    "    if wandb_pretrained_model_id is not None:\n",
    "        api = wandb.Api()\n",
    "        model_artifact = api.artifact(wandb_pretrained_model_id, type=\"model\")\n",
    "        model_artifact_dir = model_artifact.download()\n",
    "        model_filename = os.path.join(model_artifact_dir, 'best_model.pt')\n",
    "        with open(model_filename, \"rb\") as f:\n",
    "            hyperparams = json.loads(f.readline().decode())\n",
    "            model = SSMFoundational(**hyperparams)\n",
    "            model = eqx.tree_deserialise_leaves(f, model)\n",
    "            state = eqx.nn.State(model)\n",
    "        return model, state\n",
    "    else:\n",
    "        model = SSMFoundational(**hyperparams)\n",
    "        state = eqx.nn.State(model)\n",
    "        return model, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../configs/finetune.yaml')\n",
    "train_dataset, train_loader, val_dataset, val_loader = get_nlb_train_val_loaders()\n",
    "model, state = load_model_and_state(cfg.wandb_pretrained_model_id, cfg.model)\n",
    "\n",
    "key = jr.PRNGKey(cfg.rng_seed)\n",
    "train_key, val_key = jr.split(key, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmelinajingting\u001b[0m (\u001b[33mmelinajingting-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/notebooks/wandb/run-20250630_175301-mbkfn3cg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">scratch_holdout-True</a></strong> to <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7feaf23f9590>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_spec = tree_map(eqx.is_inexact_array, model)\n",
    "lr_scheduler = lambda step: cfg.optimizer.lr\n",
    "# Load JAX optimizer with scheduler\n",
    "opt = optax.chain(\n",
    "    optax.adamw(learning_rate=lr_scheduler, weight_decay=cfg.optimizer.weight_decay)\n",
    ")\n",
    "opt_state = opt.init(eqx.filter(model, filter_spec))\n",
    "\n",
    "loss_fn = mse_loss\n",
    "\n",
    "run_name = f'{cfg.finetune_mode}_holdout-{cfg.train_dataset.holdout_angles}'\n",
    "config_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "wandb.init(project=cfg.wandb.project, name=run_name, config=config_dict)  # type: ignore\n",
    "\n",
    "# Define metrics with custom x-axis\n",
    "wandb.define_metric(\"epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"epoch_train_loss\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000, Loss: 817233.1875\n",
      "Epoch 50/2000, Loss: 73276.0078\n",
      "Epoch 100/2000, Loss: 31008.3047\n",
      "Epoch 150/2000, Loss: 17932.3867\n",
      "Epoch 200/2000, Loss: 13768.1348\n",
      "Epoch 250/2000, Loss: 11209.4170\n",
      "Epoch 300/2000, Loss: 9466.0146\n",
      "Epoch 350/2000, Loss: 8810.9336\n",
      "Epoch 400/2000, Loss: 6529.4321\n",
      "Epoch 450/2000, Loss: 6865.3301\n",
      "Epoch 500/2000, Loss: 5781.3315\n",
      "Epoch 550/2000, Loss: 4819.2446\n",
      "Epoch 600/2000, Loss: 4290.4917\n",
      "Epoch 650/2000, Loss: 3444.5544\n",
      "Epoch 700/2000, Loss: 3620.8601\n",
      "Epoch 750/2000, Loss: 3223.9553\n",
      "Epoch 800/2000, Loss: 3053.0203\n",
      "Epoch 850/2000, Loss: 2951.3818\n",
      "Epoch 900/2000, Loss: 2416.7649\n",
      "Epoch 950/2000, Loss: 2646.6418\n",
      "Epoch 1000/2000, Loss: 2861.0142\n",
      "Epoch 1050/2000, Loss: 2381.5173\n",
      "Epoch 1100/2000, Loss: 2225.2947\n",
      "Epoch 1150/2000, Loss: 2028.1149\n",
      "Epoch 1200/2000, Loss: 2255.2869\n",
      "Epoch 1250/2000, Loss: 2211.0710\n",
      "Epoch 1300/2000, Loss: 2136.7700\n",
      "Epoch 1350/2000, Loss: 2190.7129\n",
      "Epoch 1400/2000, Loss: 2576.8721\n",
      "Epoch 1450/2000, Loss: 1841.8878\n",
      "Epoch 1500/2000, Loss: 1584.2074\n",
      "Epoch 1550/2000, Loss: 2084.5078\n",
      "Epoch 1600/2000, Loss: 2398.7083\n",
      "Epoch 1650/2000, Loss: 1599.4613\n",
      "Epoch 1700/2000, Loss: 1904.1731\n",
      "Epoch 1750/2000, Loss: 1575.0427\n",
      "Epoch 1800/2000, Loss: 1332.7930\n",
      "Epoch 1850/2000, Loss: 1429.4158\n",
      "Epoch 1900/2000, Loss: 1476.3236\n",
      "Epoch 1950/2000, Loss: 1648.6356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/epoch_loss</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/r2_pm_c_co</td><td>▁▄██████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1950</td></tr><tr><td>train/epoch_loss</td><td>1648.63562</td></tr><tr><td>train/loss</td><td>57.03786</td></tr><tr><td>val/r2_pm_c_co</td><td>0.85623</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scratch_holdout-True</strong> at: <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg</a><br> View project at: <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_175301-mbkfn3cg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_r2_score = 0\n",
    "for epoch in range(cfg.training.epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"neural_input\"]\n",
    "        targets = batch[\"behavior_input\"]\n",
    "        dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "        key, subkey = jr.split(train_key)\n",
    "        model, state, opt_state, loss_value, grads = make_step(\n",
    "            model,\n",
    "            state,\n",
    "            filter_spec,\n",
    "            inputs,\n",
    "            targets,\n",
    "            dataset_group_idx,\n",
    "            loss_fn,\n",
    "            opt,\n",
    "            opt_state,\n",
    "            subkey)\n",
    "\n",
    "        # Get current learning rate from scheduler\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train/loss\": loss_value,\n",
    "        })\n",
    "        \n",
    "    if epoch % cfg.training.log_every == 0:\n",
    "        # Log the epoch value so wandb can use it as x-axis for validation metrics\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        wandb.log({\"train/epoch_loss\": epoch_loss})\n",
    "        \n",
    "        total_r2_score = 0\n",
    "        group_preds = defaultdict(list)\n",
    "        group_targets = defaultdict(list)\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"neural_input\"]\n",
    "            targets = batch[\"behavior_input\"]\n",
    "            dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "            dataset_group_key = DATASET_IDX_TO_GROUP_SHORT[dataset_group_idx]\n",
    "            \n",
    "            key, subkey = jr.split(val_key)\n",
    "            batch_keys = jr.split(subkey, inputs.shape[0])\n",
    "            preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "            group_preds[dataset_group_key].append(preds)\n",
    "            group_targets[dataset_group_key].append(targets)\n",
    "            \n",
    "        for group_key, preds in group_preds.items():\n",
    "            preds = jnp.concatenate(preds, axis=0)\n",
    "            targets = jnp.concatenate(group_targets[group_key], axis=0)\n",
    "            r2_score = compute_r2_standard(preds, targets)\n",
    "            wandb.log({f\"val/r2_{group_key}\": r2_score})\n",
    "            total_r2_score += r2_score\n",
    "        avg_r2_score = total_r2_score / len(group_preds)\n",
    "    \n",
    "        if avg_r2_score > best_r2_score:\n",
    "            best_r2_score = avg_r2_score\n",
    "            # save_model_wandb(model, run_name, OmegaConf.to_container(cfg.model), wandb.run)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{cfg.training.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
