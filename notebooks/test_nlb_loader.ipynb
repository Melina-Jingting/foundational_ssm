{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundational_ssm.data_utils.loaders import get_nlb_train_val_loaders\n",
    "from foundational_ssm.models import SSMFoundational\n",
    "from omegaconf import OmegaConf\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt \n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_map\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from foundational_ssm.constants import DATASET_IDX_TO_GROUP_SHORT\n",
    "from foundational_ssm.metrics import compute_r2_standard\n",
    "from foundational_ssm.utils import save_model_wandb\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, state, inputs, key, dataset_group_idx):\n",
    "    \"\"\"Predict on a batch of inputs using JAX's vmap\"\"\"\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, _ = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "    return preds\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def mse_loss(model_params, model_static, state, inputs, targets, dataset_group_idx, key):\n",
    "    model = eqx.combine(model_params, model_static)\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "    mse = jnp.mean((preds - targets) ** 2)\n",
    "    return (mse, state)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, filter_spec, inputs, targets, dataset_group_idx, loss_fn, opt, opt_state, key):\n",
    "    model_params, model_static = eqx.partition(model, filter_spec)\n",
    "    (value, state), grads = loss_fn(model_params, model_static, state, inputs, targets, dataset_group_idx, key)\n",
    "    updates, opt_state = opt.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state, value, grads\n",
    "\n",
    "def load_model_and_state(wandb_pretrained_model_id, hyperparams):\n",
    "    \"\"\"\n",
    "    either loads a model from wandb or creates a new model from hyperparams\n",
    "    Args:\n",
    "        wandb_pretrained_model_id: wandb artifact id of the model to load\n",
    "        hyperparams: dict of hyperparams to create a new model\n",
    "    Returns:\n",
    "        model (SSMFoundational): Loaded model or None if not specified.\n",
    "    \"\"\"\n",
    "    if wandb_pretrained_model_id is not None:\n",
    "        api = wandb.Api()\n",
    "        model_artifact = api.artifact(wandb_pretrained_model_id, type=\"model\")\n",
    "        model_artifact_dir = model_artifact.download()\n",
    "        model_filename = os.path.join(model_artifact_dir, 'best_model.pt')\n",
    "        with open(model_filename, \"rb\") as f:\n",
    "            hyperparams = json.loads(f.readline().decode())\n",
    "            model = SSMFoundational(**hyperparams)\n",
    "            model = eqx.tree_deserialise_leaves(f, model)\n",
    "            state = eqx.nn.State(model)\n",
    "        return model, state\n",
    "    else:\n",
    "        model = SSMFoundational(**hyperparams)\n",
    "        state = eqx.nn.State(model)\n",
    "        return model, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reach_angle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'reach_angle'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m cfg = OmegaConf.load(\u001b[33m'\u001b[39m\u001b[33m../configs/finetune.yaml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_dataset, train_loader, val_dataset, val_loader = \u001b[43mget_nlb_train_val_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmc_rtt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_target_reaching\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholdout_angles\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model, state = load_model_and_state(cfg.wandb_pretrained_model_id, cfg.model)\n\u001b[32m      5\u001b[39m key = jr.PRNGKey(cfg.rng_seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/src/foundational_ssm/data_utils/loaders.py:464\u001b[39m, in \u001b[36mget_nlb_train_val_loaders\u001b[39m\u001b[34m(dataset, task, holdout_angles, batch_size, data_root, collate_fn)\u001b[39m\n\u001b[32m    460\u001b[39m val_spikes = smoothed_spikes[val_mask]\n\u001b[32m    461\u001b[39m val_behavior = behavior[val_mask]\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m train_held_out = \u001b[43mget_held_out_flags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_info\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msplit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m val_held_out = get_held_out_flags(trial_info[trial_info[\u001b[33m'\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m], dataset, task)\n\u001b[32m    467\u001b[39m train_dataset = NLBDictDataset(train_spikes, train_behavior, group_idx_tensor, train_held_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/src/foundational_ssm/data_utils/loaders.py:383\u001b[39m, in \u001b[36mget_held_out_flags\u001b[39m\u001b[34m(trial_info, dataset, task)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataset == \u001b[33m'\u001b[39m\u001b[33mmc_rtt\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    382\u001b[39m     flags = []\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m angle \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrial_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreach_angle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m    384\u001b[39m         held_in = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    385\u001b[39m             \u001b[38;5;28mmin\u001b[39m(angle_range) <= angle <= \u001b[38;5;28mmax\u001b[39m(angle_range)\n\u001b[32m    386\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m angle_range \u001b[38;5;129;01min\u001b[39;00m MC_RTT_CONFIG.HELD_IN_REACH_ANGLE_RANGES\n\u001b[32m    387\u001b[39m         )\n\u001b[32m    388\u001b[39m         flags.append(\u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m held_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'reach_angle'"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../configs/finetune.yaml')\n",
    "train_dataset, train_loader, val_dataset, val_loader = get_nlb_train_val_loaders('mc_rtt', 'random_target_reaching', holdout_angles=False)\n",
    "model, state = load_model_and_state(cfg.wandb_pretrained_model_id, cfg.model)\n",
    "\n",
    "key = jr.PRNGKey(cfg.rng_seed)\n",
    "train_key, val_key = jr.split(key, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from src.foundational_ssm.constants.nlb import NLB_CONFIGS\n",
    "from src.foundational_ssm.data_utils.loaders import h5_to_dict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = 'mc_rtt'\n",
    "task = 'random_target_reaching'\n",
    "\n",
    "task_config = NLB_CONFIGS[dataset]\n",
    "data_root = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb'\n",
    "\n",
    "data_path = os.path.join(data_root, task_config.H5_FILE_NAME)\n",
    "trial_info_path = os.path.join(data_root, task_config.TRIAL_INFO_FILE_NAME)\n",
    "with h5py.File(data_path, 'r') as h5file:\n",
    "    dataset_dict = h5_to_dict(h5file)\n",
    "\n",
    "trial_info = pd.read_csv(trial_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00.600000</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:00.700000</td>\n",
       "      <td>0 days 00:00:01.300000</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0 days 00:00:01.400000</td>\n",
       "      <td>0 days 00:00:02</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0 days 00:00:02.100000</td>\n",
       "      <td>0 days 00:00:02.700000</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0 days 00:00:02.800000</td>\n",
       "      <td>0 days 00:00:03.400000</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>1347</td>\n",
       "      <td>1347</td>\n",
       "      <td>0 days 00:13:56.300000</td>\n",
       "      <td>0 days 00:13:56.900000</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1348</td>\n",
       "      <td>1348</td>\n",
       "      <td>0 days 00:13:56.900000</td>\n",
       "      <td>0 days 00:13:57.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1349</td>\n",
       "      <td>1349</td>\n",
       "      <td>0 days 00:13:57.500000</td>\n",
       "      <td>0 days 00:13:58.100000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>1350</td>\n",
       "      <td>1350</td>\n",
       "      <td>0 days 00:13:58.100000</td>\n",
       "      <td>0 days 00:13:58.700000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1351</td>\n",
       "      <td>1351</td>\n",
       "      <td>0 days 00:13:58.700000</td>\n",
       "      <td>0 days 00:13:59.300000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1352 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  trial_id              start_time                end_time  \\\n",
       "0              0         0         0 days 00:00:00  0 days 00:00:00.600000   \n",
       "1              1         1  0 days 00:00:00.700000  0 days 00:00:01.300000   \n",
       "2              2         2  0 days 00:00:01.400000         0 days 00:00:02   \n",
       "3              3         3  0 days 00:00:02.100000  0 days 00:00:02.700000   \n",
       "4              4         4  0 days 00:00:02.800000  0 days 00:00:03.400000   \n",
       "...          ...       ...                     ...                     ...   \n",
       "1347        1347      1347  0 days 00:13:56.300000  0 days 00:13:56.900000   \n",
       "1348        1348      1348  0 days 00:13:56.900000  0 days 00:13:57.500000   \n",
       "1349        1349      1349  0 days 00:13:57.500000  0 days 00:13:58.100000   \n",
       "1350        1350      1350  0 days 00:13:58.100000  0 days 00:13:58.700000   \n",
       "1351        1351      1351  0 days 00:13:58.700000  0 days 00:13:59.300000   \n",
       "\n",
       "      split  \n",
       "0      test  \n",
       "1      test  \n",
       "2      test  \n",
       "3      test  \n",
       "4      test  \n",
       "...     ...  \n",
       "1347    val  \n",
       "1348  train  \n",
       "1349  train  \n",
       "1350  train  \n",
       "1351  train  \n",
       "\n",
       "[1352 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmelinajingting\u001b[0m (\u001b[33mmelinajingting-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/notebooks/wandb/run-20250630_175301-mbkfn3cg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">scratch_holdout-True</a></strong> to <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7feaf23f9590>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_spec = tree_map(eqx.is_inexact_array, model)\n",
    "lr_scheduler = lambda step: cfg.optimizer.lr\n",
    "# Load JAX optimizer with scheduler\n",
    "opt = optax.chain(\n",
    "    optax.adamw(learning_rate=lr_scheduler, weight_decay=cfg.optimizer.weight_decay)\n",
    ")\n",
    "opt_state = opt.init(eqx.filter(model, filter_spec))\n",
    "\n",
    "loss_fn = mse_loss\n",
    "\n",
    "run_name = f'{cfg.finetune_mode}_holdout-{cfg.train_dataset.holdout_angles}'\n",
    "config_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "wandb.init(project=cfg.wandb.project, name=run_name, config=config_dict)  # type: ignore\n",
    "\n",
    "# Define metrics with custom x-axis\n",
    "wandb.define_metric(\"epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"epoch_train_loss\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000, Loss: 817233.1875\n",
      "Epoch 50/2000, Loss: 73276.0078\n",
      "Epoch 100/2000, Loss: 31008.3047\n",
      "Epoch 150/2000, Loss: 17932.3867\n",
      "Epoch 200/2000, Loss: 13768.1348\n",
      "Epoch 250/2000, Loss: 11209.4170\n",
      "Epoch 300/2000, Loss: 9466.0146\n",
      "Epoch 350/2000, Loss: 8810.9336\n",
      "Epoch 400/2000, Loss: 6529.4321\n",
      "Epoch 450/2000, Loss: 6865.3301\n",
      "Epoch 500/2000, Loss: 5781.3315\n",
      "Epoch 550/2000, Loss: 4819.2446\n",
      "Epoch 600/2000, Loss: 4290.4917\n",
      "Epoch 650/2000, Loss: 3444.5544\n",
      "Epoch 700/2000, Loss: 3620.8601\n",
      "Epoch 750/2000, Loss: 3223.9553\n",
      "Epoch 800/2000, Loss: 3053.0203\n",
      "Epoch 850/2000, Loss: 2951.3818\n",
      "Epoch 900/2000, Loss: 2416.7649\n",
      "Epoch 950/2000, Loss: 2646.6418\n",
      "Epoch 1000/2000, Loss: 2861.0142\n",
      "Epoch 1050/2000, Loss: 2381.5173\n",
      "Epoch 1100/2000, Loss: 2225.2947\n",
      "Epoch 1150/2000, Loss: 2028.1149\n",
      "Epoch 1200/2000, Loss: 2255.2869\n",
      "Epoch 1250/2000, Loss: 2211.0710\n",
      "Epoch 1300/2000, Loss: 2136.7700\n",
      "Epoch 1350/2000, Loss: 2190.7129\n",
      "Epoch 1400/2000, Loss: 2576.8721\n",
      "Epoch 1450/2000, Loss: 1841.8878\n",
      "Epoch 1500/2000, Loss: 1584.2074\n",
      "Epoch 1550/2000, Loss: 2084.5078\n",
      "Epoch 1600/2000, Loss: 2398.7083\n",
      "Epoch 1650/2000, Loss: 1599.4613\n",
      "Epoch 1700/2000, Loss: 1904.1731\n",
      "Epoch 1750/2000, Loss: 1575.0427\n",
      "Epoch 1800/2000, Loss: 1332.7930\n",
      "Epoch 1850/2000, Loss: 1429.4158\n",
      "Epoch 1900/2000, Loss: 1476.3236\n",
      "Epoch 1950/2000, Loss: 1648.6356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/epoch_loss</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/r2_pm_c_co</td><td>▁▄██████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1950</td></tr><tr><td>train/epoch_loss</td><td>1648.63562</td></tr><tr><td>train/loss</td><td>57.03786</td></tr><tr><td>val/r2_pm_c_co</td><td>0.85623</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scratch_holdout-True</strong> at: <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze/runs/mbkfn3cg</a><br> View project at: <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm_finetune_mc_maze</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_175301-mbkfn3cg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_r2_score = 0\n",
    "for epoch in range(cfg.training.epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"neural_input\"]\n",
    "        targets = batch[\"behavior_input\"]\n",
    "        dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "        key, subkey = jr.split(train_key)\n",
    "        model, state, opt_state, loss_value, grads = make_step(\n",
    "            model,\n",
    "            state,\n",
    "            filter_spec,\n",
    "            inputs,\n",
    "            targets,\n",
    "            dataset_group_idx,\n",
    "            loss_fn,\n",
    "            opt,\n",
    "            opt_state,\n",
    "            subkey)\n",
    "\n",
    "        # Get current learning rate from scheduler\n",
    "        epoch_loss += loss_value\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train/loss\": loss_value,\n",
    "        })\n",
    "        \n",
    "    if epoch % cfg.training.log_every == 0:\n",
    "        # Log the epoch value so wandb can use it as x-axis for validation metrics\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        wandb.log({\"train/epoch_loss\": epoch_loss})\n",
    "        \n",
    "        total_r2_score = 0\n",
    "        group_preds = defaultdict(list)\n",
    "        group_targets = defaultdict(list)\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"neural_input\"]\n",
    "            targets = batch[\"behavior_input\"]\n",
    "            dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "            dataset_group_key = DATASET_IDX_TO_GROUP_SHORT[dataset_group_idx]\n",
    "            \n",
    "            key, subkey = jr.split(val_key)\n",
    "            batch_keys = jr.split(subkey, inputs.shape[0])\n",
    "            preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "            group_preds[dataset_group_key].append(preds)\n",
    "            group_targets[dataset_group_key].append(targets)\n",
    "            \n",
    "        for group_key, preds in group_preds.items():\n",
    "            preds = jnp.concatenate(preds, axis=0)\n",
    "            targets = jnp.concatenate(group_targets[group_key], axis=0)\n",
    "            r2_score = compute_r2_standard(preds, targets)\n",
    "            wandb.log({f\"val/r2_{group_key}\": r2_score})\n",
    "            total_r2_score += r2_score\n",
    "        avg_r2_score = total_r2_score / len(group_preds)\n",
    "    \n",
    "        if avg_r2_score > best_r2_score:\n",
    "            best_r2_score = avg_r2_score\n",
    "            # save_model_wandb(model, run_name, OmegaConf.to_container(cfg.model), wandb.run)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{cfg.training.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
