{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dd0842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp \n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_flatten_with_path, GetAttrKey, DictKey, SequenceKey\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from foundational_ssm.models import SSMFoundationalDecoder, SSMDownstreamDecoder, discretize_zoh\n",
    "from foundational_ssm.utils import load_model_and_state_from_checkpoint_wandb\n",
    "\n",
    "from foundational_ssm.samplers import RandomVariableWindowSampler \n",
    "from foundational_ssm.constants import DATA_ROOT \n",
    "from foundational_ssm.loaders import get_brainset_data_loader\n",
    "from foundational_ssm.utils.training_utils import create_optimizer_and_state\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True) # otherwise causes deadlock on jax.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def mse_single_sample_foundation(model, state, input, target, mask, dataset_group_idxs, key, dataset_group_weights=None, skip_timesteps=0):\n",
    "    \"\"\"MSE loss for foundational model (takes dataset_group_idx and mask)\"\"\"\n",
    "    pred, state = model(input, state, dataset_group_idxs, key)\n",
    "\n",
    "    # Only evaluate loss on timesteps > skip_timesteps\n",
    "    pred = pred[skip_timesteps:, :]  # Shape: (batch, seq_len - skip_timesteps, output_dim)\n",
    "    target = target[skip_timesteps:, :]  # Shape: (batch, seq_len - skip_timesteps, output_dim)\n",
    "    mask = mask[skip_timesteps:]  # Shape: (batch, seq_len - skip_timesteps)\n",
    "\n",
    "    # Only compute loss on unmasked elements\n",
    "    squared_error = (pred - target) ** 2\n",
    "    mask = mask[..., None]\n",
    "    masked_squared_error = jnp.where(mask, squared_error, 0.0)\n",
    "    \n",
    "    # dataset_group_weights = dataset_group_weights[..., None, None]  # shape (batch, 1, 1) to broadcast\n",
    "    weighted_squared_error = squared_error #* dataset_group_weights\n",
    "        \n",
    "    masked_squared_error = jnp.where(mask, weighted_squared_error, 0.0)\n",
    "    mse = masked_squared_error.sum() / mask.sum()\n",
    "    return mse\n",
    "\n",
    "\n",
    "def _path_to_str(path):\n",
    "    parts = []\n",
    "    for entry in path:\n",
    "        if isinstance(entry, GetAttrKey):\n",
    "            parts.append(str(entry.name))\n",
    "        elif isinstance(entry, DictKey):\n",
    "            parts.append(str(entry.key))\n",
    "        elif isinstance(entry, SequenceKey):\n",
    "            parts.append(f\"[{entry.idx}]\")\n",
    "        else:\n",
    "            parts.append(str(entry))\n",
    "    return \".\".join(parts)\n",
    "\n",
    "def _stack_weighted_grads_to_matrix(grads_batch_tree, lr_leaves):\n",
    "    \"\"\"Return a dict of per-leaf weighted, flattened gradients keyed by leaf path.\n",
    "\n",
    "    grads_batch_tree: PyTree of per-sample gradients; each array leaf has shape (N, ...).\n",
    "    lr_leaves: list of scalars aligned with the flattened leaf order.\n",
    "    Returns: dict[str, jnp.ndarray] mapping leaf path -> (N, leaf_size) matrix, weighted by sqrt(lr).\n",
    "    \"\"\"\n",
    "    # Filter non-array leaves, keeping structure for alignment\n",
    "    filtered = eqx.filter(grads_batch_tree, eqx.is_array)\n",
    "    path_leaves, _ = tree_flatten_with_path(filtered)\n",
    "    out = {}\n",
    "    i = 0\n",
    "    for path, leaf in path_leaves:\n",
    "        if leaf is None:\n",
    "            continue\n",
    "        g2d = jnp.reshape(leaf, (leaf.shape[0], -1))\n",
    "        lr_scalar = lr_leaves[i]\n",
    "        out[_path_to_str(path)] = g2d * jnp.sqrt(lr_scalar)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def _dicts_to_matrix(dict_list):\n",
    "    \"\"\"Concatenate per-leaf dicts into a single matrix.\n",
    "\n",
    "    - Uses a stable sorted key order from the first dict.\n",
    "    - Horizontally concatenates leaves per sample, then vertically stacks across dicts.\n",
    "    \"\"\"\n",
    "    if not dict_list:\n",
    "        return jnp.zeros((0, 0))\n",
    "    keys = sorted(dict_list[0].keys())\n",
    "    row_blocks = []\n",
    "    for d in dict_list:\n",
    "        if not keys:\n",
    "            return jnp.zeros((d[next(iter(d))].shape[0], 0)) if d else jnp.zeros((0, 0))\n",
    "        row_blocks.append(jnp.concatenate([d[k] for k in keys], axis=1))\n",
    "    return jnp.concatenate(row_blocks, axis=0)\n",
    "\n",
    "def stack_dicts(dict_list):\n",
    "    \"\"\"Stack a list of dicts (per-batch) into a single dict of stacked arrays keyed by leaf path.\n",
    "\n",
    "    Args:\n",
    "        dict_list (list): list of dicts each containing array leaves with leading batch dim.\n",
    "\n",
    "    Returns:\n",
    "        dict mapping canonical leaf path (str) -> array of shape (sum_batches, leaf_size...)\n",
    "    \"\"\"\n",
    "    if not dict_list:\n",
    "        return {}\n",
    "    keys = sorted(dict_list[0].keys())\n",
    "    stacked = {k: jnp.concatenate([d[k] for d in dict_list], axis=0) for k in keys}\n",
    "    return stacked\n",
    "\n",
    "def stack_trees_to_dict(tree_list, flatten=True):\n",
    "    \"\"\"Stack a list of PyTrees (per-batch) into a dict of stacked arrays keyed by leaf path.\n",
    "\n",
    "    Args:\n",
    "        tree_list (list): list of PyTrees each containing array leaves with leading batch dim.\n",
    "        flatten (bool): whether to reshape leaves to (batch, -1) before stacking.\n",
    "\n",
    "    Returns:\n",
    "        dict mapping canonical leaf path (str) -> array of shape (sum_batches, leaf_size...)\n",
    "\n",
    "    Notes: Requires that all trees have the same leaf paths; raises ValueError otherwise.\n",
    "    \"\"\"\n",
    "    if not tree_list:\n",
    "        return {}\n",
    "    dicts = []\n",
    "    for tree in tree_list:\n",
    "        filtered = eqx.filter(tree, eqx.is_array)\n",
    "        path_leaves, _ = tree_flatten_with_path(filtered)\n",
    "        d = {}\n",
    "        for path, leaf in path_leaves:\n",
    "            if leaf is None:\n",
    "                continue\n",
    "            if flatten:\n",
    "                d[_path_to_str(path)] = jnp.reshape(leaf, (leaf.shape[0], -1))\n",
    "            else:\n",
    "                d[_path_to_str(path)] = leaf\n",
    "        dicts.append(d)\n",
    "    return stack_dicts(dicts)\n",
    "\n",
    "\n",
    "def stack_trees_to_matrix(tree_list, flatten=True):\n",
    "    \"\"\"Convert a list of PyTrees into a single matrix by stacking leaves horizontally then samples vertically.\n",
    "\n",
    "    This is useful for building G matrices for NTK or activation similarity computations.\n",
    "    \"\"\"\n",
    "    stacked = stack_trees_to_dict(tree_list, flatten=flatten)\n",
    "    if not stacked:\n",
    "        return jnp.zeros((0, 0))\n",
    "    keys = sorted(stacked.keys())\n",
    "    return jnp.concatenate([stacked[k] for k in keys], axis=1)\n",
    "\n",
    "def compute_kernel_similarity(K_1, K_2):\n",
    "    \"\"\"Compute similarity between two kernel matrices using Frobenius inner product.\"\"\"\n",
    "    frob_inner_product = np.sum(K_1 * K_2)\n",
    "    frob_norm_1 = np.sqrt(np.sum(K_1 ** 2))\n",
    "    frob_norm_2 = np.sqrt(np.sum(K_2 ** 2))\n",
    "    similarity = 1 - frob_inner_product / (frob_norm_1 * frob_norm_2)\n",
    "    return similarity\n",
    "\n",
    "def compute_per_leaf_kernel_similarity(dict_a, dict_b):\n",
    "    \"\"\"Given two stacked dicts (key -> (N, D) array), compute per-leaf kernel similarity.\n",
    "\n",
    "    Returns a dict key->scalar where scalar is the same metric used by compute_kernel_similarity\n",
    "    applied to K_a = A @ A.T and K_b = B @ B.T.\n",
    "    \"\"\"\n",
    "    keys = sorted(set(dict_a.keys()) & set(dict_b.keys()))\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        A = dict_a[k]\n",
    "        B = dict_b[k]\n",
    "        # Ensure same shapes\n",
    "        if A.shape[0] != B.shape[0]:\n",
    "            raise ValueError(f\"Batch sizes differ for leaf {k}: {A.shape[0]} vs {B.shape[0]}\")\n",
    "        K_a = A @ A.T\n",
    "        K_b = B @ B.T\n",
    "        out[k] = compute_kernel_similarity(K_a, K_b)\n",
    "    return out\n",
    "\n",
    "def compute_per_leaf_cosine_similarity(dict_a, dict_b):\n",
    "    \"\"\"Compute per-leaf cosine similarity by vectorizing each stacked array and computing cosine(v_a, v_b).\n",
    "\n",
    "    Returns dict key -> cosine scalar in [-1,1]. If vectors are zero, returns 0.\n",
    "    \"\"\"\n",
    "    keys = sorted(set(dict_a.keys()) & set(dict_b.keys()))\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        A = dict_a[k]\n",
    "        B = dict_b[k]\n",
    "        if A.shape != B.shape:\n",
    "            raise ValueError(f\"Shapes differ for leaf {k}: {A.shape} vs {B.shape}\")\n",
    "        va = A.ravel()\n",
    "        vb = B.ravel()\n",
    "        na = np.linalg.norm(va)\n",
    "        nb = np.linalg.norm(vb)\n",
    "        if na == 0 or nb == 0:\n",
    "            out[k] = 0.0\n",
    "        else:\n",
    "            inner = np.vdot(va, vb)\n",
    "            cos_val = np.real(inner / (na * nb))  # use real part; or use np.abs(...) for magnitude\n",
    "            out[k] = 1 - cos_val\n",
    "    return out\n",
    "\n",
    "def get_per_leaf_metadata_from_trees(tree_list):\n",
    "    \"\"\"Return dict key -> { 'shape': leaf_shape_without_batch, 'dtype': dtype } from the first tree in the list.\"\"\"\n",
    "    if not tree_list:\n",
    "        return {}\n",
    "    filtered = eqx.filter(tree_list[0], eqx.is_array)\n",
    "    path_leaves, _ = tree_flatten_with_path(filtered)\n",
    "    out = {}\n",
    "    for path, leaf in path_leaves:\n",
    "        if leaf is None:\n",
    "            continue\n",
    "        out[_path_to_str(path)] = {\n",
    "            'shape': tuple(leaf.shape[1:]),\n",
    "            'dtype': str(leaf.dtype),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def mse_single_sample_downstream(model, state, input, target, mask, key, dataset_group_weights=None, skip_timesteps=0):\n",
    "    \"\"\"MSE loss for foundational model (takes dataset_group_idx and mask)\"\"\"\n",
    "    pred, state = model(input, state, key)\n",
    "\n",
    "    # Only evaluate loss on timesteps > skip_timesteps\n",
    "    pred = pred[skip_timesteps:, :]  # Shape: (batch, seq_len - skip_timesteps, output_dim)\n",
    "    target = target[skip_timesteps:, :]  # Shape: (batch, seq_len - skip_timesteps, output_dim)\n",
    "    mask = mask[skip_timesteps:]  # Shape: (batch, seq_len - skip_timesteps)\n",
    "\n",
    "    # Only compute loss on unmasked elements\n",
    "    squared_error = (pred - target) ** 2\n",
    "    mask = mask[..., None]\n",
    "    masked_squared_error = jnp.where(mask, squared_error, 0.0)\n",
    "    \n",
    "    # dataset_group_weights = dataset_group_weights[..., None, None]  # shape (batch, 1, 1) to broadcast\n",
    "    weighted_squared_error = squared_error #* dataset_group_weights\n",
    "        \n",
    "    masked_squared_error = jnp.where(mask, weighted_squared_error, 0.0)\n",
    "    mse = masked_squared_error.sum() / mask.sum()\n",
    "    print(\"Downstream MSE:\", mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f400d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader, max_neural_input = get_brainset_data_loader(    \n",
    "    dataset_args = {\n",
    "        'keep_files_open': False,\n",
    "        'lazy': True,\n",
    "        'config': '/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/dataset/reaching_analysis.yaml'\n",
    "    },\n",
    "    dataloader_args={\n",
    "        'batch_size': 16,\n",
    "        'num_workers': 2,\n",
    "        'persistent_workers': True,\n",
    "    },\n",
    "    sampler = 'TrialSampler',\n",
    "    sampler_args = {\n",
    "        'max_window_length': 5.0\n",
    "    },\n",
    "    data_root = '../' + DATA_ROOT,\n",
    "    prepend_history = 0.3,\n",
    "    sampling_rate = 200,\n",
    "    split = 'train_trial_subsample'\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4eab5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "/tmp/ipykernel_59761/4032630940.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
      "/tmp/ipykernel_59761/4032630940.py:29: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/jax/_src/lax/lax.py:5371: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/jax/_src/lax/lax.py:5371: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
      "/tmp/ipykernel_59761/1788050970.py:160: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  similarity = 1 - frob_inner_product / (frob_norm_1 * frob_norm_2)\n",
      "/tmp/ipykernel_59761/1788050970.py:160: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  similarity = 1 - frob_inner_product / (frob_norm_1 * frob_norm_2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None, 'run_name_postfix': '_normalized'}, 'rng_seed': 42, 'training': {'epochs': 1001, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'SequentialFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'val', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 1024, 'num_workers': 0, 'persistent_workers': False}}, 'dataset_cfg': 'configs/dataset/reaching.yaml', 'train_loader': {'sampler': 'RandomFixedWindowSampler', 'dataset_args': {'lazy': True, 'split': 'train', 'keep_files_open': False}, 'sampler_args': {'drop_short': False, 'window_length': 3.28, 'min_window_length': 0.88}, 'sampling_rate': 200, 'dataloader_args': {'batch_size': 512, 'num_workers': 12, 'persistent_workers': True}}, 'skip_timesteps': 56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "# Collect gradients and activations eqx.nn.inference_mode \n",
    "from tqdm import tqdm\n",
    "\n",
    "layers = [2]\n",
    "skip_timesteps = 0.3 * 200\n",
    "model_cls = SSMFoundationalDecoder\n",
    "models = {}\n",
    "results = []\n",
    "G_list = [] \n",
    "A_list = []\n",
    "loss_fn = mse_single_sample_foundation \n",
    "epochs = list(range(0,501,100))\n",
    "layer_keys = [\"post_encoder\",\"ssm_x\", \"ssm_y\", \"ssm_post_gelu\", \"ssm_post_glu\", \"pre_decoder\"] \n",
    "\n",
    "for l in layers:\n",
    "    checkpoint_name = f'melinajingting-ucl/foundational_ssm_pretrain/l{l}_reaching_normalized_checkpoint'\n",
    "    model_cfg = OmegaConf.load(f'/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/model/l{l}.yaml')\n",
    "    for epoch in epochs:\n",
    "        artifact_full_name = f'{checkpoint_name}:epoch_{epoch}'\n",
    "        model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=model_cls)\n",
    "        models.update({\n",
    "            f'epoch {epoch}':model\n",
    "        })  \n",
    "        \n",
    "        inf_model = eqx.nn.inference_mode(model)\n",
    "        grads_list = []\n",
    "        activations_list = []\n",
    "        for batch in loader:\n",
    "            batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
    "            inputs = batch[\"neural_input\"]\n",
    "            targets = batch[\"behavior_input\"]\n",
    "            mask = batch[\"mask\"]\n",
    "            dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
    "\n",
    "            grads = jax.vmap(eqx.filter_grad(loss_fn), in_axes=(None, None, 0, 0, 0, 0, None, None), axis_name='batch')(model, state, inputs, targets, mask, dataset_group_idxs, jr.PRNGKey(0), skip_timesteps)\n",
    "            grads_list.append(grads)\n",
    "\n",
    "            _, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, 0, None))(inputs, state, dataset_group_idxs, layer_keys)\n",
    "            activations_list.append(activations)\n",
    "\n",
    "        opt, opt_state, lr_scheduler, lr_tree = create_optimizer_and_state(model, optimizer_cfg=model_cfg.optimizer, model_cfg=model_cfg.model, return_lr_tree=True)\n",
    "        lr_leaves = [leaf for leaf in jax.tree_util.tree_leaves(eqx.filter(lr_tree, eqx.is_array)) if leaf is not None]\n",
    "        \n",
    "        G_dict = stack_trees_to_dict(grads_list, flatten=True)\n",
    "        G_dict = jax.device_get(G_dict)           # moves whole PyTree to host\n",
    "        G_dict = {k: np.asarray(v) for k, v in G_dict.items()}\n",
    "\n",
    "        A_dict = stack_dicts(activations_list)\n",
    "        A_dict = jax.device_get(A_dict)\n",
    "        A_dict = {k: np.asarray(v) for k, v in A_dict.items()}\n",
    "\n",
    "        if epoch == 0:\n",
    "            G_dict_0 = G_dict\n",
    "            A_dict_0 = A_dict\n",
    "        G_list.append(G_dict)\n",
    "        A_list.append(A_dict)\n",
    "        \n",
    "        kernel_similarity = compute_per_leaf_kernel_similarity(G_dict, G_dict_0)\n",
    "        activation_similarity = compute_per_leaf_cosine_similarity(A_dict, A_dict_0)\n",
    "        _results = {\n",
    "            'l': l,\n",
    "            'epoch': epoch,\n",
    "            'checkpoint_name': checkpoint_name.split('/')[-1]\n",
    "        }\n",
    "        _results.update(kernel_similarity)\n",
    "        _results.update(activation_similarity)\n",
    "        results.append(_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c46c2",
   "metadata": {},
   "source": [
    "# Downstream task, from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2292ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/jax/_src/lax/lax.py:5371: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59761/1131127135.py:158: RuntimeWarning: overflow encountered in square\n",
      "  frob_norm_1 = np.sqrt(np.sum(K_1 ** 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': True, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n"
     ]
    }
   ],
   "source": [
    "# Collect gradients and activations eqx.nn.inference_mode \n",
    "from tqdm import tqdm\n",
    "from foundational_ssm.utils.downstream_utils import get_rtt_datasets\n",
    "cfg = OmegaConf.create({\n",
    "                        'model':{\n",
    "                            'input_dim': 72,\n",
    "                            'context_dim': 4,\n",
    "                            'ssm_io_dim': 256,\n",
    "                            'ssm_dim': 128,\n",
    "                            'ssm_init_diag_blocks': 4,\n",
    "                            'ssm_num_layers': 2,\n",
    "                            'output_dim': 2,\n",
    "                            'rng_seed': 42,\n",
    "                            'dt_min': 0.001,\n",
    "                            'dt_max': 0.01,\n",
    "                            'dropout_p': 0.03,\n",
    "                            'ssm_dropout_p': 0.01\n",
    "                        },\n",
    "                        'optimizer':{\n",
    "                            'lr': 0.002,\n",
    "                            'weight_decay': 0.01,\n",
    "                            'mode': 'all'\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "dataset_cfg = OmegaConf.create({\n",
    "    'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5',\n",
    "    'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5',\n",
    "    'batch_size': 64,\n",
    "    'phase': 'test',\n",
    "    'skip_timesteps': 60\n",
    "})\n",
    "train_data, val_data, data = get_rtt_datasets(dataset_cfg, jr.PRNGKey(0))\n",
    "loss_fn = mse_single_sample_downstream \n",
    "\n",
    "model_cls = SSMDownstreamDecoder\n",
    "checkpoint_name = 'melinajingting-ucl/foundational_ssm_rtt/l2_scratch_all_checkpoint'\n",
    "models = {}\n",
    "epochs = list(range(0,501,100))\n",
    "train_trial_subset = {k: v[1:65] for k, v in train_data.items()}\n",
    "\n",
    "for epoch in epochs:\n",
    "    artifact_full_name = f'{checkpoint_name}:epoch_{epoch}'\n",
    "    model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=model_cls)\n",
    "    models.update({\n",
    "        f'epoch {epoch}':model\n",
    "    })\n",
    "    \n",
    "    inf_model = eqx.nn.inference_mode(model)\n",
    "    batch = train_trial_subset\n",
    "    inputs = batch[\"neural_input\"]\n",
    "    targets = batch[\"behavior_input\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
    "    grads = jax.vmap(eqx.filter_grad(loss_fn), in_axes=(None, None, 0, 0, 0, None, None), axis_name='batch')(model, state, inputs, targets, mask, jr.PRNGKey(0), skip_timesteps)\n",
    "    opt, opt_state, lr_scheduler, lr_tree = create_optimizer_and_state(model, optimizer_cfg=cfg.optimizer, model_cfg=cfg.model, return_lr_tree=True)\n",
    "    init_out, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(inputs, state, layer_keys)\n",
    "    \n",
    "    G_dict = stack_trees_to_dict([grads], flatten=True)\n",
    "    G_dict = jax.device_get(G_dict)           # moves whole PyTree to host\n",
    "    G_dict = {k: np.asarray(v) for k, v in G_dict.items()}\n",
    "\n",
    "    A_dict = jax.device_get(activations)\n",
    "    A_dict = {k: np.asarray(v) for k, v in A_dict.items()}\n",
    "    \n",
    "    if epoch == 0:\n",
    "        G_dict_0 = G_dict\n",
    "        A_dict_0 = A_dict\n",
    "    kernel_similarity = compute_per_leaf_kernel_similarity(G_dict, G_dict_0)\n",
    "    activation_similarity = compute_per_leaf_cosine_similarity(A_dict, A_dict_0)\n",
    "    _results = {\n",
    "        'l': l,\n",
    "        'epoch': epoch,\n",
    "        'checkpoint_name': checkpoint_name.split('/')[-1]\n",
    "    }\n",
    "    _results.update(kernel_similarity)\n",
    "    _results.update(activation_similarity)\n",
    "    results.append(_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066a743",
   "metadata": {},
   "source": [
    "# Downstream Task, From Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f9c5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59761/1131127135.py:158: RuntimeWarning: overflow encountered in square\n",
      "  frob_norm_1 = np.sqrt(np.sum(K_1 ** 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n",
      "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'input_dim': 130, 'output_dim': 2, 'ssm_io_dim': 256, 'ssm_dropout_p': 0.01, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'downstream', 'decoding', 'rtt'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_rtt'}, 'device': 'cuda', 'dataset': {'test': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_val.h5', 'phase': 'test', 'train': '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized_train.h5', 'batch_size': 64, 'skip_timesteps': 56}, 'rng_seed': 42, 'training': {'epochs': 1001, 'from_scratch': False, 'log_val_every': 100, 'checkpoint_every': 100, 'save_activations': False, 'save_checkpoints': True, 'log_pred_and_activations_every': 999}, 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'optimizer.mode': 'all', 'model.checkpoint': 'melinajingting-ucl/foundational_ssm_pretrain/l2_reaching_normalized_checkpoint:best', 'training.from_scratch': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream MSE: LinearizeTracer<float32[]>\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'melinajingting-ucl/foundational_ssm_rtt/l2_reaching_normalized_all_checkpoint'\n",
    "train_trial_subset = {k: v[1:65] for k, v in train_data.items()}\n",
    "for epoch in epochs:\n",
    "    artifact_full_name = f'{checkpoint_name}:epoch_{epoch}'\n",
    "    model, state, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=model_cls)\n",
    "    batch = train_trial_subset\n",
    "    inputs = batch[\"neural_input\"]\n",
    "    targets = batch[\"behavior_input\"]\n",
    "    mask = batch[\"mask\"]\n",
    "    dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
    "    grads = jax.vmap(eqx.filter_grad(loss_fn), in_axes=(None, None, 0, 0, 0, None, None), axis_name='batch')(model, state, inputs, targets, mask, jr.PRNGKey(0), skip_timesteps)\n",
    "    opt, opt_state, lr_scheduler, lr_tree = create_optimizer_and_state(model, optimizer_cfg=cfg.optimizer, model_cfg=cfg.model, return_lr_tree=True)\n",
    "    inf_model = eqx.nn.inference_mode(model)\n",
    "    init_out, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(inputs, state, layer_keys)\n",
    "    \n",
    "    G_dict = stack_trees_to_dict([grads], flatten=True)\n",
    "    G_dict = jax.device_get(G_dict)           # moves whole PyTree to host\n",
    "    G_dict = {k: np.asarray(v) for k, v in G_dict.items()}\n",
    "\n",
    "    A_dict = jax.device_get(activations)\n",
    "    A_dict = {k: np.asarray(v) for k, v in A_dict.items()}\n",
    "    \n",
    "    if epoch == 0:\n",
    "        G_dict_0 = G_dict\n",
    "        A_dict_0 = A_dict\n",
    "    G_list.append(G_dict)\n",
    "    A_list.append(A_dict)\n",
    "    kernel_similarity = compute_per_leaf_kernel_similarity(G_dict, G_dict_0)\n",
    "    activation_similarity = compute_per_leaf_cosine_similarity(A_dict, A_dict_0)\n",
    "    _results = {\n",
    "        'l': l,\n",
    "        'epoch': epoch,\n",
    "        'checkpoint_name': checkpoint_name.split('/')[-1]\n",
    "    }\n",
    "    _results.update(kernel_similarity)\n",
    "    _results.update(activation_similarity)\n",
    "    results.append(_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e557f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results) \n",
    "# results_df = results_df[results_df['checkpoint_name'] == 'l2_reaching_normalized_checkpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43196f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps: NTK (kernel) and Activations (cosine) without subplots, with publication-friendly labels and encoder index averaging\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = results_df.copy()\n",
    "\n",
    "# Identify columns\n",
    "meta_cols = {'l','epoch'}\n",
    "all_cols = [c for c in df.columns if c not in meta_cols]\n",
    "kernel_cols = [c for c in all_cols if ('.' in c) and ('bias' not in c) and ('encoders.[9]' not in c)]\n",
    "activation_cols = [c for c in all_cols if '.' not in c]\n",
    "\n",
    "epochs_sorted = sorted(df['epoch'].unique())\n",
    "layers_sorted = sorted(df['l'].unique())\n",
    "\n",
    "def _split_path_segments(col: str):\n",
    "    # Split on '.' but keep bracket indices as separate tokens\n",
    "    # e.g., 'encoders.[3].weight' -> ['encoders', '[3]', 'weight']\n",
    "    parts = []\n",
    "    for token in re.split(r'(\\[\\d+\\])|\\.', col):\n",
    "        if token and token != '.':\n",
    "            parts.append(token)\n",
    "    return parts\n",
    "\n",
    "def _collapse_indices(parts):\n",
    "    # Collapse bracket tokens into previous token: ['encoders', '[3]', 'weight'] -> ['encoders[3]', 'weight']\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if re.fullmatch(r'\\[\\d+\\]', p):\n",
    "            if out:\n",
    "                out[-1] = f\"{out[-1]}{p}\"\n",
    "            else:\n",
    "                out.append(p)\n",
    "        else:\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def pretty_kernel_label(col: str, max_depth: int = 3):\n",
    "    parts = _collapse_indices(_split_path_segments(col))\n",
    "    # Keep only the last `max_depth` segments for brevity\n",
    "    if 'glu' in parts and len(parts) > max_depth:\n",
    "        parts = parts[-max_depth+1:]\n",
    "    elif len(parts) > max_depth:\n",
    "        parts = parts[-max_depth:]\n",
    "    # Optional light abbreviations\n",
    "    parts = [p.replace('weight', 'W').replace('kernel', 'K') for p in parts]\n",
    "    return ' · '.join(parts)\n",
    "\n",
    "def _title_case_preserve_acronyms(s: str):\n",
    "    # Title-case tokens but keep common acronyms uppercased\n",
    "    acronyms = {'ssm','io','gelu','glu','rtt','nlb'}\n",
    "    toks = re.split(r'[_\\s]+', s)\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if t.lower() in acronyms:\n",
    "            out.append(t.upper())\n",
    "        else:\n",
    "            # keep single-letter variables (x,y) lower\n",
    "            out.append(t if len(t) == 1 else t.capitalize())\n",
    "    return ' '.join(out)\n",
    "\n",
    "def pretty_activation_label(col: str):\n",
    "    m = re.match(r'^(.*)_(\\d+)$', col)\n",
    "    if m:\n",
    "        base, idx = m.group(1), m.group(2)\n",
    "        base_pretty = _title_case_preserve_acronyms(base)\n",
    "        return f\"{base_pretty} [idx={idx}]\"\n",
    "    # Otherwise just prettify underscores/case\n",
    "    return _title_case_preserve_acronyms(col)\n",
    "\n",
    "def format_row_label(col: str, lval: int, kind: str):\n",
    "    if kind == 'kernel':\n",
    "        comp = pretty_kernel_label(col)\n",
    "    else:\n",
    "        comp = pretty_activation_label(col)\n",
    "    return f\"{comp}\"\n",
    "\n",
    "def _encoder_base_key(col: str) -> str:\n",
    "    # If this is an encoder param, strip numeric indices: encoders.[3].weight -> encoders.[].weight\n",
    "    if re.match(r'^encoders\\.', col):\n",
    "        return re.sub(r'\\[\\d+\\]', '[]', col)\n",
    "    return col\n",
    "\n",
    "def build_heatmap_data(df, columns, kind='kernel'):\n",
    "    rows = []  # list of (row_label, lval, group_cols)\n",
    "    cols_set = set(df.columns)\n",
    "    if kind == 'kernel':\n",
    "        # Group encoder columns by base (index removed) and keep others 1:1\n",
    "        groups = {}  # base_key -> list of original columns\n",
    "        for c in columns:\n",
    "            base = _encoder_base_key(c)\n",
    "            groups.setdefault(base, []).append(c)\n",
    "        for base in sorted(groups.keys()):\n",
    "            present_cols = [c for c in groups[base] if c in cols_set]\n",
    "            if not present_cols:\n",
    "                continue\n",
    "            for lval in layers_sorted:\n",
    "                rows.append((format_row_label(base, lval, kind), lval, present_cols))\n",
    "    else:  # activation\n",
    "        for c in columns:\n",
    "            if c not in cols_set:\n",
    "                continue\n",
    "            for lval in layers_sorted:\n",
    "                rows.append((format_row_label(c, lval, kind), lval, [c]))\n",
    "    # Build matrix\n",
    "    row_labels = [r[0] for r in rows]\n",
    "    M = np.full((len(rows), len(epochs_sorted)), np.nan, dtype=float)\n",
    "    for i, (label, lval, group_cols) in enumerate(rows):\n",
    "        # Subset for the layer once (safe: all group_cols are present)\n",
    "        sub = df[df['l'] == lval][['epoch'] + group_cols]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for j, ep in enumerate(epochs_sorted):\n",
    "            se = sub[sub['epoch'] == ep]\n",
    "            if se.empty:\n",
    "                continue\n",
    "            vals = se[group_cols].to_numpy().astype(float).ravel()\n",
    "            if np.isnan(vals).all():\n",
    "                continue\n",
    "            M[i, j] = np.nanmean(vals)\n",
    "    return row_labels, M\n",
    "\n",
    "def plot_heatmap(row_labels, M, title, cmap='viridis', vmin=None, vmax=None, save_path=None):\n",
    "    # Auto-size figure based on number of rows and columns\n",
    "    nrows = len(row_labels)\n",
    "    ncols = len(epochs_sorted)\n",
    "    height = max(4.0, min(1.0 + 0.35 * nrows, 24.0))\n",
    "    width = max(6.0, min(1.0 + 0.6 * ncols, 24.0))\n",
    "    fig, ax = plt.subplots(figsize=(width, height), dpi=200)\n",
    "    im = ax.imshow(M, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('component')\n",
    "    # Ticks\n",
    "    ax.set_xticks(np.arange(len(epochs_sorted)))\n",
    "    ax.set_xticklabels([str(e) for e in epochs_sorted], rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(len(row_labels)))\n",
    "    ax.set_yticklabels(row_labels, fontsize=7)\n",
    "    ax.grid(False)\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('value')\n",
    "    # Adjust left margin based on max label length for readability\n",
    "    max_len = max((len(s) for s in row_labels), default=0)\n",
    "    left_margin = min(0.55, 0.12 + 0.0065 * max_len)\n",
    "    fig.subplots_adjust(left=left_margin)\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0806f9a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['[0].context_embedding.weight'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m figures_directory = \u001b[33m'\u001b[39m\u001b[33m/cs/student/projects1/ml/2024/mlaimon/UCL-ML-Thesis/Writeup/figures\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Build and plot NTK heatmap\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m kernel_row_labels, kernel_M = \u001b[43mbuild_heatmap_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkernel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m plot_heatmap(kernel_row_labels, kernel_M, title=\u001b[33m'\u001b[39m\u001b[33mEvolution of NTK Matrix similarity during pretraining\u001b[39m\u001b[33m'\u001b[39m, save_path=os.path.join(figures_directory, \u001b[33m'\u001b[39m\u001b[33mntk_heatmap_pretrain.pdf\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Build and plot activation heatmap\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mbuild_heatmap_data\u001b[39m\u001b[34m(df, columns, kind)\u001b[39m\n\u001b[32m    102\u001b[39m M = np.full((\u001b[38;5;28mlen\u001b[39m(rows), \u001b[38;5;28mlen\u001b[39m(epochs_sorted)), np.nan, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (label, lval, group_cols) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rows):\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Subset for the layer once\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     sub = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mlval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sub.empty:\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['[0].context_embedding.weight'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "df = results_df[results_df['checkpoint_name'] == 'l2_reaching_normalized_checkpoint'] \\\n",
    "    .drop(columns=['checkpoint_name']) \\\n",
    "    .dropna(axis=1, how='any')\n",
    "figures_directory = '/cs/student/projects1/ml/2024/mlaimon/UCL-ML-Thesis/Writeup/figures'\n",
    "# Build and plot NTK heatmap\n",
    "kernel_row_labels, kernel_M = build_heatmap_data(df, kernel_cols, kind='kernel')\n",
    "plot_heatmap(kernel_row_labels, kernel_M, title='Evolution of NTK Matrix similarity during pretraining', save_path=os.path.join(figures_directory, 'ntk_heatmap_pretrain.pdf'))\n",
    "\n",
    "# Build and plot activation heatmap\n",
    "act_row_labels, act_M = build_heatmap_data(df, activation_cols, kind='activation')\n",
    "plot_heatmap(act_row_labels, act_M, title=\"Evolution of (1 - cosine similarity of activations) during pretraining\", save_path=os.path.join(figures_directory, 'activation_heatmap_pretrain.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f00b203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['l', 'epoch', 'context_embedding.weight', 'decoder.bias',\n",
       "       'decoder.weight', 'encoders.[0].bias', 'encoders.[0].weight',\n",
       "       'encoders.[1].bias', 'encoders.[1].weight', 'encoders.[2].bias',\n",
       "       'encoders.[2].weight', 'encoders.[3].bias', 'encoders.[3].weight',\n",
       "       'encoders.[4].bias', 'encoders.[4].weight', 'encoders.[5].bias',\n",
       "       'encoders.[5].weight', 'encoders.[6].bias', 'encoders.[6].weight',\n",
       "       'encoders.[7].bias', 'encoders.[7].weight', 'encoders.[8].bias',\n",
       "       'encoders.[8].weight', 'ssm_blocks.[0].glu.w1.bias',\n",
       "       'ssm_blocks.[0].glu.w1.weight', 'ssm_blocks.[0].glu.w2.bias',\n",
       "       'ssm_blocks.[0].glu.w2.weight', 'ssm_blocks.[0].ssm.B',\n",
       "       'ssm_blocks.[0].ssm.C', 'ssm_blocks.[0].ssm.D',\n",
       "       'ssm_blocks.[0].ssm.Lambda_im', 'ssm_blocks.[0].ssm.Lambda_re',\n",
       "       'ssm_blocks.[0].ssm.log_step', 'ssm_blocks.[1].glu.w1.bias',\n",
       "       'ssm_blocks.[1].glu.w1.weight', 'ssm_blocks.[1].glu.w2.bias',\n",
       "       'ssm_blocks.[1].glu.w2.weight', 'ssm_blocks.[1].ssm.B',\n",
       "       'ssm_blocks.[1].ssm.C', 'ssm_blocks.[1].ssm.D',\n",
       "       'ssm_blocks.[1].ssm.Lambda_im', 'ssm_blocks.[1].ssm.Lambda_re',\n",
       "       'ssm_blocks.[1].ssm.log_step', 'post_encoder', 'ssm_post_gelu_0',\n",
       "       'ssm_post_gelu_1', 'ssm_post_glu_0', 'ssm_post_glu_1', 'ssm_x_0',\n",
       "       'ssm_x_1', 'ssm_y_0', 'ssm_y_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77465edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_df[results_df['checkpoint_name'] == 'l2_scra']\n",
    "# Build and plot NTK heatmap\n",
    "kernel_row_labels, kernel_M = build_heatmap_data(df, kernel_cols, kind='kernel')\n",
    "plot_heatmap(kernel_row_labels, kernel_M, title='Evolution of NTK Matrix similarity during pretraining')\n",
    "\n",
    "# Build and plot activation heatmap\n",
    "act_row_labels, act_M = build_heatmap_data(df, activation_cols, kind='activation')\n",
    "plot_heatmap(act_row_labels, act_M, title=\"Evolution of (1 - cosine similarity of activations) during pretraining\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
