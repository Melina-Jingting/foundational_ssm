{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc5371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlb_tools.nwb_interface:Loading /nfs/ghome/live/mlaimon/data/foundational_ssm/motor/raw/000128/sub-Jenkins/sub-Jenkins_ses-full_desc-test_ecephys.nwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/ghome/live/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/nfs/ghome/live/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/nfs/ghome/live/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.11/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "INFO:nlb_tools.nwb_interface:Loading /nfs/ghome/live/mlaimon/data/foundational_ssm/motor/raw/000128/sub-Jenkins/sub-Jenkins_ses-full_desc-train_behavior+ecephys.nwb\n",
      "INFO:nlb_tools.nwb_interface:Resampling data to 5 ms.\n"
     ]
    }
   ],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import (\n",
    "    make_train_input_tensors,\n",
    "    make_eval_input_tensors,\n",
    "    make_eval_target_tensors,\n",
    ")\n",
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from LRU_pytorch import LRU\n",
    "\n",
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = 'mc_maze_small'\n",
    "datapath = '~/data/foundational_ssm/motor/raw/000128/sub-Jenkins/'\n",
    "dataset = NWBDataset(datapath)\n",
    "\n",
    "# Phase and binning\n",
    "phase = 'val'  # 'val' or 'test'\n",
    "bin_width = 5\n",
    "dataset.resample(bin_width)\n",
    "suffix = '' if bin_width == 5 else f'_{int(bin_width)}'\n",
    "\n",
    "# Prepare training data\n",
    "train_split = 'train' if phase == 'val' else ['train', 'val']\n",
    "train_dict = make_train_input_tensors(\n",
    "    dataset, dataset_name=dataset_name, trial_split=train_split, save_file=False\n",
    ")\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']\n",
    "print(\"Train held-in shape:\", train_spikes_heldin.shape)\n",
    "\n",
    "# Prepare evaluation data\n",
    "eval_dict = make_eval_input_tensors(\n",
    "    dataset, dataset_name=dataset_name, trial_split=phase, save_file=False\n",
    ")\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']\n",
    "print(\"Eval dict keys:\", eval_dict.keys())\n",
    "print(\"Eval held-in shape:\", eval_spikes_heldin.shape)\n",
    "\n",
    "# Prepare targets\n",
    "target_dict = make_eval_target_tensors(\n",
    "    dataset,\n",
    "    dataset_name=dataset_name,\n",
    "    train_trial_split='train',\n",
    "    eval_trial_split='val',\n",
    "    include_psth=True,\n",
    "    save_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e902f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "def fit_poisson(train_input, eval_input, train_output, alpha=0.0):\n",
    "    train_pred = []\n",
    "    eval_pred = []\n",
    "    # train Poisson GLM for each output column\n",
    "    for chan in range(train_output.shape[1]):\n",
    "        pr = PoissonRegressor(alpha=alpha, max_iter=500)\n",
    "        pr.fit(train_input, train_output[:, chan])\n",
    "        train_pred.append(pr.predict(train_input))\n",
    "        eval_pred.append(pr.predict(eval_input))\n",
    "    train_pred = np.vstack(train_pred).T\n",
    "    eval_pred = np.vstack(eval_pred).T\n",
    "    return train_pred, eval_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign useful variables\n",
    "tlength = train_spikes_heldin.shape[1]\n",
    "num_train = train_spikes_heldin.shape[0]\n",
    "num_eval = eval_spikes_heldin.shape[0]\n",
    "num_heldin = train_spikes_heldin.shape[2]\n",
    "num_heldout = train_spikes_heldout.shape[2]\n",
    "\n",
    "# Smooth spikes with 40 ms std gaussian\n",
    "import scipy.signal as signal\n",
    "kern_sd_ms = 40\n",
    "kern_sd = int(round(kern_sd_ms / dataset.bin_width))\n",
    "window = signal.gaussian(kern_sd * 6, kern_sd, sym=True)\n",
    "window /= np.sum(window)\n",
    "filt = lambda x: np.convolve(x, window, 'same')\n",
    "\n",
    "train_spksmth_heldin = np.apply_along_axis(filt, 1, train_spikes_heldin)\n",
    "eval_spksmth_heldin = np.apply_along_axis(filt, 1, eval_spikes_heldin)\n",
    "\n",
    "## Generate rate predictions\n",
    "\n",
    "# Reshape data to 2d for regression\n",
    "train_spksmth_heldin_s = train_spksmth_heldin.reshape(-1, train_spksmth_heldin.shape[2])\n",
    "eval_spksmth_heldin_s = eval_spksmth_heldin.reshape(-1, eval_spksmth_heldin.shape[2])\n",
    "train_spikes_heldout_s = train_spikes_heldout.reshape(-1, train_spikes_heldout.shape[2])\n",
    "\n",
    "# Train Poisson regressor from log of held-in smoothed spikes to held-out spikes\n",
    "train_spksmth_heldout_s, eval_spksmth_heldout_s = fit_poisson(\n",
    "    np.log(train_spksmth_heldin_s + 1e-4), # add constant offset to prevent taking log of 0\n",
    "    np.log(eval_spksmth_heldin_s + 1e-4),\n",
    "    train_spikes_heldout_s,\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "# Reshape data back to the same 3d shape as the input arrays\n",
    "train_rates_heldin = train_spksmth_heldin_s.reshape((num_train, tlength, num_heldin))\n",
    "train_rates_heldout = train_spksmth_heldout_s.reshape((num_train, tlength, num_heldout))\n",
    "eval_rates_heldin = eval_spksmth_heldin_s.reshape((num_eval, tlength, num_heldin))\n",
    "eval_rates_heldout = eval_spksmth_heldout_s.reshape((num_eval, tlength, num_heldout))\n",
    "\n",
    "## Prepare submission data\n",
    "\n",
    "output_dict = {\n",
    "    dataset_name + suffix: {\n",
    "        'train_rates_heldin': train_rates_heldin,\n",
    "        'train_rates_heldout': train_rates_heldout,\n",
    "        'eval_rates_heldin': eval_rates_heldin,\n",
    "        'eval_rates_heldout': eval_rates_heldout\n",
    "    }\n",
    "}\n",
    "\n",
    "## Make data to evaluate predictions with\n",
    "\n",
    "# Reset logging level to hide excessive info messages\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# If 'val' phase, make the target data\n",
    "if phase == 'val':\n",
    "    # Note that the RTT task is not well suited to trial averaging, so PSTHs are not made for it\n",
    "    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=True, save_file=False)\n",
    "\n",
    "    # Demonstrate target_dict structure\n",
    "    print(target_dict.keys())\n",
    "    print(target_dict[dataset_name + suffix].keys())\n",
    "\n",
    "# Set logging level again\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "if phase == 'val':\n",
    "    print(evaluate(target_dict, output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d851a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
