{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data leakage check is disabled. Please be absolutely sure that there is no leakage between train and other splits.\n",
      "WARNING:root:Data leakage check is disabled. Please be absolutely sure that there is no leakage between valid and other splits.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Typing\n",
    "from typing import List, Dict\n",
    "\n",
    "# Hydra & config\n",
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# JAX & Equinox\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "from jax.tree_util import tree_map, tree_flatten_with_path\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "# Foundational SSM core imports\n",
    "from foundational_ssm.data_utils import get_train_val_loaders, get_dataset_config\n",
    "from foundational_ssm.models import SSMFoundational\n",
    "from foundational_ssm.loss import CombinedLoss\n",
    "from foundational_ssm.utils import log_model_params_and_grads_wandb, save_model_wandb\n",
    "from foundational_ssm.constants import DATASET_IDX_TO_GROUP_SHORT\n",
    "\n",
    "\n",
    "def create_cosine_annealing_scheduler(initial_lr, total_steps, min_lr=0.0, warmup_steps=0):\n",
    "    \"\"\"\n",
    "    Creates a cosine annealing learning rate scheduler with optional warmup.\n",
    "    \n",
    "    Args:\n",
    "        initial_lr: Initial learning rate\n",
    "        total_steps: Total number of training steps\n",
    "        min_lr: Minimum learning rate (default: 0.0)\n",
    "        warmup_steps: Number of warmup steps (default: 0)\n",
    "        \n",
    "    Returns:\n",
    "        optax scheduler function\n",
    "    \"\"\"\n",
    "    if warmup_steps > 0:\n",
    "        # Warmup followed by cosine annealing\n",
    "        warmup_scheduler = optax.linear_schedule(\n",
    "            init_value=0.0,\n",
    "            end_value=initial_lr,\n",
    "            transition_steps=warmup_steps\n",
    "        )\n",
    "        \n",
    "        cosine_scheduler = optax.cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            decay_steps=total_steps - warmup_steps,\n",
    "            alpha=min_lr / initial_lr  # alpha determines the minimum value\n",
    "        )\n",
    "        \n",
    "        # Combine warmup and cosine annealing\n",
    "        scheduler = optax.join_schedules(\n",
    "            schedules=[warmup_scheduler, cosine_scheduler],\n",
    "            boundaries=[warmup_steps]\n",
    "        )\n",
    "    else:\n",
    "        # Pure cosine annealing\n",
    "        scheduler = optax.cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            decay_steps=total_steps,\n",
    "            alpha=min_lr / initial_lr\n",
    "        )\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "def compute_r2_standard(preds, targets):\n",
    "    \"\"\"\n",
    "    Computes the standard coefficient of determination (R²) for each output dimension.\n",
    "    \n",
    "    Args:\n",
    "        preds: Predictions array of shape (num_samples, output_dim)\n",
    "        targets: Targets array of shape (num_samples, output_dim)\n",
    "        \n",
    "    Returns:\n",
    "        The mean R² across all output dimensions.\n",
    "    \"\"\"\n",
    "    preds_flat = preds.reshape(-1, preds.shape[-1]) \n",
    "    targets_flat = targets.reshape(-1, targets.shape[-1])\n",
    "    ss_res = jnp.sum((targets_flat - preds_flat) ** 2, axis=0) \n",
    "    ss_tot = jnp.sum((targets_flat - jnp.mean(targets_flat, axis=0)) ** 2, axis=0)\n",
    "    zero_variance = ss_tot < 1e-8\n",
    "    r2_per_dim = 1 - ss_res / (ss_tot + 1e-8) # Add epsilon for stability\n",
    "    \n",
    "    return jnp.mean(r2_per_dim)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def predict_batch(model, state, inputs, key):\n",
    "    \"\"\"Predict on a batch of inputs using JAX's vmap\"\"\"\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, _ = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0))(inputs, state, batch_keys)\n",
    "    return preds\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def mse_loss(model_params, model_static, state, inputs, targets, dataset_group_idx, key):\n",
    "    model = eqx.combine(model_params, model_static)\n",
    "    batch_keys = jr.split(key, inputs.shape[0])\n",
    "    preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "    mse = jnp.mean((preds - targets) ** 2)\n",
    "    return (mse, state)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, filter_spec, inputs, targets, dataset_group_idx, loss_fn, opt, opt_state, key):\n",
    "    model_params, model_static = eqx.partition(model, filter_spec)\n",
    "    (value, state), grads = loss_fn(model_params, model_static, state, inputs, targets, dataset_group_idx, key)\n",
    "    updates, opt_state = opt.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state, value, grads\n",
    "\n",
    "\n",
    "cfg = OmegaConf.load(\"../configs/pretrain.yaml\")\n",
    "train_dataset, train_loader, val_dataset, val_loader = get_train_val_loaders(\n",
    "        train_config=get_dataset_config(\n",
    "            cfg.train_dataset.name,\n",
    "            subjects=cfg.train_dataset.subjects\n",
    "        ),\n",
    "        val_config=get_dataset_config(\n",
    "            cfg.val_dataset.name,\n",
    "            subjects=cfg.val_dataset.subjects\n",
    "        ),\n",
    "        batch_size=cfg.train_dataset.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_intervals = train_dataset.get_sampling_intervals()\n",
    "for key in sampling_intervals.keys():\n",
    "    start, end = sampling_intervals[key].start, sampling_intervals[key].end\n",
    "    break\n",
    "sample = train_dataset.get(key, start[0], end[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.000000e-02, 1.079510e+02, 1.273530e+02, 1.465120e+02,\n",
       "       1.795240e+02, 1.887090e+02, 2.156120e+02, 2.396230e+02,\n",
       "       2.559310e+02, 2.839890e+02, 2.965720e+02, 3.076960e+02,\n",
       "       3.229320e+02, 3.565710e+02, 3.652160e+02, 3.747960e+02,\n",
       "       3.871380e+02, 4.740500e+02, 4.998200e+02, 5.317270e+02,\n",
       "       6.112500e+02, 6.210280e+02, 6.593040e+02, 6.945620e+02,\n",
       "       7.935070e+02, 8.018160e+02, 8.563610e+02, 8.912490e+02,\n",
       "       9.141740e+02, 1.006181e+03, 1.016083e+03, 1.061353e+03,\n",
       "       1.130236e+03, 1.141875e+03, 1.150690e+03, 1.169084e+03,\n",
       "       1.198355e+03, 1.231772e+03, 1.241222e+03, 1.278927e+03,\n",
       "       1.292284e+03, 1.301047e+03, 1.310493e+03])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_intervals[key].start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(config_path=\"../configs\", config_name=\"pretrain\", version_base=\"1.3\")\n",
    "def main(cfg: DictConfig):\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_train_val_loaders(\n",
    "        train_config=get_dataset_config(\n",
    "            cfg.train_dataset.name,\n",
    "            subjects=cfg.train_dataset.subjects\n",
    "        ),\n",
    "        val_config=get_dataset_config(\n",
    "            cfg.val_dataset.name,\n",
    "            subjects=cfg.val_dataset.subjects\n",
    "        ),\n",
    "        batch_size=cfg.train_dataset.batch_size\n",
    "    )\n",
    "    \n",
    "    key = jr.PRNGKey(cfg.rng_seed)\n",
    "    model_key, train_key, val_key = jr.split(key, 3)\n",
    "\n",
    "    model = SSMFoundational(\n",
    "            ssm_io_dim = cfg.model.ssm_io_dim,\n",
    "            ssm_dim = cfg.model.ssm_dim,\n",
    "            ssm_init_diag_blocks = cfg.model.ssm_init_diag_blocks,\n",
    "            ssm_num_layers = cfg.model.ssm_num_layers,\n",
    "            output_dim = cfg.model.output_dim,\n",
    "            rng_seed = cfg.model.model_rng_seed,\n",
    "        )\n",
    "    state = eqx.nn.State(model)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    # Filter spec for freezing parts of the model\n",
    "    filter_spec = tree_map(eqx.is_inexact_array, model)\n",
    "    \n",
    "    # Calculate total training steps for scheduler\n",
    "    total_steps = len(train_loader) * cfg.training.epochs\n",
    "    \n",
    "    # Create scheduler based on config\n",
    "    use_cosine_scheduler = getattr(cfg.optimizer, 'use_cosine_scheduler', True)  # Default to True for backward compatibility\n",
    "    \n",
    "    if use_cosine_scheduler:\n",
    "        # Create cosine annealing scheduler\n",
    "        lr_scheduler = create_cosine_annealing_scheduler(\n",
    "            initial_lr=cfg.optimizer.lr,\n",
    "            total_steps=total_steps,\n",
    "            min_lr=getattr(cfg.optimizer, 'min_lr', 0.0),  # Default to 0.0 if not specified\n",
    "            warmup_steps=getattr(cfg.optimizer, 'warmup_steps', 0)  # Default to 0 if not specified\n",
    "        )\n",
    "    else:\n",
    "        # Use constant learning rate\n",
    "        lr_scheduler = lambda step: cfg.optimizer.lr\n",
    "    \n",
    "    # Load JAX optimizer with scheduler\n",
    "    opt = optax.chain(\n",
    "        optax.adamw(learning_rate=lr_scheduler, weight_decay=cfg.optimizer.weight_decay)\n",
    "    )\n",
    "    opt_state = opt.init(eqx.filter(model, filter_spec))\n",
    "    \n",
    "    # Load JAX loss function\n",
    "    loss_fn = mse_loss\n",
    "    \n",
    "    run_name = f\"{cfg.train_dataset.name}_l{cfg.model.ssm_num_layers}_d{cfg.model.ssm_dim}\"\n",
    "    config_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "    wandb.init(project=cfg.wandb_project, name=run_name, config=config_dict)  # type: ignore\n",
    "    \n",
    "    best_r2_score = 0\n",
    "    save_model_wandb(model, run_name, OmegaConf.to_container(cfg.model), wandb.run)\n",
    "    \n",
    "    # Track current step for scheduler\n",
    "    current_step = 0\n",
    "    \n",
    "    for epoch in range(cfg.training.epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[\"neural_input\"]\n",
    "            targets = batch[\"behavior_input\"]\n",
    "            dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "            \n",
    "            key, subkey = jr.split(train_key)\n",
    "            \n",
    "            model, state, opt_state, loss_value, grads = make_step(\n",
    "                model,\n",
    "                state,\n",
    "                filter_spec,\n",
    "                inputs, \n",
    "                targets, \n",
    "                dataset_group_idx,\n",
    "                loss_fn,\n",
    "                opt,\n",
    "                opt_state,  \n",
    "                subkey\n",
    "            )\n",
    "            \n",
    "            # Get current learning rate from scheduler\n",
    "            current_lr = lr_scheduler(current_step)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"loss\": loss_value,\n",
    "                \"learning_rate\": current_lr,\n",
    "                \"step\": current_step\n",
    "            })\n",
    "            epoch_loss += loss_value\n",
    "            current_step += 1\n",
    "            \n",
    "        if epoch % cfg.training.log_every == 0:\n",
    "            wandb.log({\"epoch_train_loss\": epoch_loss})\n",
    "            \n",
    "            total_r2_score = 0\n",
    "            group_preds = defaultdict(list)\n",
    "            group_targets = defaultdict(list)\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[\"neural_input\"]\n",
    "                targets = batch[\"behavior_input\"]\n",
    "                dataset_group_idx = batch[\"dataset_group_idx\"][0]\n",
    "                dataset_group_key = DATASET_IDX_TO_GROUP_SHORT[dataset_group_idx]\n",
    "                \n",
    "                key, subkey = jr.split(val_key)\n",
    "                batch_keys = jr.split(subkey, inputs.shape[0])\n",
    "                preds, state = jax.vmap(model, axis_name=\"batch\", in_axes=(0, None, 0, None), out_axes=(0, None))(inputs, state, batch_keys, dataset_group_idx)\n",
    "                group_preds[dataset_group_key].append(preds)\n",
    "                group_targets[dataset_group_key].append(targets)\n",
    "                \n",
    "            for group_key, preds in group_preds.items():\n",
    "                preds = jnp.concatenate(preds, axis=0)\n",
    "                targets = jnp.concatenate(group_targets[group_key], axis=0)\n",
    "                r2_score = compute_r2_standard(preds, targets)\n",
    "                wandb.log({f\"val_r2_{group_key}\": r2_score, \"epoch_train_loss\": epoch_loss})\n",
    "                total_r2_score += r2_score\n",
    "            avg_r2_score = total_r2_score / len(group_preds)\n",
    "            \n",
    "            if avg_r2_score > best_r2_score:\n",
    "                best_r2_score = avg_r2_score\n",
    "                save_model_wandb(model, run_name, OmegaConf.to_container(cfg.model), wandb.run)\n",
    "            \n",
    "            print(f\"Epoch {epoch}/{cfg.training.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
