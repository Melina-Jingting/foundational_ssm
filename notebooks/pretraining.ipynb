{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546c4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Suppress warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Typing\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# External modules\n",
    "from LRU_pytorch import LRU\n",
    "from temporaldata import Data\n",
    "from torch_brain.nn import InfiniteVocabEmbedding\n",
    "\n",
    "# Hydra/OmegaConf\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Utils (project-specific)\n",
    "sys.path.append(\"../\")\n",
    "from utils.data import get_dataset_config, get_train_val_loaders\n",
    "from utils.preprocessing import bin_spikes\n",
    "from utils.loss import r2_score, move_to_gpu\n",
    "from utils.plotting import plot_training_curves\n",
    "\n",
    "# Device and data root\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = \"/nfs/ghome/live/mlaimon/data/foundational_ssm/motor/processed/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea356a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb:\n",
      "  project: foundational_ssm\n",
      "  run_name: ssm_neural_behavior\n",
      "  tags:\n",
      "  - neural\n",
      "  - behavior\n",
      "  - masking\n",
      "  log_freq: 10\n",
      "dataset:\n",
      "  name: perich_miller_population_2018\n",
      "  subjects:\n",
      "  - j\n",
      "  batch_size: 64\n",
      "model:\n",
      "  num_neural_features: 192\n",
      "  num_behavior_features: 2\n",
      "  num_context_features: 32\n",
      "  embedding_dim: 64\n",
      "  ssm_projection_dim: 64\n",
      "  ssm_hidden_dim: 64\n",
      "  ssm_num_layers: 1\n",
      "  ssm_dropout: 0.1\n",
      "  pred_neural_dim: 192\n",
      "  pred_behavior_dim: 2\n",
      "  sequence_length: 1.0\n",
      "  sampling_rate: 100\n",
      "  lin_dropout: 0.1\n",
      "  activation_fn: relu\n",
      "training:\n",
      "  learning_rate: 0.001\n",
      "  mask_prob: 0.5\n",
      "  num_epochs: 100\n",
      "  neural_weight: 1.0\n",
      "  behavior_weight: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "config = OmegaConf.create({\n",
    "    \"wandb\": {\n",
    "        \"project\": \"foundational_ssm\",\n",
    "        \"run_name\": \"ssm_neural_behavior\",\n",
    "        \"tags\": [\"neural\", \"behavior\", \"masking\"],\n",
    "        \"log_freq\": 1  # Log validation metrics every 10 epochs\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"name\": \"perich_miller_population_2018\",\n",
    "        \"subjects\": [\"j\"],\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_neural_features\": 192,\n",
    "        \"num_behavior_features\": 2,\n",
    "        \"num_context_features\": 32,\n",
    "        \"embedding_dim\": 64,\n",
    "        \"ssm_projection_dim\": 64,\n",
    "        \"ssm_hidden_dim\": 64,\n",
    "        \"ssm_num_layers\": 1,\n",
    "        \"ssm_dropout\": 0.1,\n",
    "        \"pred_neural_dim\": 192,\n",
    "        \"pred_behavior_dim\": 2,\n",
    "        \"sequence_length\": 1.0,\n",
    "        \"sampling_rate\": 100,\n",
    "        \"lin_dropout\": 0.1,\n",
    "        \"activation_fn\": \"relu\"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"mask_prob\": 0.5,\n",
    "        \"num_epochs\": 100,\n",
    "        \"neural_weight\": 1.0,\n",
    "        \"behavior_weight\": 1.0\n",
    "    }\n",
    "})\n",
    "\n",
    "# Print the configuration\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68240b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationMetrics:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def compute_metrics(self, dataloader, model):\n",
    "        model.eval()\n",
    "        \n",
    "        # Initialize overall metrics\n",
    "        metrics = {\n",
    "            \"encoding_loss\": 0.0,  \n",
    "            \"decoding_loss\": 0.0,  \n",
    "            \"combined_loss\": 0.0,\n",
    "            \"behavior_r2\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Initialize per-subject metrics\n",
    "        subject_metrics = {}\n",
    "        for subj_id in model.subject_ids:\n",
    "            subject_metrics[subj_id] = {\n",
    "                \"encoding_loss\": 0.0,\n",
    "                \"decoding_loss\": 0.0,\n",
    "                \"combined_loss\": 0.0,\n",
    "                \"behavior_r2\": 0.0,\n",
    "                \"sample_count\": 0\n",
    "            }\n",
    "        \n",
    "        # Collect predictions for RÂ² calculation\n",
    "        all_behavior_targets = []\n",
    "        all_behavior_preds = []\n",
    "        \n",
    "        # Per-subject predictions and targets\n",
    "        subject_behavior_targets = {subj_id: [] for subj_id in model.subject_ids}\n",
    "        subject_behavior_preds = {subj_id: [] for subj_id in model.subject_ids}\n",
    "        \n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = move_to_gpu(batch, self.device)\n",
    "                batch_size = batch[\"behavior_input\"].shape[0]\n",
    "                \n",
    "                # Forward pass for encoding (neural prediction)\n",
    "                encoding_predictions = model(\n",
    "                    **batch,\n",
    "                    neural_mask=torch.zeros(batch_size, device=self.device),\n",
    "                    behavior_mask=torch.ones(batch_size, device=self.device)\n",
    "                )\n",
    "                \n",
    "                # Forward pass for decoding (behavior prediction)\n",
    "                decoding_predictions = model(\n",
    "                    **batch,\n",
    "                    neural_mask=torch.ones(batch_size, device=self.device),\n",
    "                    behavior_mask=torch.zeros(batch_size, device=self.device)\n",
    "                )\n",
    "                \n",
    "                # Overall metrics\n",
    "                encoding_loss = F.poisson_nll_loss(\n",
    "                    input=encoding_predictions[\"pred_neural\"],\n",
    "                    target=batch[\"neural_input\"],\n",
    "                    log_input=False,\n",
    "                    reduction='mean'\n",
    "                )\n",
    "                \n",
    "                decoding_loss = F.mse_loss(\n",
    "                    input=decoding_predictions[\"pred_behavior\"],\n",
    "                    target=batch[\"behavior_input\"],\n",
    "                    reduction='mean'\n",
    "                )\n",
    "                \n",
    "                combined_loss = encoding_loss + decoding_loss\n",
    "                \n",
    "                metrics[\"encoding_loss\"] += encoding_loss.item()\n",
    "                metrics[\"decoding_loss\"] += decoding_loss.item()\n",
    "                metrics[\"combined_loss\"] += combined_loss.item()\n",
    "                \n",
    "                # Collect overall predictions\n",
    "                all_behavior_targets.append(batch[\"behavior_input\"])\n",
    "                all_behavior_preds.append(decoding_predictions[\"pred_behavior\"])\n",
    "                \n",
    "                # Per-subject metrics\n",
    "                for i, subj_id in enumerate(batch[\"subject_id\"]):\n",
    "                    # Calculate per-subject losses\n",
    "                    subj_encoding_loss = F.poisson_nll_loss(\n",
    "                        input=encoding_predictions[\"pred_neural\"][i:i+1],\n",
    "                        target=batch[\"neural_input\"][i:i+1],\n",
    "                        log_input=False,\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "                    \n",
    "                    subj_decoding_loss = F.mse_loss(\n",
    "                        input=decoding_predictions[\"pred_behavior\"][i:i+1],\n",
    "                        target=batch[\"behavior_input\"][i:i+1],\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "                    \n",
    "                    subject_metrics[subj_id][\"encoding_loss\"] += subj_encoding_loss.item()\n",
    "                    subject_metrics[subj_id][\"decoding_loss\"] += subj_decoding_loss.item()\n",
    "                    subject_metrics[subj_id][\"combined_loss\"] += (subj_encoding_loss + subj_decoding_loss).item()\n",
    "                    subject_metrics[subj_id][\"sample_count\"] += 1\n",
    "                    \n",
    "                    # Store per-subject predictions for R2 calculation\n",
    "                    subject_behavior_targets[subj_id].append(batch[\"behavior_input\"][i:i+1])\n",
    "                    subject_behavior_preds[subj_id].append(decoding_predictions[\"pred_behavior\"][i:i+1])\n",
    "                \n",
    "                num_batches += 1\n",
    "        \n",
    "        # Average overall metrics\n",
    "        for key in [\"encoding_loss\", \"decoding_loss\", \"combined_loss\"]:\n",
    "            metrics[key] /= num_batches if num_batches > 0 else 1\n",
    "            \n",
    "        # Calculate overall behavior RÂ²\n",
    "        behavior_targets = torch.cat(all_behavior_targets)\n",
    "        behavior_preds = torch.cat(all_behavior_preds)\n",
    "        metrics[\"behavior_r2\"] = r2_score(behavior_preds.cpu(), behavior_targets.cpu())\n",
    "        \n",
    "        # Calculate per-subject RÂ² and average per-subject metrics\n",
    "        for subj_id in model.subject_ids:\n",
    "            if subject_metrics[subj_id][\"sample_count\"] > 0:\n",
    "                # Average per-subject losses\n",
    "                count = subject_metrics[subj_id][\"sample_count\"]\n",
    "                for key in [\"encoding_loss\", \"decoding_loss\", \"combined_loss\"]:\n",
    "                    subject_metrics[subj_id][key] /= count\n",
    "                \n",
    "                # Calculate per-subject RÂ²\n",
    "                if len(subject_behavior_targets[subj_id]) > 0:\n",
    "                    subj_targets = torch.cat(subject_behavior_targets[subj_id])\n",
    "                    subj_preds = torch.cat(subject_behavior_preds[subj_id])\n",
    "                    subject_metrics[subj_id][\"behavior_r2\"] = r2_score(\n",
    "                        subj_preds.cpu(), subj_targets.cpu()\n",
    "                    )\n",
    "        \n",
    "        # Add per-subject metrics to the overall metrics\n",
    "        metrics[\"per_subject\"] = subject_metrics\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98866347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, train_loader, val_loader, loss_fn, config):\n",
    "    \n",
    "    # Initialize wandb and validation metrics\n",
    "    wandb.init(\n",
    "        project=config.wandb.project,\n",
    "        name=config.wandb.run_name,\n",
    "        tags=config.wandb.tags,\n",
    "        config=OmegaConf.to_container(config, resolve=True)\n",
    "    )\n",
    "    wandb.watch(model, log=\"all\", log_freq=config.wandb.log_freq)\n",
    "    validator = ValidationMetrics(device)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    train_losses = []\n",
    "    val_metrics_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.training.num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "               \n",
    "\n",
    "        # Training steps\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = move_to_gpu(batch, device)\n",
    "            loss = training_step(batch, model, optimizer, loss_fn, config.training.mask_prob)\n",
    "            train_losses.append(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            wandb.log({\n",
    "                \"batch/loss\": loss.item(),\n",
    "                \"batch/step\": epoch * len(train_loader) + batch_idx\n",
    "            })\n",
    "            \n",
    "        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        wandb.log({\n",
    "            \"train/loss\": avg_epoch_loss,\n",
    "            \"train/epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        # Evaluate on validation set periodically\n",
    "        if (epoch + 1) % config.wandb.log_freq == 0 or epoch == config.training.num_epochs - 1:\n",
    "            val_metrics = validator.compute_metrics(val_loader, model)\n",
    "            val_metrics_history.append(val_metrics)\n",
    "            \n",
    "            # Log overall validation metrics\n",
    "            wandb.log({\n",
    "                \"val/encoding_loss\": val_metrics[\"encoding_loss\"],\n",
    "                \"val/decoding_loss\": val_metrics[\"decoding_loss\"],\n",
    "                \"val/combined_loss\": val_metrics[\"combined_loss\"],\n",
    "                \"val/behavior_r2\": val_metrics[\"behavior_r2\"],\n",
    "                \"val/epoch\": epoch + 1\n",
    "            })\n",
    "            \n",
    "            # Log per-subject metrics\n",
    "            for subj_id, subj_metrics in val_metrics[\"per_subject\"].items():\n",
    "                wandb.log({\n",
    "                    f\"val/subject/{subj_id}/encoding_loss\": subj_metrics[\"encoding_loss\"],\n",
    "                    f\"val/subject/{subj_id}/decoding_loss\": subj_metrics[\"decoding_loss\"],\n",
    "                    f\"val/subject/{subj_id}/combined_loss\": subj_metrics[\"combined_loss\"],\n",
    "                    f\"val/subject/{subj_id}/behavior_r2\": subj_metrics[\"behavior_r2\"],\n",
    "                    \"epoch\": epoch + 1\n",
    "                })\n",
    "            \n",
    "            # Print validation summary\n",
    "            print(f\"Epoch {epoch+1}/{config.training.num_epochs} | \" +\n",
    "                  f\"Train Loss: {avg_epoch_loss:.4f} | \" +\n",
    "                  f\"Val Decoding Loss: {val_metrics['decoding_loss']:.4f} | \" +\n",
    "                  f\"Val Behavior RÂ²: {val_metrics['behavior_r2']:.4f} | \" +\n",
    "                  f\"Val Encoding Loss: {val_metrics['encoding_loss']:.4f}\")\n",
    "            \n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "    return train_losses, val_metrics_history\n",
    "\n",
    "\n",
    "def training_step(batch, model, optimizer, loss_fn, mask_prob=0.5):\n",
    "    \n",
    "    # 1. Prepare the masks\n",
    "    batch_size = batch[\"neural_input\"].shape[0]\n",
    "    device = batch[\"neural_input\"].device\n",
    "    neural_mask = torch.ones(batch_size, device=device)\n",
    "    behavior_mask = torch.ones(batch_size, device=device)\n",
    "    for i in range(batch_size):\n",
    "        mask_type = np.random.choice(['neural', 'behavior', 'none'], p=[mask_prob/2, mask_prob/2, 1-mask_prob])\n",
    "        if mask_type == 'neural':\n",
    "            neural_mask[i] = 0.0\n",
    "        elif mask_type == 'behavior':\n",
    "            behavior_mask[i] = 0.0\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    optimizer.zero_grad()                  \n",
    "    pred = model(\n",
    "        **batch,\n",
    "        neural_mask=neural_mask,\n",
    "        behavior_mask=behavior_mask\n",
    "    )  \n",
    "    target = batch\n",
    "    \n",
    "    # 3. Compute loss\n",
    "    loss = loss_fn(pred, target)         \n",
    "    loss.backward()                     \n",
    "    optimizer.step()                       \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9b06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, neural_weight=1.0, behavior_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.neural_weight = neural_weight\n",
    "        self.behavior_weight = behavior_weight\n",
    "        # For Poisson NLL, predictions should be rates (>=0).\n",
    "        # Targets are counts.\n",
    "        # log_input=False means inputs are rates, not log-rates.\n",
    "        # full=True includes Stirling's approximation for log(target!).\n",
    "        # reduction='mean' averages loss over all elements.\n",
    "\n",
    "    def forward(self, predictions: Dict[str, torch.Tensor], targets: Dict[str, torch.Tensor]):\n",
    "        # Neural Loss: Poisson NLL\n",
    "        # Ensure pred_neural is non-negative if it's not already (e.g., via model's final activation)\n",
    "        # For Poisson NLL, target should be counts (neural_input before normalization if any)\n",
    "        # and prediction should be expected counts (rates).\n",
    "        loss_neural = F.poisson_nll_loss(\n",
    "            input=predictions[\"pred_neural\"],\n",
    "            target=targets[\"neural_input\"]\n",
    "        )\n",
    "\n",
    "        # Behavior Loss: MSE\n",
    "        loss_behavior = F.mse_loss(\n",
    "            input=predictions[\"pred_behavior\"],\n",
    "            target=targets[\"behavior_input\"],\n",
    "            reduction='mean'\n",
    "        )\n",
    "        # print(f\"Loss Neural: {loss_neural.item()}, Loss Behavior: {loss_behavior.item()}\")\n",
    "        combined_loss = (self.neural_weight * loss_neural) + \\\n",
    "                        (self.behavior_weight * loss_behavior)\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df22758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_binned_features_to_global(\n",
    "    session_binned_features,\n",
    "    session_unit_id_strings,\n",
    "    max_global_units=192\n",
    "):\n",
    "    \"\"\"\n",
    "    Map session-specific binned neural features to a global, padded array.\n",
    "\n",
    "    Args:\n",
    "        session_binned_features (np.ndarray): (num_bins, num_session_units)\n",
    "        session_unit_id_strings (list/np.ndarray): Unit ID strings for the session (len = num_session_units)\n",
    "        max_global_units (int): Output array's second dimension size\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (num_bins, max_global_units)\n",
    "    \"\"\"\n",
    "    if not (isinstance(session_binned_features, np.ndarray) and session_binned_features.ndim == 2):\n",
    "        raise ValueError(\"session_binned_features must be 2D np.ndarray\")\n",
    "    if len(session_unit_id_strings) != session_binned_features.shape[1]:\n",
    "        raise ValueError(\"session_unit_id_strings length must match num_session_units\")\n",
    "\n",
    "    global_binned = np.zeros((session_binned_features.shape[0], max_global_units), dtype=session_binned_features.dtype)\n",
    "    for i, unit_str in enumerate(session_unit_id_strings):\n",
    "        m = re.search(r'elec(\\d+)', unit_str)\n",
    "        if m:\n",
    "            idx = int(m.group(1)) - 1\n",
    "            if 0 <= idx < max_global_units:\n",
    "                global_binned[:, idx] = session_binned_features[:, i]\n",
    "    return global_binned\n",
    "\n",
    "class SSMNeuroModel(nn.Module):\n",
    "    \"\"\"\n",
    "    SSM-based model for neural decoding, inspired by the provided diagram.\n",
    "    It processes multiple input modalities (Neural, Behavior, Stimuli, Context),\n",
    "    embeds them, passes them through an SSM core, and then decodes them into\n",
    "    multiple output predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # Input feature dimensions for each modality (before embedding)\n",
    "                 num_neural_features: int,\n",
    "                 num_behavior_features: int,\n",
    "                 num_context_features: int,\n",
    "                 # Embedding dimension (D in the diagram)\n",
    "                 subject_ids: List[str], # List of subject IDs for which the model is initialized\n",
    "                 embedding_dim: int,\n",
    "                 # SSM core dimensions\n",
    "                 ssm_projection_dim: int, # Dimension after initial projection, M in diagram\n",
    "                 ssm_hidden_dim: int,     # Hidden dimension of SSM blocks\n",
    "                 ssm_num_layers: int,\n",
    "                 ssm_dropout: float,\n",
    "                 # Output feature dimensions for each prediction head\n",
    "                 pred_neural_dim: int,\n",
    "                 pred_behavior_dim: int,\n",
    "                 # General params\n",
    "                 sequence_length: float, # Max duration of input sequence (seconds)\n",
    "                 sampling_rate: float = 100, # Hz, e.g., for converting sec to samples\n",
    "                 lin_dropout: float = 0.1,\n",
    "                 activation_fn: str = \"relu\", # Type of activation: \"relu\", \"gelu\", \"tanh\", etc.\n",
    "                 embed_init_scale: float = 0.02,\n",
    "                 bin_size: float = 1e-3, # Size of bins for neural features (seconds)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_neural_features = num_neural_features\n",
    "        self.num_behavior_features = num_behavior_features\n",
    "        self.num_context_features = num_context_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.ssm_projection_dim = ssm_projection_dim\n",
    "        self.ssm_hidden_dim = ssm_hidden_dim\n",
    "        self.sequence_length_sec = sequence_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        # Calculate sequence length in samples (timesteps)\n",
    "        self.num_timesteps = int(sequence_length * sampling_rate)\n",
    "        self.lin_dropout_rate = lin_dropout\n",
    "        self.embed_init_scale = embed_init_scale\n",
    "        self.subject_ids = subject_ids\n",
    "        \n",
    "        # Activation function\n",
    "        if activation_fn == \"relu\": self.activation = nn.ReLU()\n",
    "        elif activation_fn == \"gelu\": self.activation = nn.GELU()\n",
    "        elif activation_fn == \"tanh\": self.activation = nn.Tanh()\n",
    "        else: raise ValueError(f\"Unsupported activation_fn: {activation_fn}\")\n",
    "\n",
    "        # 1. Tokenization Embeddings\n",
    "        self.session_emb = InfiniteVocabEmbedding(embedding_dim=self.num_context_features, \n",
    "                                                  init_scale=self.embed_init_scale)     \n",
    "        \n",
    "        self.unit_emb = InfiniteVocabEmbedding(embedding_dim=self.embedding_dim, init_scale=self.embed_init_scale)\n",
    "\n",
    "\n",
    "        # 2. Subject-Specific Embedders (map modality features to embedding_dim)\n",
    "        self.neural_embedders = nn.ModuleDict({\n",
    "            subj_id: nn.Linear(self.num_neural_features, self.embedding_dim)\n",
    "            for subj_id in self.subject_ids\n",
    "        })\n",
    "        self.behavior_embedders = nn.ModuleDict({\n",
    "            subj_id: nn.Linear(self.num_behavior_features, self.embedding_dim)\n",
    "            for subj_id in self.subject_ids\n",
    "        })\n",
    "        \n",
    "        self.context_embedder = nn.Linear(self.num_context_features, self.embedding_dim)\n",
    "\n",
    "        # Total dimension after concatenating D-dimensional embeddings from 4 modalities\n",
    "        num_active_modalities = 3 # Neural, Behavior, Context (No Stimuli in this version)\n",
    "        concatenated_dim = self.embedding_dim * num_active_modalities\n",
    "        \n",
    "        # 3. Foundational SSM Core\n",
    "        # self.projection_to_ssm_input = nn.Linear(concatenated_dim, ssm_projection_dim)\n",
    "        self.ssm_blocks = nn.ModuleList()\n",
    "        for i in range(ssm_num_layers):\n",
    "            self.ssm_blocks.append(\n",
    "                LRU(\n",
    "                    in_features=concatenated_dim,\n",
    "                    out_features=concatenated_dim,\n",
    "                    state_features=ssm_hidden_dim\n",
    "                )\n",
    "            )\n",
    "        self.ssm_output_dim = concatenated_dim \n",
    "\n",
    "        # 4. Subject-Specific Decoders\n",
    "        self.decoder_neural_modules = nn.ModuleDict({\n",
    "            subj_id: nn.Linear(self.ssm_output_dim, pred_neural_dim)\n",
    "            for subj_id in self.subject_ids\n",
    "        })\n",
    "        self.decoder_behavior_modules = nn.ModuleDict({\n",
    "            subj_id: nn.Linear(self.ssm_output_dim, pred_behavior_dim)\n",
    "            for subj_id in self.subject_ids\n",
    "        })\n",
    "\n",
    "        self.dropout = nn.Dropout(self.lin_dropout_rate)\n",
    "\n",
    "    def forward(self,\n",
    "                neural_input: torch.Tensor, # Shape: (batch, seq_len, num_neural_features)\n",
    "                behavior_input: torch.Tensor, # Shape: (batch, seq_len, num_behavior_features)\n",
    "                session_id: torch.Tensor,  # Shape: (batch)\n",
    "                subject_id: List[str], # Shape: (batch)\n",
    "                neural_mask: Optional[torch.Tensor] = None,\n",
    "                behavior_mask: Optional[torch.Tensor] = None,\n",
    "                ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        if self.unit_emb.is_lazy():\n",
    "            raise ValueError(\n",
    "                \"Unit vocabulary has not been initialized, please use \"\n",
    "                \"`model.unit_emb.initialize_vocab(unit_ids)`\"\n",
    "            )\n",
    "        if self.session_emb.is_lazy():\n",
    "            raise ValueError(\n",
    "                \"Session vocabulary has not been initialized, please use \"\n",
    "                \"`model.session_emb.initialize_vocab(session_ids)`\"\n",
    "            )\n",
    "            \n",
    "        batch_size, seq_len, _ = neural_input.shape\n",
    "        \n",
    "        session_tokens = [self.session_emb.tokenizer(sid) for sid in session_id]\n",
    "        session_embs = torch.stack([\n",
    "            self.session_emb(torch.tensor(tokens).unsqueeze(0)).squeeze(0) \n",
    "            for tokens in session_tokens\n",
    "        ])  \n",
    "        context_input = session_embs.unsqueeze(1).expand(-1, seq_len, -1) # Shape: (batch, seq, embed_dim)\n",
    "        \n",
    "\n",
    "        embedded_neural = []\n",
    "        embedded_behavior = []\n",
    "        \n",
    "        # Apply masks if provided \n",
    "        if neural_mask is not None:\n",
    "            neural_input = neural_input * neural_mask.unsqueeze(1).unsqueeze(2)\n",
    "        if behavior_mask is not None:\n",
    "            behavior_input = behavior_input * behavior_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # 1. Embed inputs\n",
    "        for i in range(batch_size):\n",
    "            subj_id = subject_id[i]\n",
    "            if subj_id not in self.subject_ids:\n",
    "                raise ValueError(f\"Unknown subject_id '{subj_id}' in batch. Model not initialized for this subject.\")\n",
    "\n",
    "            emb_n = self.neural_embedders[subj_id](neural_input[i]) # Shape: (seq, num_neural_features)\n",
    "            embedded_neural.append(self.dropout(self.activation(emb_n)))\n",
    "\n",
    "            emb_b = self.behavior_embedders[subj_id](behavior_input[i]) # Shape: (seq, num_behavior_features)\n",
    "            embedded_behavior.append(self.dropout(self.activation(emb_b)))\n",
    "        \n",
    "        embedded_neural = torch.stack(embedded_neural, dim=0)  # Shape: (batch, seq_len, embedding_dim)\n",
    "        embedded_behavior = torch.stack(embedded_behavior, dim=0)  # Shape: (batch, seq_len, embedding_dim)\n",
    "        embedded_context = self.dropout(self.activation(self.context_embedder(context_input))) # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "    \n",
    "        # 2. Concatenate embeddings\n",
    "        # Shape: (batch, seq_len, embedding_dim * 3)\n",
    "        concatenated_embeddings = torch.cat([\n",
    "            embedded_neural, embedded_behavior, embedded_context\n",
    "        ], dim=-1)\n",
    "\n",
    "        # 3. Pass through Foundational SSM Core\n",
    "        # ssm_core_input = self.dropout(self.activation(self.projection_to_ssm_input(concatenated_embeddings)))\n",
    "        ssm_core_input = concatenated_embeddings # Shape: (batch, seq_len, concatenated_dim)\n",
    "        ssm_layer_output = ssm_core_input\n",
    "        for ssm_block in self.ssm_blocks:\n",
    "            ssm_layer_output = ssm_block(ssm_layer_output) # Adjust if it returns state tuple\n",
    "            if isinstance(ssm_layer_output, tuple): # e.g. LSTM output, hidden\n",
    "                ssm_layer_output = ssm_layer_output[0]\n",
    "            ssm_layer_output = self.dropout(ssm_layer_output) # General dropout after each block processing\n",
    "        final_ssm_output = ssm_layer_output # Shape: (batch, seq_len, self.ssm_output_dim)\n",
    "\n",
    "        # 4. Decode\n",
    "        all_pred_neural = []\n",
    "        all_pred_behavior = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            subj_id = subject_id[i]\n",
    "            ssm_out_sample = final_ssm_output[i] # (seq_len, ssm_output_dim)\n",
    "\n",
    "            pred_n = self.decoder_neural_modules[subj_id](ssm_out_sample)\n",
    "            all_pred_neural.append(pred_n)\n",
    "\n",
    "            pred_b = self.decoder_behavior_modules[subj_id](ssm_out_sample)\n",
    "            all_pred_behavior.append(pred_b)\n",
    "        \n",
    "        predictions = {\n",
    "            \"pred_neural\": torch.stack(all_pred_neural),\n",
    "            \"pred_behavior\": torch.stack(all_pred_behavior),\n",
    "        }\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def tokenize(self, data: Data) -> Dict:\n",
    "        r\"\"\"Tokenizer used to tokenize Data for the POYO model.\n",
    "\n",
    "        This tokenizer can be called as a transform. If you are applying multiple\n",
    "        transforms, make sure to apply this one last.\n",
    "\n",
    "        This code runs on CPU. Do not access GPU tensors inside this function.\n",
    "        \"\"\"\n",
    "        \n",
    "        unit_ids = data.units.id\n",
    "        spikes = data.spikes        \n",
    "        binned_spikes = bin_spikes(\n",
    "            spikes=spikes,\n",
    "            num_units=len(unit_ids),\n",
    "            bin_size= 1 / self.sampling_rate,\n",
    "            num_bins=self.num_timesteps  \n",
    "        ).T\n",
    "        neural_input = map_binned_features_to_global(\n",
    "            session_binned_features=binned_spikes,\n",
    "            session_unit_id_strings=unit_ids,\n",
    "            max_global_units=self.num_neural_features\n",
    "        ) # (N_timesteps, N_global_units)\n",
    "        \n",
    "        behavior_input = data.cursor.vel # (N_timesteps, N_behavior_features)        \n",
    "\n",
    "        data_dict = {\n",
    "            \"neural_input\": torch.tensor(neural_input, dtype=torch.float32),\n",
    "            \"behavior_input\": torch.tensor(behavior_input, dtype=torch.float32),\n",
    "            \"session_id\": data.session.id,\n",
    "            \"subject_id\": data.subject.id\n",
    "        }\n",
    "\n",
    "        return data_dict\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0de744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with config parameters\n",
    "train_dataset, train_loader, val_dataset, val_loader = get_train_val_loaders(\n",
    "    train_config=get_dataset_config(\n",
    "        config.dataset.name,\n",
    "        subjects=config.dataset.subjects\n",
    "    ),\n",
    "    batch_size=config.dataset.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a98fe673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Units in Session: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmelinajingting\u001b[0m (\u001b[33mmelinajingting-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/ghome/live/mlaimon/foundational_ssm/notebooks/wandb/run-20250604_221449-lv4naaa0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/melinajingting-ucl/foundational_ssm/runs/lv4naaa0' target=\"_blank\">ssm_neural_behavior</a></strong> to <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/melinajingting-ucl/foundational_ssm/runs/lv4naaa0' target=\"_blank\">https://wandb.ai/melinajingting-ucl/foundational_ssm/runs/lv4naaa0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print dataset info\n",
    "num_units = len(train_dataset.get_unit_ids())\n",
    "print(f\"Num Units in Session: {num_units}\")\n",
    "\n",
    "# Initialize model with config\n",
    "model = SSMNeuroModel(\n",
    "    num_neural_features=config.model.num_neural_features,\n",
    "    num_behavior_features=config.model.num_behavior_features,\n",
    "    num_context_features=config.model.num_context_features,\n",
    "    embedding_dim=config.model.embedding_dim,\n",
    "    ssm_projection_dim=config.model.ssm_projection_dim,\n",
    "    ssm_hidden_dim=config.model.ssm_hidden_dim,\n",
    "    ssm_num_layers=config.model.ssm_num_layers,\n",
    "    ssm_dropout=config.model.ssm_dropout,\n",
    "    pred_neural_dim=config.model.pred_neural_dim,\n",
    "    pred_behavior_dim=config.model.pred_behavior_dim,\n",
    "    sequence_length=config.model.sequence_length,\n",
    "    sampling_rate=config.model.sampling_rate,\n",
    "    lin_dropout=config.model.lin_dropout,\n",
    "    activation_fn=config.model.activation_fn,\n",
    "    subject_ids=train_dataset.get_subject_ids()\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize vocabularies\n",
    "model.session_emb.initialize_vocab(train_dataset.get_session_ids())\n",
    "model.unit_emb.initialize_vocab(train_dataset.get_unit_ids())\n",
    "\n",
    "# Connect tokenizer to datasets\n",
    "transform = model.tokenize\n",
    "train_dataset.transform = transform\n",
    "val_dataset.transform = transform\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)\n",
    "\n",
    "# Setup loss function\n",
    "loss_fn = CombinedLoss(\n",
    "    neural_weight=config.training.neural_weight,\n",
    "    behavior_weight=config.training.behavior_weight\n",
    ")\n",
    "\n",
    "# Train with wandb logging\n",
    "train_losses, val_metrics_history = train(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_fn, \n",
    "    config\n",
    ")\n",
    "\n",
    "# Plot training curves (original visualization)\n",
    "plot_training_curves(val_metrics_history, train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d24303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, idx in enumerate(train_loader.sampler):\n",
    "    # print(f\"Index {i}: {idx}\")\n",
    "    item = train_dataset[idx]\n",
    "    # print(item[\"neural_input\"].shape, item[\"behavior_input\"].shape, item[\"context_input\"].shape)\n",
    "    model_output = model(\n",
    "        neural_input=item[\"neural_input\"].unsqueeze(0).to(device),\n",
    "        behavior_input=item[\"behavior_input\"].unsqueeze(0).to(device),\n",
    "        session_id=[item[\"session_id\"]],\n",
    "        subject_id=[item[\"subject_id\"]]\n",
    "    )\n",
    "    if i == 0:  # Only print first 11 indices\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e227e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
