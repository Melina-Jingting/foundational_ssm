---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[6], line 29
     27 poyo_tokenize_time = np.zeros(1000)
     28 start_time = time.time()
---> 29 for i, batch in tqdm(enumerate(train_loader)):
     30     poyo_tokenize_time[i] = time.time() - start_time
     31     start_time = time.time()

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/tqdm/std.py:1181, in tqdm.__iter__(self)
   1178 time = self._time
   1180 try:
-> 1181     for obj in iterable:
   1182         yield obj
   1183         # Update and possibly print the progressbar.
   1184         # Note: does not call self.update(1) for speed optimisation.

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733, in _BaseDataLoaderIter.__next__(self)
    730 if self._sampler_iter is None:
    731     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    732     self._reset()  # type: ignore[call-arg]
--> 733 data = self._next_data()
    734 self._num_yielded += 1
    735 if (
    736     self._dataset_kind == _DatasetKind.Iterable
    737     and self._IterableDataset_len_called is not None
    738     and self._num_yielded > self._IterableDataset_len_called
    739 ):

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789, in _SingleProcessDataLoaderIter._next_data(self)
    787 def _next_data(self):
    788     index = self._next_index()  # may raise StopIteration
--> 789     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    790     if self._pin_memory:
    791         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
     50         data = self.dataset.__getitems__(possibly_batched_index)
     51     else:
---> 52         data = [self.dataset[idx] for idx in possibly_batched_index]
     53 else:
     54     data = self.dataset[possibly_batched_index]

File /cs/student/projects1/ml/2024/mlaimon/foundational_ssm/src/foundational_ssm/data_utils/dataset.py:493, in TorchBrainDataset.__getitem__(self, index)
    492 def __getitem__(self, index: DatasetIndex):
--> 493     sample = self.get(index.recording_id, index.start, index.end)
    495     # apply transform
    496     if self.transform is not None:

File /cs/student/projects1/ml/2024/mlaimon/foundational_ssm/src/foundational_ssm/data_utils/dataset.py:326, in TorchBrainDataset.get(self, recording_id, start, end)
    315 r"""This is the main method to extract a slice from a recording. It returns a
    316 Data object that contains all data for recording :obj:`recording_id` between
    317 times :obj:`start` and :obj:`end`.
   (...)    323     end: The end time of the slice.
    324 """
    325 data = self._get_data_object(recording_id)
--> 326 sample = data.slice(start, end)
    328 if self._check_for_data_leakage_flag and self.split is not None:
    329     sample._check_for_data_leakage(self.split)

File /cs/student/projects1/ml/2024/mlaimon/temporaldata/temporaldata/temporaldata.py:2880, in Data.slice(self, start, end, reset_origin)
   2874 for key, value in self.__dict__.items():
   2875     # todo update domain
   2876     if key != "_domain" and (
   2877         isinstance(value, (IrregularTimeSeries, RegularTimeSeries, Interval))
   2878         or (isinstance(value, Data) and value.domain is not None)
   2879     ):
-> 2880         out.__dict__[key] = value.slice(start, end, reset_origin)
   2881     else:
   2882         out.__dict__[key] = copy.copy(value)

File /cs/student/projects1/ml/2024/mlaimon/temporaldata/temporaldata/temporaldata.py:2885, in Data.slice(self, start, end, reset_origin)
   2882         out.__dict__[key] = copy.copy(value)
   2884 # update domain
-> 2885 out._domain = copy.copy(self._domain) & Interval(start, end)
   2886 if reset_origin:
   2887     out._domain.start -= start

File /cs/student/projects1/ml/2024/mlaimon/temporaldata/temporaldata/temporaldata.py:2251, in Interval.__and__(self, other)
   2246 def __and__(self, other):
   2247     """Intersection of two intervals.
   2248     Only start/end times are considered for the intersection,
   2249     and only start/end times are returned in the resulting Interval
   2250     """
-> 2251     if not self.is_disjoint():
   2252         raise ValueError("left Interval object must be disjoint.")
   2253     if not other.is_disjoint():

File /cs/student/projects1/ml/2024/mlaimon/temporaldata/temporaldata/temporaldata.py:1737, in Interval.is_disjoint(self)
   1735         return False
   1736     return tmp_copy.is_disjoint()
-> 1737 return bool(np.all(self.end[:-1] <= self.start[1:]))

File /cs/student/projects1/ml/2024/mlaimon/temporaldata/temporaldata/temporaldata.py:2403, in LazyInterval.__getattribute__(self, name)
   2401     out = out[self._lazy_ops["mask"]]
   2402 if len(self._lazy_ops) == 0:
-> 2403     out = out[:]
   2405 if name in self._unicode_keys:
   2406     # convert back to unicode
   2407     out = out.astype("U")

File h5py/_objects.pyx:56, in h5py._objects.with_phil.wrapper()

File h5py/_objects.pyx:57, in h5py._objects.with_phil.wrapper()

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/h5py/_hl/dataset.py:818, in Dataset.__getitem__(self, args, new_dtype)
    806 """ Read a slice from the HDF5 dataset.
    807 
    808 Takes slices and recarray-style field names (more than one is
   (...)    814 * Boolean "mask" array indexing
    815 """
    816 args = args if isinstance(args, tuple) else (args,)
--> 818 if self._fast_read_ok and (new_dtype is None):
    819     try:
    820         return self._fast_reader.read(args)

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/h5py/_hl/base.py:535, in cached_property.__get__(self, obj, cls)
    532 if obj is None:
    533     return self
--> 535 value = obj.__dict__[self.func.__name__] = self.func(obj)
    536 return value

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/h5py/_hl/dataset.py:800, in Dataset._fast_read_ok(self)
    796 @cached_property
    797 def _fast_read_ok(self):
    798     """Is this dataset suitable for simple reading"""
    799     return (
--> 800         self._extent_type == h5s.SIMPLE
    801         and isinstance(self.id.get_type(), (h5t.TypeIntegerID, h5t.TypeFloatID))
    802     )

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/h5py/_hl/base.py:535, in cached_property.__get__(self, obj, cls)
    532 if obj is None:
    533     return self
--> 535 value = obj.__dict__[self.func.__name__] = self.func(obj)
    536 return value

File h5py/_objects.pyx:56, in h5py._objects.with_phil.wrapper()

File h5py/_objects.pyx:57, in h5py._objects.with_phil.wrapper()

File /cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/foundational_ssm/lib/python3.13/site-packages/h5py/_hl/dataset.py:679, in Dataset._extent_type(self)
    675 @cached_property
    676 @with_phil
    677 def _extent_type(self):
    678     """Get extent type for this dataset - SIMPLE, SCALAR or NULL"""
--> 679     return self.id.get_space().get_simple_extent_type()

File h5py/_objects.pyx:56, in h5py._objects.with_phil.wrapper()

File h5py/_objects.pyx:57, in h5py._objects.with_phil.wrapper()

File h5py/h5d.pyx:374, in h5py.h5d.DatasetID.get_space()

RuntimeError: Unable to synchronously get dataspace (identifier is not of specified type)