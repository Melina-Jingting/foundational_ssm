{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95df258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_brain.data import Dataset, collate, chain\n",
    "from foundational_ssm.data_utils import get_dataset_config\n",
    "from torch_brain.data.sampler import RandomFixedWindowSampler, SequentialFixedWindowSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from foundational_ssm.constants import DATA_ROOT\n",
    "from foundational_ssm.data_utils.samplers import GroupedRandomFixedWindowSampler\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332d34fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data leakage check is disabled. Please be absolutely sure that there is no leakage between None and other splits.\n"
     ]
    }
   ],
   "source": [
    "config_path = \"/cs/student/projects1/ml/2024/mlaimon/foundational_ssm/configs/pretrain.yaml\"\n",
    "cfg = OmegaConf.load(config_path) \n",
    "train_dataset = Dataset(\n",
    "    root=DATA_ROOT,\n",
    "    config=get_dataset_config(\n",
    "        cfg.train_dataset.name,\n",
    "        subjects=cfg.train_dataset.subjects\n",
    "    ),\n",
    "    # split=\"train\",\n",
    ")\n",
    "train_sampling_intervals = train_dataset.get_sampling_intervals()\n",
    "train_dataset.disable_data_leakage_check()\n",
    "\n",
    "from torch_brain.models import POYO\n",
    "from torch_brain.registry import MODALITY_REGISTRY\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any, Dict\n",
    "\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "poyo_model = POYO(\n",
    "    sequence_length=1.0,\n",
    "    readout_spec=MODALITY_REGISTRY['cursor_velocity_2d'],\n",
    "    latent_step=1.0 / 8,\n",
    "    num_latents_per_step=16,\n",
    "    dim=64,\n",
    "    depth=6,\n",
    "    dim_head=64,\n",
    "    cross_heads=2,\n",
    "    self_heads=8,\n",
    ").to(device)\n",
    "poyo_model.unit_emb.initialize_vocab(train_dataset.get_unit_ids())\n",
    "poyo_model.session_emb.initialize_vocab(train_dataset.get_session_ids())\n",
    "\n",
    "\n",
    "# def transform_brainsets_to_fixed_dim_samples(\n",
    "#     data: Any,\n",
    "#     sampling_rate: int = 100,\n",
    "#     sampling_window_ms: int = 1000\n",
    "# ) -> Dict[str, torch.Tensor | str]:\n",
    "#     \"\"\"Convert a *temporaldata* sample to a dictionary of Torch tensors.\n",
    "\n",
    "#     The function takes care of binning & smoothing spikes, cropping/padding neural\n",
    "#     and behavioural features to a globally consistent dimensionality that depends\n",
    "#     on the *(dataset, subject, task)* triple.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data: temporaldata.Data\n",
    "#         Sample returned by **torch-brain**/**temporaldata**.\n",
    "#     sampling_rate: int, default=100\n",
    "#         Target sampling rate *Hz* used for binning.\n",
    "#     sampling_window_ms: int, default=1000   \n",
    "#         Length of the temporal window after binning.\n",
    "#     kern_sd_ms: int, default=20\n",
    "#         Standard deviation of the Gaussian kernel (in ms) for smoothing spikes.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     Dict[str, torch.Tensor]\n",
    "#         Dictionary with keys ``neural_input``, ``behavior_input``, ``session_id``\n",
    "#         and ``subject_id``.\n",
    "#     \"\"\"\n",
    "#     def _ensure_dim(arr: np.ndarray, target_dim: int, pad_value: float = 0.0, *, axis: int = 1) -> np.ndarray:\n",
    "#         \"\"\"\n",
    "#         Crop or pad `arr` along `axis` to match `target_dim`, right-aligning the original data.\n",
    "#         Pads with `pad_value` if needed.\n",
    "#         \"\"\"\n",
    "#         current_dim = arr.shape[axis]\n",
    "        \n",
    "#         # Pad if too small\n",
    "#         shape = list(arr.shape)\n",
    "#         shape[axis] = target_dim\n",
    "#         result = np.full(shape, pad_value, dtype=arr.dtype)\n",
    "        \n",
    "#         # Right-align: place arr at the end along the axis\n",
    "#         idx = [slice(None)] * arr.ndim\n",
    "#         idx[axis] = slice(-current_dim, None)\n",
    "#         result[tuple(idx)] = arr\n",
    "#         return result\n",
    "    \n",
    "#     num_timesteps = int(sampling_rate * sampling_window_ms / 1000)\n",
    "    \n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 1. Bin + smooth spikes\n",
    "#     # ------------------------------------------------------------------\n",
    "#     smoothed_spikes = data.smoothed_spikes.smoothed_spikes\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 2. Prepare behaviour signal (cursor velocity)\n",
    "#     # ------------------------------------------------------------------\n",
    "#     behavior_input = data.cursor.vel  # np.ndarray, (timesteps?, features)\n",
    "\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 3. Align channel dimensions based on (dataset, subject, task)\n",
    "#     # ------------------------------------------------------------------\n",
    "#     smoothed_spikes = _ensure_dim(smoothed_spikes, 353, axis=1)\n",
    "#     behavior_input = _ensure_dim(behavior_input, 2, axis=1)\n",
    "#     smoothed_spikes = _ensure_dim(smoothed_spikes, num_timesteps, axis=0)\n",
    "#     behavior_input = _ensure_dim(behavior_input, num_timesteps, axis=0)\n",
    "#     # smoothed_spikes = smoothed_spikes[:80, :10]\n",
    "#     # behavior_input = behavior_input[:80, :2]\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 4. Pack into torch tensors\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # dataset, subject, task = parse_session_id(data.session.id)\n",
    "#     # group_tuple = (dataset, subject, task)\n",
    "#     # group_idx = DATASET_GROUP_TO_IDX[group_tuple]\n",
    "\n",
    "#     return {\n",
    "#         \"neural_input\": torch.as_tensor(smoothed_spikes, dtype=torch.float32),\n",
    "#         \"behavior_input\": torch.as_tensor(behavior_input, dtype=torch.float32),\n",
    "#         # \"dataset_group_idx\": torch.as_tensor(group_idx, dtype=torch.int32),\n",
    "#     }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3016c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping 107.37806666666742 seconds of data due to short intervals. Remaining: 107778.0 seconds.\n",
      "106it [00:43,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_sampler = RandomFixedWindowSampler(\n",
    "    sampling_intervals=train_sampling_intervals,\n",
    "    window_length=1.0,\n",
    "    # batch_size=256,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "train_dataset.transform = poyo_model.tokenize\n",
    "\n",
    "poyo_tokenize_time = np.zeros(1000)\n",
    "start_time = time.time()\n",
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    poyo_tokenize_time[i] = time.time() - start_time\n",
    "    start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dd276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping 107.37806666666742 seconds of data due to short intervals. Remaining: 107778.0 seconds.\n",
      "106it [01:18,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from foundational_ssm.data_utils.loaders import transform_brainsets_to_fixed_dim_samples_with_binning_and_smoothing, transform_brainsets_to_fixed_dim_samples\n",
    "\n",
    "\n",
    "train_sampler = RandomFixedWindowSampler(\n",
    "    sampling_intervals=train_sampling_intervals,\n",
    "    window_length=1.0,\n",
    "    # batch_size=256,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "train_dataset.transform = transform_brainsets_to_fixed_dim_samples\n",
    "\n",
    "poyo_tokenize_time = np.zeros(1000)\n",
    "start_time = time.time()\n",
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    poyo_tokenize_time[i] = time.time() - start_time\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf55bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping 107.37806666666742 seconds of data due to short intervals. Remaining: 107778.0 seconds.\n",
      "106it [00:42,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from foundational_ssm.data_utils.loaders import transform_brainsets_to_fixed_dim_samples_with_binning_and_smoothing, transform_brainsets_to_fixed_dim_samples\n",
    "\n",
    "\n",
    "train_sampler = RandomFixedWindowSampler(\n",
    "    sampling_intervals=train_sampling_intervals,\n",
    "    window_length=1.0,\n",
    "    # batch_size=256,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "train_dataset.transform = transform_brainsets_to_fixed_dim_samples_with_binning_and_smoothing\n",
    "\n",
    "poyo_tokenize_time = np.zeros(1000)\n",
    "start_time = time.time()\n",
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    poyo_tokenize_time[i] = time.time() - start_time\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3697faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:53,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from foundational_ssm.data_utils.loaders import transform_brainsets_to_fixed_dim_samples_with_binning_and_smoothing, transform_brainsets_to_fixed_dim_samples\n",
    "\n",
    "\n",
    "train_sampler = RandomFixedWindowSampler(\n",
    "    sampling_intervals=train_sampling_intervals,\n",
    "    window_length=1.0,\n",
    "    # batch_size=256,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "train_dataset.transform = transform_brainsets_to_fixed_dim_samples_with_binning_and_smoothing\n",
    "\n",
    "poyo_tokenize_time = np.zeros(1000)\n",
    "start_time = time.time()\n",
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    poyo_tokenize_time[i] = time.time() - start_time\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279001c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational_ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
