{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from foundational_ssm.collate import pad_collate\n",
    "import torch\n",
    "\n",
    "dataset_folder = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/' \n",
    "\n",
    "datasets = [\n",
    "    {'name':'mc_maze', 'subpath':'./000128/sub-Jenkins/'},\n",
    "    {'name':'mc_rtt', 'subpath':'./000129/sub-Indy/'},\n",
    "    {'name':'area2_bump', 'subpath':'./000127/sub-Han/'},\n",
    "    {'name':'dmfc_rsg', 'subpath':'./000130/sub-Haydn/'},\n",
    "]\n",
    "\n",
    "import h5py\n",
    "DTYPE_FLOAT = np.float32 \n",
    "\n",
    "def dict_to_h5(tensor_dict, output_h5_file):\n",
    "    with h5py.File(output_h5_file, 'w') as f:\n",
    "        # Iterate through the items in your 'batch' dictionary\n",
    "        for key, value in tensor_dict.items():\n",
    "            # Convert PyTorch tensor to NumPy array before saving\n",
    "            # Ensure data type consistency for saving\n",
    "            if torch.is_tensor(value):\n",
    "                # For boolean tensors, convert to int8 if you want them to take less space in HDF5\n",
    "                # (np.bool_ usually takes 1 byte, but some systems/HDF5 viewers prefer integer)\n",
    "                if value.dtype == torch.bool:\n",
    "                    data_to_save = value.to(torch.int8).numpy()\n",
    "                else:\n",
    "                    data_to_save = value.numpy()\n",
    "            else:\n",
    "                data_to_save = value # If any value isn't a tensor (e.g., a simple scalar or list)\n",
    "\n",
    "            f.create_dataset(key, data=data_to_save)\n",
    "            print(f\"  - Saved '{key}' with shape {data_to_save.shape} and dtype {data_to_save.dtype}\")\n",
    "\n",
    "        print(f\"Successfully saved all data from 'batch' to {output_h5_file}\")\n",
    "\n",
    "def pad_collate(batch, fixed_seq_len=None):\n",
    "    # Assume batch is a list of dicts with keys: 'neural_input', 'behavior_input', etc.\n",
    "    # Each 'neural_input' is a tensor of shape (timesteps, units)\n",
    "    neural_inputs = [item['neural_input'] for item in batch if item is not None]  # (timesteps, units)\n",
    "    behavioral_inputs = [item['behavior_input'] for item in batch if item is not None]\n",
    "    \n",
    "    # Determine the fixed sequence length\n",
    "    if fixed_seq_len is None:\n",
    "        max_len = max(x.shape[0] for x in neural_inputs)\n",
    "    else:\n",
    "        max_len = fixed_seq_len\n",
    "\n",
    "    # Pad or truncate each sequence to fixed length\n",
    "    def pad_or_truncate(tensor, max_len):\n",
    "        seq_len = tensor.shape[0]\n",
    "        if seq_len == max_len:\n",
    "            return tensor\n",
    "        elif seq_len > max_len:\n",
    "            return tensor[:max_len]\n",
    "        else:\n",
    "            pad_shape = (max_len - seq_len,) + tensor.shape[1:]\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad_tensor], dim=0)\n",
    "\n",
    "    padded_neural = torch.stack([pad_or_truncate(x, max_len) for x in neural_inputs if x is not None])  # (batch, max_len, units)\n",
    "    padded_behavior = torch.stack([pad_or_truncate(x, max_len) for x in behavioral_inputs if x is not None])\n",
    "\n",
    "    # Create mask: 1 for real data, 0 for padding\n",
    "    lengths = [x.shape[0] for x in neural_inputs]\n",
    "    mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :min(l, max_len)] = 1\n",
    "\n",
    "    # Stack other fields (e.g., dataset_group_idx)\n",
    "    dataset_group_idx = torch.stack([item['dataset_group_idx'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'neural_input': padded_neural,\n",
    "        'behavior_input': padded_behavior,\n",
    "        'mask': mask,\n",
    "        'dataset_group_idx': dataset_group_idx,\n",
    "        # add other fields as needed\n",
    "    }\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def build_split_batch(nwb_dataset, split, prepend_duration=299, dataset_group_idx=9, bin_width=5, dtype_neural=np.int8, dtype_behavior=np.float32, train_std: Optional[np.ndarray]=None, behavior_attribute=\"cursor_vel\"):\n",
    "    \"\"\"Build padded batch for a given split.\n",
    "\n",
    "    If split == 'train' and train_std is None, compute per-axis std across all valid behavior timepoints and normalize behavior by it.\n",
    "    If split != 'train' and train_std is provided, normalize behavior by the provided train_std.\n",
    "\n",
    "    Returns a dict with tensors; when train std is computed it is added under key 'behavior_std' (numpy array).\n",
    "    \"\"\"\n",
    "    # Ensure data resampled already if required\n",
    "    if bin_width is not None:\n",
    "        nwb_dataset.resample(bin_width)\n",
    "\n",
    "    # Trial rows for this split\n",
    "    trials = nwb_dataset.trial_info\n",
    "    sel = trials['split'] == split\n",
    "    trials_sel = trials[sel].reset_index(drop=True)\n",
    "    if len(trials_sel) == 0:\n",
    "        return {'neural_input': torch.empty(0), 'behavior_input': torch.empty(0), 'mask': torch.empty(0), 'dataset_group_idx': torch.empty(0)}\n",
    "\n",
    "    # Convert series to numpy arrays once\n",
    "    # adjust these attribute names to match what you actually use\n",
    "    behavior_arr = getattr(nwb_dataset.data, behavior_attribute).to_numpy(dtype=dtype_behavior)    # shape (T, B)\n",
    "    neural_arr   = nwb_dataset.data.spikes.to_numpy(dtype=dtype_neural)         # shape (T, N)\n",
    "    time_index = nwb_dataset.data.index.values\n",
    "\n",
    "    # compute start/end times for each trial (include prepend)\n",
    "    starts = trials_sel['start_time'] - pd.Timedelta(prepend_duration, 'ms')\n",
    "    ends   = trials_sel['end_time']\n",
    "\n",
    "    # Map timestamps to integer indices via searchsorted (faster than repeated .loc)\n",
    "    start_idx = np.searchsorted(time_index, starts.values)\n",
    "    end_idx = np.searchsorted(time_index, ends.values, side='right')\n",
    "\n",
    "    # Clip indices to valid range\n",
    "    T = behavior_arr.shape[0]\n",
    "    start_idx = np.clip(start_idx, 0, T-1)\n",
    "    end_idx = np.clip(end_idx, 1, T)  # end exclusive\n",
    "\n",
    "    lengths = (end_idx - start_idx).astype(int)\n",
    "    max_len = lengths.max()\n",
    "\n",
    "    # dims\n",
    "    beh_dim = behavior_arr.shape[1] if behavior_arr.ndim > 1 else 1\n",
    "    neu_dim = neural_arr.shape[1] if neural_arr.ndim > 1 else 1\n",
    "    n_trials = len(trials_sel)\n",
    "\n",
    "    # Pre-allocate padded numpy arrays\n",
    "    padded_behavior = np.zeros((n_trials, max_len, beh_dim), dtype=behavior_arr.dtype)\n",
    "    padded_neural   = np.zeros((n_trials, max_len, neu_dim), dtype=neural_arr.dtype)\n",
    "    mask = np.zeros((n_trials, max_len), dtype=bool)\n",
    "\n",
    "    # Fill slices (short Python loop; memory copies are contiguous)\n",
    "    for i, (s, e) in enumerate(zip(start_idx, end_idx)):\n",
    "        L = int(e - s)\n",
    "        if L <= 0:\n",
    "            continue\n",
    "        padded_behavior[i, :L, :] = behavior_arr[s:e]\n",
    "        padded_neural[i, :L, :] = neural_arr[s:e]\n",
    "        mask[i, :L] = True\n",
    "\n",
    "    # Compute / apply behavior normalization\n",
    "    behavior_std = None\n",
    "    # Flatten only the valid (masked) timepoints across trials\n",
    "    offset = prepend_duration // bin_width + 1\n",
    "    mask_flat = mask[:, offset:].reshape(-1)\n",
    "    if mask_flat.any():\n",
    "        flat_beh = padded_behavior[:, offset:, ].reshape(-1, beh_dim)\n",
    "        valid_beh = flat_beh[mask_flat]\n",
    "        if split == 'train' and train_std is None:\n",
    "            behavior_std = np.nanstd(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "        elif train_std is not None:\n",
    "            # Use provided train std to normalize (expected shape (beh_dim,))\n",
    "            ts = np.asarray(train_std)\n",
    "            if ts.size != beh_dim:\n",
    "                raise ValueError(f\"train_std length {ts.size} does not match behavior dim {beh_dim}\")\n",
    "            padded_behavior = padded_behavior / ts[None, None, :]\n",
    "        else:\n",
    "            behavior_std = np.std(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "    else:\n",
    "        # No valid samples found; leave data as-is\n",
    "        behavior_std = None\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    t_behavior = torch.from_numpy(padded_behavior)\n",
    "    t_neural = torch.from_numpy(padded_neural)\n",
    "    t_mask = torch.from_numpy(mask)\n",
    "    t_dataset_idx = torch.full((n_trials,), dataset_group_idx, dtype=torch.int8)\n",
    "\n",
    "    # Optionally include trial_idx\n",
    "    t_trial_idx = torch.from_numpy(trials_sel['trial_id'].values.astype(np.int32))\n",
    "\n",
    "    out = {\n",
    "        'neural_input': t_neural,\n",
    "        'behavior_input': t_behavior,\n",
    "        'mask': t_mask,\n",
    "        'dataset_group_idx': t_dataset_idx,\n",
    "        'trial_idx': t_trial_idx\n",
    "    }\n",
    "    if behavior_std is not None:\n",
    "        out['behavior_std'] = behavior_std\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3d9d5",
   "metadata": {},
   "source": [
    "# RTT , Target-based trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d17bca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (378, 768, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (378, 768, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (378, 768) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (378,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (378,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (163, 694, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (163, 694, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (163, 694) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (163,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (163,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [68.90626 67.18307]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "processed_data_folder = os.path.join(dataset_folder, 'processed', 'nlb')\n",
    "processed_data_path = os.path.join(processed_data_folder, dataset_name + '.h5')\n",
    "trial_info_path = os.path.join(processed_data_folder, dataset_name + '.csv')\n",
    "\n",
    "if not os.path.exists(processed_data_folder):\n",
    "    print(f\"Creating directory: {processed_data_folder}\")\n",
    "    os.makedirs(processed_data_folder, exist_ok=True)\n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "\n",
    "# Find when target pos changes\n",
    "has_change = nwb_dataset.data.target_pos.fillna(-1000).diff(axis=0).any(axis=1) # filling NaNs with arbitrary scalar to treat as one block\n",
    "# Find if target pos change corresponds to NaN-padded gap between files\n",
    "change_nan = nwb_dataset.data[has_change].isna().any(axis=1)\n",
    "# Drop trials containing the gap and immediately before and after, as those trials may be cut short\n",
    "drop_trial = (change_nan | change_nan.shift(1, fill_value=True) | change_nan.shift(-1, fill_value=True))[:-1]\n",
    "# Add start and end times to trial info\n",
    "change_times = nwb_dataset.data.index[has_change]\n",
    "start_times = change_times[:-1][~drop_trial]\n",
    "end_times = change_times[1:][~drop_trial]\n",
    "# Get target position per trial\n",
    "start_pos = nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy().tolist()\n",
    "target_pos = nwb_dataset.data.target_pos.loc[start_times].to_numpy().tolist()\n",
    "# Compute reach distance and angle\n",
    "reach_dist = nwb_dataset.data.target_pos.loc[end_times - pd.Timedelta(1, 'ms')].to_numpy() - nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy()\n",
    "reach_angle = np.arctan2(reach_dist[:, 1], reach_dist[:, 0]) / np.pi * 180\n",
    "# Create trial info\n",
    "nwb_dataset.trial_info = pd.DataFrame({\n",
    "    'trial_id': np.arange(len(start_times)),\n",
    "    'start_time': start_times,\n",
    "    'end_time': end_times,\n",
    "    'duration': (end_times - start_times).total_seconds(),\n",
    "    'start_pos': start_pos,\n",
    "    'target_pos': target_pos,\n",
    "    'reach_dist_x': reach_dist[:, 0],\n",
    "    'reach_dist_y': reach_dist[:, 1],\n",
    "    'reach_angle': reach_angle,\n",
    "})\n",
    "nwb_dataset.resample(5)\n",
    "nwb_dataset.trial_info['split'] = 'train'\n",
    "nwb_dataset.trial_info.loc[int(len(nwb_dataset.trial_info)*0.7):, 'split'] = 'val' # 70% train\n",
    "nwb_dataset.trial_info.to_csv('/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized.csv')\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"finger_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"finger_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13552d04",
   "metadata": {},
   "source": [
    "# RTT, Original Trials Prepended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b62d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (810, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (810, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (810, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (810,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (810,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (270, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (270, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (270, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (270,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (270,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"finger_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"finger_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b295e23",
   "metadata": {},
   "source": [
    "# Area2 Bump, Prepended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fa2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (272, 1253, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (272, 1253, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (272, 1253) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (272,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (272,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (92, 1772, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (92, 1772, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (92, 1772) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (92,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (92,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "  - Saved 'mask' with shape (272, 1253) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (272,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (272,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (92, 1772, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (92, 1772, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (92, 1772) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (92,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (92,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [10.381516  9.213864]\n",
      "Train behavior per-axis std: [10.381516  9.213864]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[2]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"hand_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"hand_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f08a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data variance\n",
    "beh = train_batch['behavior_input'][:, prepend_duration // bin_width + 1:, :]\n",
    "mask = train_batch['mask'][:, prepend_duration // bin_width + 1:].reshape(-1)\n",
    "beh = np.array(beh.reshape(-1, beh.shape[-1])[mask])\n",
    "np.std(beh, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8925f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
