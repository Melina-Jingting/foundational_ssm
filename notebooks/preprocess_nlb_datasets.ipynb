{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa3393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from foundational_ssm.collate import pad_collate\n",
    "import torch\n",
    "\n",
    "dataset_folder = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/' \n",
    "\n",
    "datasets = [\n",
    "    {'name':'mc_maze', 'subpath':'./000128/sub-Jenkins/'},\n",
    "    {'name':'mc_rtt', 'subpath':'./000129/sub-Indy/'},\n",
    "    {'name':'area2_bump', 'subpath':'./000127/sub-Han/'},\n",
    "    {'name':'dmfc_rsg', 'subpath':'./000130/sub-Haydn/'},\n",
    "]\n",
    "\n",
    "import h5py\n",
    "DTYPE_FLOAT = np.float32 \n",
    "\n",
    "def dict_to_h5(tensor_dict, output_h5_file):\n",
    "    with h5py.File(output_h5_file, 'w') as f:\n",
    "        # Iterate through the items in your 'batch' dictionary\n",
    "        for key, value in tensor_dict.items():\n",
    "            # Convert PyTorch tensor to NumPy array before saving\n",
    "            # Ensure data type consistency for saving\n",
    "            if torch.is_tensor(value):\n",
    "                # For boolean tensors, convert to int8 if you want them to take less space in HDF5\n",
    "                # (np.bool_ usually takes 1 byte, but some systems/HDF5 viewers prefer integer)\n",
    "                if value.dtype == torch.bool:\n",
    "                    data_to_save = value.to(torch.int8).numpy()\n",
    "                else:\n",
    "                    data_to_save = value.numpy()\n",
    "            else:\n",
    "                data_to_save = value # If any value isn't a tensor (e.g., a simple scalar or list)\n",
    "\n",
    "            f.create_dataset(key, data=data_to_save)\n",
    "            print(f\"  - Saved '{key}' with shape {data_to_save.shape} and dtype {data_to_save.dtype}\")\n",
    "\n",
    "        print(f\"Successfully saved all data from 'batch' to {output_h5_file}\")\n",
    "\n",
    "def pad_collate(batch, fixed_seq_len=None):\n",
    "    # Assume batch is a list of dicts with keys: 'neural_input', 'behavior_input', etc.\n",
    "    # Each 'neural_input' is a tensor of shape (timesteps, units)\n",
    "    neural_inputs = [item['neural_input'] for item in batch if item is not None]  # (timesteps, units)\n",
    "    behavioral_inputs = [item['behavior_input'] for item in batch if item is not None]\n",
    "    \n",
    "    # Determine the fixed sequence length\n",
    "    if fixed_seq_len is None:\n",
    "        max_len = max(x.shape[0] for x in neural_inputs)\n",
    "    else:\n",
    "        max_len = fixed_seq_len\n",
    "\n",
    "    # Pad or truncate each sequence to fixed length\n",
    "    def pad_or_truncate(tensor, max_len):\n",
    "        seq_len = tensor.shape[0]\n",
    "        if seq_len == max_len:\n",
    "            return tensor\n",
    "        elif seq_len > max_len:\n",
    "            return tensor[:max_len]\n",
    "        else:\n",
    "            pad_shape = (max_len - seq_len,) + tensor.shape[1:]\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad_tensor], dim=0)\n",
    "\n",
    "    padded_neural = torch.stack([pad_or_truncate(x, max_len) for x in neural_inputs if x is not None])  # (batch, max_len, units)\n",
    "    padded_behavior = torch.stack([pad_or_truncate(x, max_len) for x in behavioral_inputs if x is not None])\n",
    "\n",
    "    # Create mask: 1 for real data, 0 for padding\n",
    "    lengths = [x.shape[0] for x in neural_inputs]\n",
    "    mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :min(l, max_len)] = 1\n",
    "\n",
    "    # Stack other fields (e.g., dataset_group_idx)\n",
    "    dataset_group_idx = torch.stack([item['dataset_group_idx'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'neural_input': padded_neural,\n",
    "        'behavior_input': padded_behavior,\n",
    "        'mask': mask,\n",
    "        'dataset_group_idx': dataset_group_idx,\n",
    "        # add other fields as needed\n",
    "    }\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def build_split_batch(nwb_dataset, splits, prepend_duration=299, dataset_group_idx=9, bin_width=5, dtype_neural=np.int8, dtype_behavior=np.float32, train_std: Optional[np.ndarray]=None, behavior_attribute=\"cursor_vel\", start_field='start_time', end_field='end_time'):\n",
    "    \"\"\"Build padded batch for a given split.\n",
    "\n",
    "    If split == 'train' and train_std is None, compute per-axis std across all valid behavior timepoints and normalize behavior by it.\n",
    "    If split != 'train' and train_std is provided, normalize behavior by the provided train_std.\n",
    "\n",
    "    Returns a dict with tensors; when train std is computed it is added under key 'behavior_std' (numpy array).\n",
    "    \"\"\"\n",
    "    # Ensure data resampled already if required\n",
    "    if bin_width is not None:\n",
    "        nwb_dataset.resample(bin_width)\n",
    "\n",
    "    # Trial rows for this split\n",
    "    trials = nwb_dataset.trial_info\n",
    "    sel = trials['split'].isin(splits if isinstance(splits, list) else [splits])\n",
    "    trials_sel = trials[sel].reset_index(drop=True)\n",
    "    if len(trials_sel) == 0:\n",
    "        return {'neural_input': torch.empty(0), 'behavior_input': torch.empty(0), 'mask': torch.empty(0), 'dataset_group_idx': torch.empty(0)}\n",
    "\n",
    "    # Convert series to numpy arrays once\n",
    "    # adjust these attribute names to match what you actually use\n",
    "    behavior_arr = getattr(nwb_dataset.data, behavior_attribute).to_numpy(dtype=dtype_behavior)    # shape (T, B)\n",
    "    neural_arr   = nwb_dataset.data.spikes.to_numpy(dtype=dtype_neural)         # shape (T, N)\n",
    "    time_index = nwb_dataset.data.index.values\n",
    "\n",
    "    # compute start/end times for each trial (include prepend)\n",
    "    starts = trials_sel[start_field] - pd.Timedelta(prepend_duration, 'ms')\n",
    "    ends   = trials_sel[end_field]\n",
    "\n",
    "    # Map timestamps to integer indices via searchsorted (faster than repeated .loc)\n",
    "    start_idx = np.searchsorted(time_index, starts.values)\n",
    "    end_idx = np.searchsorted(time_index, ends.values, side='right')\n",
    "\n",
    "    # Clip indices to valid range\n",
    "    T = behavior_arr.shape[0]\n",
    "    start_idx = np.clip(start_idx, 0, T-1)\n",
    "    end_idx = np.clip(end_idx, 1, T)  # end exclusive\n",
    "\n",
    "    lengths = (end_idx - start_idx).astype(int)\n",
    "    max_len = lengths.max()\n",
    "\n",
    "    # dims\n",
    "    beh_dim = behavior_arr.shape[1] if behavior_arr.ndim > 1 else 1\n",
    "    neu_dim = neural_arr.shape[1] if neural_arr.ndim > 1 else 1\n",
    "    n_trials = len(trials_sel)\n",
    "\n",
    "    # Pre-allocate padded numpy arrays\n",
    "    padded_behavior = np.zeros((n_trials, max_len, beh_dim), dtype=behavior_arr.dtype)\n",
    "    padded_neural   = np.zeros((n_trials, max_len, neu_dim), dtype=neural_arr.dtype)\n",
    "    mask = np.zeros((n_trials, max_len), dtype=bool)\n",
    "\n",
    "    # Fill slices (short Python loop; memory copies are contiguous)\n",
    "    for i, (s, e) in enumerate(zip(start_idx, end_idx)):\n",
    "        L = int(e - s)\n",
    "        if L <= 0:\n",
    "            continue\n",
    "        padded_behavior[i, :L, :] = behavior_arr[s:e]\n",
    "        padded_neural[i, :L, :] = neural_arr[s:e]\n",
    "        mask[i, :L] = True\n",
    "\n",
    "    # Compute / apply behavior normalization\n",
    "    behavior_std = None\n",
    "    # Flatten only the valid (masked) timepoints across trials\n",
    "    offset = prepend_duration // bin_width + 1\n",
    "    mask_flat = mask[:, offset:].reshape(-1)\n",
    "    if mask_flat.any():\n",
    "        flat_beh = padded_behavior[:, offset:, ].reshape(-1, beh_dim)\n",
    "        valid_beh = flat_beh[mask_flat]\n",
    "        if len(splits) == 1 and splits[0] == 'train' and train_std is None:\n",
    "            behavior_std = np.nanstd(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "        elif train_std is not None:\n",
    "            # Use provided train std to normalize (expected shape (beh_dim,))\n",
    "            ts = np.asarray(train_std)\n",
    "            if ts.size != beh_dim:\n",
    "                raise ValueError(f\"train_std length {ts.size} does not match behavior dim {beh_dim}\")\n",
    "            padded_behavior = padded_behavior / ts[None, None, :]\n",
    "        else:\n",
    "            behavior_std = np.std(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "    else:\n",
    "        # No valid samples found; leave data as-is\n",
    "        behavior_std = None\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    t_behavior = torch.from_numpy(padded_behavior)\n",
    "    t_neural = torch.from_numpy(padded_neural)\n",
    "    t_mask = torch.from_numpy(mask)\n",
    "    t_dataset_idx = torch.full((n_trials,), dataset_group_idx, dtype=torch.int8)\n",
    "\n",
    "    # Optionally include trial_idx\n",
    "    t_trial_idx = torch.from_numpy(trials_sel['trial_id'].values.astype(np.int32))\n",
    "\n",
    "    out = {\n",
    "        'neural_input': t_neural,\n",
    "        'behavior_input': t_behavior,\n",
    "        'mask': t_mask,\n",
    "        'dataset_group_idx': t_dataset_idx,\n",
    "        'trial_idx': t_trial_idx\n",
    "    }\n",
    "    if behavior_std is not None:\n",
    "        out['behavior_std'] = behavior_std\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3d9d5",
   "metadata": {},
   "source": [
    "# RTT , Target-based trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d17bca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (378, 768, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (378, 768, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (378, 768) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (378,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (378,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (163, 694, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (163, 694, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (163, 694) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (163,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (163,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [68.90626 67.18307]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "processed_data_folder = os.path.join(dataset_folder, 'processed', 'nlb')\n",
    "processed_data_path = os.path.join(processed_data_folder, dataset_name + '.h5')\n",
    "trial_info_path = os.path.join(processed_data_folder, dataset_name + '.csv')\n",
    "\n",
    "if not os.path.exists(processed_data_folder):\n",
    "    print(f\"Creating directory: {processed_data_folder}\")\n",
    "    os.makedirs(processed_data_folder, exist_ok=True)\n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "\n",
    "# Find when target pos changes\n",
    "has_change = nwb_dataset.data.target_pos.fillna(-1000).diff(axis=0).any(axis=1) # filling NaNs with arbitrary scalar to treat as one block\n",
    "# Find if target pos change corresponds to NaN-padded gap between files\n",
    "change_nan = nwb_dataset.data[has_change].isna().any(axis=1)\n",
    "# Drop trials containing the gap and immediately before and after, as those trials may be cut short\n",
    "drop_trial = (change_nan | change_nan.shift(1, fill_value=True) | change_nan.shift(-1, fill_value=True))[:-1]\n",
    "# Add start and end times to trial info\n",
    "change_times = nwb_dataset.data.index[has_change]\n",
    "start_times = change_times[:-1][~drop_trial]\n",
    "end_times = change_times[1:][~drop_trial]\n",
    "# Get target position per trial\n",
    "start_pos = nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy().tolist()\n",
    "target_pos = nwb_dataset.data.target_pos.loc[start_times].to_numpy().tolist()\n",
    "# Compute reach distance and angle\n",
    "reach_dist = nwb_dataset.data.target_pos.loc[end_times - pd.Timedelta(1, 'ms')].to_numpy() - nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy()\n",
    "reach_angle = np.arctan2(reach_dist[:, 1], reach_dist[:, 0]) / np.pi * 180\n",
    "# Create trial info\n",
    "nwb_dataset.trial_info = pd.DataFrame({\n",
    "    'trial_id': np.arange(len(start_times)),\n",
    "    'start_time': start_times,\n",
    "    'end_time': end_times,\n",
    "    'duration': (end_times - start_times).total_seconds(),\n",
    "    'start_pos': start_pos,\n",
    "    'target_pos': target_pos,\n",
    "    'reach_dist_x': reach_dist[:, 0],\n",
    "    'reach_dist_y': reach_dist[:, 1],\n",
    "    'reach_angle': reach_angle,\n",
    "})\n",
    "nwb_dataset.resample(5)\n",
    "nwb_dataset.trial_info['split'] = 'train'\n",
    "nwb_dataset.trial_info.loc[int(len(nwb_dataset.trial_info)*0.7):, 'split'] = 'val' # 70% train\n",
    "nwb_dataset.trial_info.to_csv('/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized.csv')\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"finger_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"finger_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13552d04",
   "metadata": {},
   "source": [
    "# RTT, Original Trials Prepended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b62d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (810, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (810, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (810, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (810,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (810,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (270, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (270, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (270, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (270,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (270,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"finger_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"finger_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b295e23",
   "metadata": {},
   "source": [
    "# Area2 Bump, Prepended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406fa2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (272, 872, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (272, 872, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (272, 872) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (272,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (272,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (92, 644, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (92, 644, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (92, 644) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (92,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (92,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "  - Saved 'neural_input' with shape (364, 872, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (364, 872, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (364, 872) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (364,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (364,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_all.h5\n",
      "Train behavior per-axis std: [16.952703 14.840082]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[2]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "# nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "# bin_width = 5\n",
    "# nwb_dataset.resample(bin_width)\n",
    "# suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "# train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"hand_vel\", start_field='move_onset_time')\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"hand_vel\", start_field='move_onset_time')\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "all_out = build_split_batch(nwb_dataset, ['train','val'], prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"hand_vel\", start_field='move_onset_time')\n",
    "all_batch = {k: v for k, v in all_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "dict_to_h5(all_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_all.h5')\n",
    "\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f08a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data variance\n",
    "beh = train_batch['behavior_input'][:, prepend_duration // bin_width + 1:, :]\n",
    "mask = train_batch['mask'][:, prepend_duration // bin_width + 1:].reshape(-1)\n",
    "beh = np.array(beh.reshape(-1, beh.shape[-1])[mask])\n",
    "np.std(beh, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4c367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>move_onset_time</th>\n",
       "      <th>split</th>\n",
       "      <th>result</th>\n",
       "      <th>ctr_hold</th>\n",
       "      <th>ctr_hold_bump</th>\n",
       "      <th>bump_dir</th>\n",
       "      <th>target_on_time</th>\n",
       "      <th>target_dir</th>\n",
       "      <th>go_cue_time</th>\n",
       "      <th>bump_time</th>\n",
       "      <th>cond_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00.600000</td>\n",
       "      <td>0 days 00:00:00.100000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:00.700000</td>\n",
       "      <td>0 days 00:00:01.300000</td>\n",
       "      <td>0 days 00:00:00.800000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0 days 00:00:01.400000</td>\n",
       "      <td>0 days 00:00:02</td>\n",
       "      <td>0 days 00:00:01.500000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0 days 00:00:02.100000</td>\n",
       "      <td>0 days 00:00:02.700000</td>\n",
       "      <td>0 days 00:00:02.200000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0 days 00:00:02.800000</td>\n",
       "      <td>0 days 00:00:03.400000</td>\n",
       "      <td>0 days 00:00:02.900000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>919</td>\n",
       "      <td>0 days 00:37:55.997000</td>\n",
       "      <td>0 days 00:37:58.982000</td>\n",
       "      <td>0 days 00:37:57.457000</td>\n",
       "      <td>train</td>\n",
       "      <td>R</td>\n",
       "      <td>0.951575</td>\n",
       "      <td>True</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0 days 00:37:58.380000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0 days 00:37:58.381000</td>\n",
       "      <td>0 days 00:37:57.462000</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>920</td>\n",
       "      <td>0 days 00:37:59.485000</td>\n",
       "      <td>0 days 00:38:02.696000</td>\n",
       "      <td>0 days 00:38:02.314000</td>\n",
       "      <td>train</td>\n",
       "      <td>R</td>\n",
       "      <td>1.031749</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:38:02.002000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0 days 00:38:02.003000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>921</td>\n",
       "      <td>0 days 00:38:03.199000</td>\n",
       "      <td>0 days 00:38:05.391000</td>\n",
       "      <td>0 days 00:38:04.291000</td>\n",
       "      <td>train</td>\n",
       "      <td>R</td>\n",
       "      <td>0.585132</td>\n",
       "      <td>True</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0 days 00:38:04.748000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0 days 00:38:04.749000</td>\n",
       "      <td>0 days 00:38:04.276000</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>922</td>\n",
       "      <td>0 days 00:38:05.893000</td>\n",
       "      <td>0 days 00:38:09.100000</td>\n",
       "      <td>0 days 00:38:07.371000</td>\n",
       "      <td>none</td>\n",
       "      <td>I</td>\n",
       "      <td>1.283537</td>\n",
       "      <td>True</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0 days 00:38:08.095000</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0 days 00:38:08.096000</td>\n",
       "      <td>0 days 00:38:07.371000</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>923</td>\n",
       "      <td>0 days 00:38:09.602000</td>\n",
       "      <td>0 days 00:38:11.547000</td>\n",
       "      <td>0 days 00:38:11.181000</td>\n",
       "      <td>train</td>\n",
       "      <td>R</td>\n",
       "      <td>0.597692</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:38:10.903000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0 days 00:38:10.904000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial_id             start_time               end_time  \\\n",
       "0           0        0 days 00:00:00 0 days 00:00:00.600000   \n",
       "1           1 0 days 00:00:00.700000 0 days 00:00:01.300000   \n",
       "2           2 0 days 00:00:01.400000        0 days 00:00:02   \n",
       "3           3 0 days 00:00:02.100000 0 days 00:00:02.700000   \n",
       "4           4 0 days 00:00:02.800000 0 days 00:00:03.400000   \n",
       "..        ...                    ...                    ...   \n",
       "919       919 0 days 00:37:55.997000 0 days 00:37:58.982000   \n",
       "920       920 0 days 00:37:59.485000 0 days 00:38:02.696000   \n",
       "921       921 0 days 00:38:03.199000 0 days 00:38:05.391000   \n",
       "922       922 0 days 00:38:05.893000 0 days 00:38:09.100000   \n",
       "923       923 0 days 00:38:09.602000 0 days 00:38:11.547000   \n",
       "\n",
       "           move_onset_time  split result  ctr_hold ctr_hold_bump  bump_dir  \\\n",
       "0   0 days 00:00:00.100000   test    NaN       NaN           NaN       NaN   \n",
       "1   0 days 00:00:00.800000   test    NaN       NaN           NaN       NaN   \n",
       "2   0 days 00:00:01.500000   test    NaN       NaN           NaN       NaN   \n",
       "3   0 days 00:00:02.200000   test    NaN       NaN           NaN       NaN   \n",
       "4   0 days 00:00:02.900000   test    NaN       NaN           NaN       NaN   \n",
       "..                     ...    ...    ...       ...           ...       ...   \n",
       "919 0 days 00:37:57.457000  train      R  0.951575          True     180.0   \n",
       "920 0 days 00:38:02.314000  train      R  1.031749         False       NaN   \n",
       "921 0 days 00:38:04.291000  train      R  0.585132          True     315.0   \n",
       "922 0 days 00:38:07.371000   none      I  1.283537          True     180.0   \n",
       "923 0 days 00:38:11.181000  train      R  0.597692         False       NaN   \n",
       "\n",
       "            target_on_time  target_dir            go_cue_time  \\\n",
       "0                      NaT         NaN                    NaT   \n",
       "1                      NaT         NaN                    NaT   \n",
       "2                      NaT         NaN                    NaT   \n",
       "3                      NaT         NaN                    NaT   \n",
       "4                      NaT         NaN                    NaT   \n",
       "..                     ...         ...                    ...   \n",
       "919 0 days 00:37:58.380000       135.0 0 days 00:37:58.381000   \n",
       "920 0 days 00:38:02.002000        45.0 0 days 00:38:02.003000   \n",
       "921 0 days 00:38:04.748000        45.0 0 days 00:38:04.749000   \n",
       "922 0 days 00:38:08.095000       225.0 0 days 00:38:08.096000   \n",
       "923 0 days 00:38:10.903000        45.0 0 days 00:38:10.904000   \n",
       "\n",
       "                 bump_time  cond_dir  \n",
       "0                      NaT       NaN  \n",
       "1                      NaT       NaN  \n",
       "2                      NaT       NaN  \n",
       "3                      NaT       NaN  \n",
       "4                      NaT       NaN  \n",
       "..                     ...       ...  \n",
       "919 0 days 00:37:57.462000     180.0  \n",
       "920                    NaT      45.0  \n",
       "921 0 days 00:38:04.276000     315.0  \n",
       "922 0 days 00:38:07.371000     180.0  \n",
       "923                    NaT      45.0  \n",
       "\n",
       "[924 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb_dataset.trial_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4952944",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'heldout_spikes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'heldout_spikes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset_dict = \u001b[43mmake_train_input_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnwb_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_data_path\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_forward_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_behavior\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/nlb_tools/make_tensors.py:303\u001b[39m, in \u001b[36mmake_train_input_tensors\u001b[39m\u001b[34m(dataset, dataset_name, trial_split, update_params, save_file, return_dict, save_path, include_behavior, include_forward_pred, seed)\u001b[39m\n\u001b[32m    300\u001b[39m         make_params[\u001b[33m'\u001b[39m\u001b[33mstart_field\u001b[39m\u001b[33m'\u001b[39m] = align_jit.name\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Make output spiking arrays and put into data_dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m train_dict = \u001b[43mmake_stacked_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspk_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhospk_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m data_dict = {\n\u001b[32m    305\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_spikes_heldin\u001b[39m\u001b[33m'\u001b[39m: train_dict[spk_field],\n\u001b[32m    306\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_spikes_heldout\u001b[39m\u001b[33m'\u001b[39m: train_dict[hospk_field],\n\u001b[32m    307\u001b[39m }\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Add behavior data if necessary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/nlb_tools/make_tensors.py:711\u001b[39m, in \u001b[36mmake_stacked_array\u001b[39m\u001b[34m(dataset, fields, make_params, include_trials)\u001b[39m\n\u001b[32m    709\u001b[39m array_dict = {}\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     array_dict[field] = np.stack(\u001b[43m[\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrouped\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/nlb_tools/make_tensors.py:711\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    709\u001b[39m array_dict = {}\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     array_dict[field] = np.stack([\u001b[43mtrial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m.to_numpy() \u001b[38;5;28;01mfor\u001b[39;00m _, trial \u001b[38;5;129;01min\u001b[39;00m grouped])\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/frame.py:3806\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   3805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3806\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3807\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   3808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/frame.py:3857\u001b[39m, in \u001b[36mDataFrame._getitem_multilevel\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   3856\u001b[39m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3857\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3858\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np.ndarray)):\n\u001b[32m   3859\u001b[39m         new_columns = \u001b[38;5;28mself\u001b[39m.columns[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/multi.py:2916\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key, method)\u001b[39m\n\u001b[32m   2913\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[32m   2915\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[32m   2919\u001b[39m keylen = \u001b[38;5;28mlen\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/multi.py:3263\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3259\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m     idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_loc_single_level_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m level > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lexsort_depth == \u001b[32m0\u001b[39m:\n\u001b[32m   3266\u001b[39m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[32m   3267\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   3268\u001b[39m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/multi.py:2849\u001b[39m, in \u001b[36mMultiIndex._get_loc_single_level_index\u001b[39m\u001b[34m(self, level_index, key)\u001b[39m\n\u001b[32m   2847\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m   2848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2849\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlevel_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m-> \u001b[39m\u001b[32m3804\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3808\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3809\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'heldout_spikes'"
     ]
    }
   ],
   "source": [
    "dataset_dict = make_train_input_tensors(nwb_dataset, dataset_name=dataset_name, trial_split=['train'], save_file=False, save_path='processed_data_path', include_forward_pred=True, include_behavior=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19b98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
