{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from foundational_ssm.collate import pad_collate\n",
    "import torch\n",
    "\n",
    "dataset_folder = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/' \n",
    "\n",
    "datasets = [\n",
    "    {'name':'mc_maze', 'subpath':'./000128/sub-Jenkins/'},\n",
    "    {'name':'mc_rtt', 'subpath':'./000129/sub-Indy/'},\n",
    "    {'name':'area2_bump', 'subpath':'./000127/sub-Han/'},\n",
    "    {'name':'dmfc_rsg', 'subpath':'./000130/sub-Haydn/'},\n",
    "]\n",
    "\n",
    "import h5py\n",
    "DTYPE_FLOAT = np.float32 \n",
    "\n",
    "def dict_to_h5(tensor_dict, output_h5_file):\n",
    "    with h5py.File(output_h5_file, 'w') as f:\n",
    "        # Iterate through the items in your 'batch' dictionary\n",
    "        for key, value in tensor_dict.items():\n",
    "            # Convert PyTorch tensor to NumPy array before saving\n",
    "            # Ensure data type consistency for saving\n",
    "            if torch.is_tensor(value):\n",
    "                # For boolean tensors, convert to int8 if you want them to take less space in HDF5\n",
    "                # (np.bool_ usually takes 1 byte, but some systems/HDF5 viewers prefer integer)\n",
    "                if value.dtype == torch.bool:\n",
    "                    data_to_save = value.to(torch.int8).numpy()\n",
    "                else:\n",
    "                    data_to_save = value.numpy()\n",
    "            else:\n",
    "                data_to_save = value # If any value isn't a tensor (e.g., a simple scalar or list)\n",
    "\n",
    "            f.create_dataset(key, data=data_to_save)\n",
    "            print(f\"  - Saved '{key}' with shape {data_to_save.shape} and dtype {data_to_save.dtype}\")\n",
    "\n",
    "        print(f\"Successfully saved all data from 'batch' to {output_h5_file}\")\n",
    "\n",
    "def pad_collate(batch, fixed_seq_len=None):\n",
    "    # Assume batch is a list of dicts with keys: 'neural_input', 'behavior_input', etc.\n",
    "    # Each 'neural_input' is a tensor of shape (timesteps, units)\n",
    "    neural_inputs = [item['neural_input'] for item in batch if item is not None]  # (timesteps, units)\n",
    "    behavioral_inputs = [item['behavior_input'] for item in batch if item is not None]\n",
    "    \n",
    "    # Determine the fixed sequence length\n",
    "    if fixed_seq_len is None:\n",
    "        max_len = max(x.shape[0] for x in neural_inputs)\n",
    "    else:\n",
    "        max_len = fixed_seq_len\n",
    "\n",
    "    # Pad or truncate each sequence to fixed length\n",
    "    def pad_or_truncate(tensor, max_len):\n",
    "        seq_len = tensor.shape[0]\n",
    "        if seq_len == max_len:\n",
    "            return tensor\n",
    "        elif seq_len > max_len:\n",
    "            return tensor[:max_len]\n",
    "        else:\n",
    "            pad_shape = (max_len - seq_len,) + tensor.shape[1:]\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad_tensor], dim=0)\n",
    "\n",
    "    padded_neural = torch.stack([pad_or_truncate(x, max_len) for x in neural_inputs if x is not None])  # (batch, max_len, units)\n",
    "    padded_behavior = torch.stack([pad_or_truncate(x, max_len) for x in behavioral_inputs if x is not None])\n",
    "\n",
    "    # Create mask: 1 for real data, 0 for padding\n",
    "    lengths = [x.shape[0] for x in neural_inputs]\n",
    "    mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :min(l, max_len)] = 1\n",
    "\n",
    "    # Stack other fields (e.g., dataset_group_idx)\n",
    "    dataset_group_idx = torch.stack([item['dataset_group_idx'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'neural_input': padded_neural,\n",
    "        'behavior_input': padded_behavior,\n",
    "        'mask': mask,\n",
    "        'dataset_group_idx': dataset_group_idx,\n",
    "        # add other fields as needed\n",
    "    }\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def build_split_batch(nwb_dataset, split, prepend_duration=299, dataset_group_idx=9, bin_width=5, dtype_neural=np.int8, dtype_behavior=np.float32, train_std: Optional[np.ndarray]=None, behavior_attribute=\"cursor_vel\"):\n",
    "    \"\"\"Build padded batch for a given split.\n",
    "\n",
    "    If split == 'train' and train_std is None, compute per-axis std across all valid behavior timepoints and normalize behavior by it.\n",
    "    If split != 'train' and train_std is provided, normalize behavior by the provided train_std.\n",
    "\n",
    "    Returns a dict with tensors; when train std is computed it is added under key 'behavior_std' (numpy array).\n",
    "    \"\"\"\n",
    "    # Ensure data resampled already if required\n",
    "    if bin_width is not None:\n",
    "        nwb_dataset.resample(bin_width)\n",
    "\n",
    "    # Trial rows for this split\n",
    "    trials = nwb_dataset.trial_info\n",
    "    sel = trials['split'] == split\n",
    "    trials_sel = trials[sel].reset_index(drop=True)\n",
    "    if len(trials_sel) == 0:\n",
    "        return {'neural_input': torch.empty(0), 'behavior_input': torch.empty(0), 'mask': torch.empty(0), 'dataset_group_idx': torch.empty(0)}\n",
    "\n",
    "    # Convert series to numpy arrays once\n",
    "    # adjust these attribute names to match what you actually use\n",
    "    behavior_arr = getattr(nwb_dataset.data, behavior_attribute).to_numpy(dtype=dtype_behavior)    # shape (T, B)\n",
    "    neural_arr   = nwb_dataset.data.spikes.to_numpy(dtype=dtype_neural)         # shape (T, N)\n",
    "    time_index = nwb_dataset.data.index.values\n",
    "\n",
    "    # compute start/end times for each trial (include prepend)\n",
    "    starts = trials_sel['start_time'] - pd.Timedelta(prepend_duration, 'ms')\n",
    "    ends   = trials_sel['end_time']\n",
    "\n",
    "    # Map timestamps to integer indices via searchsorted (faster than repeated .loc)\n",
    "    start_idx = np.searchsorted(time_index, starts.values)\n",
    "    end_idx = np.searchsorted(time_index, ends.values, side='right')\n",
    "\n",
    "    # Clip indices to valid range\n",
    "    T = behavior_arr.shape[0]\n",
    "    start_idx = np.clip(start_idx, 0, T-1)\n",
    "    end_idx = np.clip(end_idx, 1, T)  # end exclusive\n",
    "\n",
    "    lengths = (end_idx - start_idx).astype(int)\n",
    "    max_len = lengths.max()\n",
    "\n",
    "    # dims\n",
    "    beh_dim = behavior_arr.shape[1] if behavior_arr.ndim > 1 else 1\n",
    "    neu_dim = neural_arr.shape[1] if neural_arr.ndim > 1 else 1\n",
    "    n_trials = len(trials_sel)\n",
    "\n",
    "    # Pre-allocate padded numpy arrays\n",
    "    padded_behavior = np.zeros((n_trials, max_len, beh_dim), dtype=behavior_arr.dtype)\n",
    "    padded_neural   = np.zeros((n_trials, max_len, neu_dim), dtype=neural_arr.dtype)\n",
    "    mask = np.zeros((n_trials, max_len), dtype=bool)\n",
    "\n",
    "    # Fill slices (short Python loop; memory copies are contiguous)\n",
    "    for i, (s, e) in enumerate(zip(start_idx, end_idx)):\n",
    "        L = int(e - s)\n",
    "        if L <= 0:\n",
    "            continue\n",
    "        padded_behavior[i, :L, :] = behavior_arr[s:e]\n",
    "        padded_neural[i, :L, :] = neural_arr[s:e]\n",
    "        mask[i, :L] = True\n",
    "\n",
    "    # Compute / apply behavior normalization\n",
    "    behavior_std = None\n",
    "    # Flatten only the valid (masked) timepoints across trials\n",
    "    offset = prepend_duration // bin_width + 1\n",
    "    mask_flat = mask[:, offset:].reshape(-1)\n",
    "    if mask_flat.any():\n",
    "        flat_beh = padded_behavior[:, offset:, ].reshape(-1, beh_dim)\n",
    "        valid_beh = flat_beh[mask_flat]\n",
    "        if split == 'train' and train_std is None:\n",
    "            behavior_std = np.nanstd(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "        elif train_std is not None:\n",
    "            # Use provided train std to normalize (expected shape (beh_dim,))\n",
    "            ts = np.asarray(train_std)\n",
    "            if ts.size != beh_dim:\n",
    "                raise ValueError(f\"train_std length {ts.size} does not match behavior dim {beh_dim}\")\n",
    "            padded_behavior = padded_behavior / ts[None, None, :]\n",
    "        else:\n",
    "            behavior_std = np.std(valid_beh, axis=0, ddof=0)\n",
    "            padded_behavior = padded_behavior / behavior_std[None, None, :]\n",
    "    else:\n",
    "        # No valid samples found; leave data as-is\n",
    "        behavior_std = None\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    t_behavior = torch.from_numpy(padded_behavior)\n",
    "    t_neural = torch.from_numpy(padded_neural)\n",
    "    t_mask = torch.from_numpy(mask)\n",
    "    t_dataset_idx = torch.full((n_trials,), dataset_group_idx, dtype=torch.int8)\n",
    "\n",
    "    # Optionally include trial_idx\n",
    "    t_trial_idx = torch.from_numpy(trials_sel['trial_id'].values.astype(np.int32))\n",
    "\n",
    "    out = {\n",
    "        'neural_input': t_neural,\n",
    "        'behavior_input': t_behavior,\n",
    "        'mask': t_mask,\n",
    "        'dataset_group_idx': t_dataset_idx,\n",
    "        'trial_idx': t_trial_idx\n",
    "    }\n",
    "    if behavior_std is not None:\n",
    "        out['behavior_std'] = behavior_std\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3d9d5",
   "metadata": {},
   "source": [
    "# RTT , Target-based trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'split'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m train_batch = []\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Build train split, compute train std, then build val split using the train_std\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m train_out = \u001b[43mbuild_split_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnwb_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_duration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_group_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_attribute\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinger_vel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m train_std = train_out.get(\u001b[33m'\u001b[39m\u001b[33mbehavior_std\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Remove behavior_std from the batch dict before saving\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mbuild_split_batch\u001b[39m\u001b[34m(nwb_dataset, split, prepend_duration, dataset_group_idx, bin_width, dtype_neural, dtype_behavior, train_std, behavior_attribute)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Trial rows for this split\u001b[39;00m\n\u001b[32m    104\u001b[39m trials = nwb_dataset.trial_info\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m sel = \u001b[43mtrials\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msplit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m == split\n\u001b[32m    106\u001b[39m trials_sel = trials[sel].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials_sel) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m3807\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   3809\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cs/student/projects1/ml/2024/mlaimon/anaconda3/envs/preprocessing_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m-> \u001b[39m\u001b[32m3804\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3808\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3809\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "processed_data_folder = os.path.join(dataset_folder, 'processed', 'nlb')\n",
    "processed_data_path = os.path.join(processed_data_folder, dataset_name + '.h5')\n",
    "trial_info_path = os.path.join(processed_data_folder, dataset_name + '.csv')\n",
    "\n",
    "if not os.path.exists(processed_data_folder):\n",
    "    print(f\"Creating directory: {processed_data_folder}\")\n",
    "    os.makedirs(processed_data_folder, exist_ok=True)\n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "\n",
    "# Find when target pos changes\n",
    "has_change = nwb_dataset.data.target_pos.fillna(-1000).diff(axis=0).any(axis=1) # filling NaNs with arbitrary scalar to treat as one block\n",
    "# Find if target pos change corresponds to NaN-padded gap between files\n",
    "change_nan = nwb_dataset.data[has_change].isna().any(axis=1)\n",
    "# Drop trials containing the gap and immediately before and after, as those trials may be cut short\n",
    "drop_trial = (change_nan | change_nan.shift(1, fill_value=True) | change_nan.shift(-1, fill_value=True))[:-1]\n",
    "# Add start and end times to trial info\n",
    "change_times = nwb_dataset.data.index[has_change]\n",
    "start_times = change_times[:-1][~drop_trial]\n",
    "end_times = change_times[1:][~drop_trial]\n",
    "# Get target position per trial\n",
    "start_pos = nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy().tolist()\n",
    "target_pos = nwb_dataset.data.target_pos.loc[start_times].to_numpy().tolist()\n",
    "# Compute reach distance and angle\n",
    "reach_dist = nwb_dataset.data.target_pos.loc[end_times - pd.Timedelta(1, 'ms')].to_numpy() - nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy()\n",
    "reach_angle = np.arctan2(reach_dist[:, 1], reach_dist[:, 0]) / np.pi * 180\n",
    "# Create trial info\n",
    "nwb_dataset.trial_info = pd.DataFrame({\n",
    "    'trial_id': np.arange(len(start_times)),\n",
    "    'start_time': start_times,\n",
    "    'end_time': end_times,\n",
    "    'duration': (end_times - start_times).total_seconds(),\n",
    "    'start_pos': start_pos,\n",
    "    'target_pos': target_pos,\n",
    "    'reach_dist_x': reach_dist[:, 0],\n",
    "    'reach_dist_y': reach_dist[:, 1],\n",
    "    'reach_angle': reach_angle,\n",
    "})\n",
    "nwb_dataset.resample(5)\n",
    "nwb_dataset.trial_info['split'] = 'train'\n",
    "nwb_dataset.trial_info.loc[int(len(nwb_dataset.trial_info)*0.7):, 'split'] = 'val' # 70% train\n",
    "nwb_dataset.trial_info.to_csv('/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized.csv')\n",
    "\n",
    "    \n",
    "batch = []\n",
    "max_len = 0\n",
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "for i, row in nwb_dataset.trial_info.iterrows():\n",
    "    start = row['start_time'] - pd.Timedelta(prepend_duration, 'ms')\n",
    "    end = row['end_time']\n",
    "    # Load data slices. This is where memory efficiency of nwb_dataset is key.\n",
    "    behavior_data = nwb_dataset.data.finger_vel[start:end].to_numpy()\n",
    "    neural_data = nwb_dataset.data.spikes[start:end].to_numpy(dtype=np.int8)\n",
    "    cursor_pos = nwb_dataset.data.cursor_pos[start:end].to_numpy()\n",
    "\n",
    "    # Convert to torch tensors with desired dtype immediately.\n",
    "    batch.append({\n",
    "        'behavior_input': torch.from_numpy(behavior_data),\n",
    "        'neural_input': torch.from_numpy(neural_data),\n",
    "        'cursor_pos': torch.from_numpy(cursor_pos),\n",
    "        'dataset_group_idx': torch.tensor(9, dtype=torch.int8) # Ensure it's a torch.Tensor\n",
    "    })\n",
    "    if max_len < behavior_data.shape[0]:\n",
    "        max_len = behavior_data.shape[0]\n",
    "    \n",
    "batch = pad_collate(batch)\n",
    "train_proportion = 0.7\n",
    "train_data = {k: v[:int(len(v) * train_proportion)] for k, v in batch.items()}\n",
    "val_data = {k: v[int(len(v) * train_proportion):] for k, v in batch.items()}\n",
    "data_dir = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb'\n",
    "train_h5_file = os.path.join(data_dir, 'mc_rtt_trialized_train.h5')\n",
    "val_h5_file = os.path.join(data_dir, 'mc_rtt_trialized_val.h5')\n",
    "h5_file = os.path.join(data_dir, 'mc_rtt_trialized.h5')\n",
    "dict_to_h5(train_data, train_h5_file)\n",
    "dict_to_h5(val_data, val_h5_file)\n",
    "dict_to_h5(batch, h5_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13552d04",
   "metadata": {},
   "source": [
    "# RTT, Original Trials Prepended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b62d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (810, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (810, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (810, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (810,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (810,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (270, 180, 130) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (270, 180, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (270, 180) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (270,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (270,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n",
      "Train behavior per-axis std: [73.42055  67.324356]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"finger_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"finger_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b295e23",
   "metadata": {},
   "source": [
    "# Area2 Bump, Prepended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fa2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n",
      "Dataset already at 5 ms resolution, skipping resampling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'neural_input' with shape (272, 1253, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (272, 1253, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (272, 1253) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (272,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (272,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (92, 1772, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (92, 1772, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (92, 1772) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (92,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (92,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "  - Saved 'mask' with shape (272, 1253) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (272,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (272,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5\n",
      "  - Saved 'neural_input' with shape (92, 1772, 65) and dtype int8\n",
      "  - Saved 'behavior_input' with shape (92, 1772, 2) and dtype float32\n",
      "  - Saved 'mask' with shape (92, 1772) and dtype int8\n",
      "  - Saved 'dataset_group_idx' with shape (92,) and dtype int8\n",
      "  - Saved 'trial_idx' with shape (92,) and dtype int32\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5\n",
      "Train behavior per-axis std: [10.381516  9.213864]\n",
      "Train behavior per-axis std: [10.381516  9.213864]\n"
     ]
    }
   ],
   "source": [
    "prepend_duration = 299  # ms to prepend to each trial\n",
    "\n",
    "\n",
    "d = datasets[2]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "train_batch = []\n",
    "\n",
    "# Build train split, compute train std, then build val split using the train_std\n",
    "train_out = build_split_batch(nwb_dataset, 'train', prepend_duration=299, dataset_group_idx=9, bin_width=5, behavior_attribute=\"hand_vel\")\n",
    "train_std = train_out.get('behavior_std')\n",
    "# Remove behavior_std from the batch dict before saving\n",
    "train_batch = {k: v for k, v in train_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Build val batch using train statistics for normalization\n",
    "val_out = build_split_batch(nwb_dataset, 'val', prepend_duration=299, dataset_group_idx=9, bin_width=5, train_std=train_std, behavior_attribute=\"hand_vel\")\n",
    "val_batch = {k: v for k, v in val_out.items() if k != 'behavior_std'}\n",
    "\n",
    "# Persist\n",
    "dict_to_h5(train_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_train.h5')\n",
    "dict_to_h5(val_batch, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_area2bump_prepend_val.h5')\n",
    "\n",
    "# Optionally print the computed train std\n",
    "print('Train behavior per-axis std:', train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "60f08a59",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m beh = \u001b[43mtrain_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbehavior_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:, prepend_duration // bin_width + \u001b[32m1\u001b[39m:, :]\n\u001b[32m      2\u001b[39m mask = train_batch[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m][:, prepend_duration // bin_width + \u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m beh = np.array(beh.reshape(-\u001b[32m1\u001b[39m, beh.shape[-\u001b[32m1\u001b[39m])[mask])\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "beh = train_batch['behavior_input'][:, prepend_duration // bin_width + 1:, :]\n",
    "mask = train_batch['mask'][:, prepend_duration // bin_width + 1:].reshape(-1)\n",
    "beh = np.array(beh.reshape(-1, beh.shape[-1])[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "675aa73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8925f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
