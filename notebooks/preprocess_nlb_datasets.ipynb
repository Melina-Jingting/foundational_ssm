{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa3393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from foundational_ssm.collate import pad_collate\n",
    "\n",
    "\n",
    "dataset_folder = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/' \n",
    "\n",
    "datasets = [\n",
    "    {'name':'mc_maze', 'subpath':'./000128/sub-Jenkins/'},\n",
    "    {'name':'mc_rtt', 'subpath':'./000129/sub-Indy/'},\n",
    "    {'name':'area2_bump', 'subpath':'./000127/sub-Han/'},\n",
    "    {'name':'dmfc_rsg', 'subpath':'./000130/sub-Haydn/'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fc7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "processed_data_folder = os.path.join(dataset_folder, 'processed', 'nlb')\n",
    "processed_data_path = os.path.join(processed_data_folder, dataset_name + '.h5')\n",
    "trial_info_path = os.path.join(processed_data_folder, dataset_name + '.csv')\n",
    "\n",
    "if not os.path.exists(processed_data_folder):\n",
    "    print(f\"Creating directory: {processed_data_folder}\")\n",
    "    os.makedirs(processed_data_folder, exist_ok=True)\n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path, split_heldout=False)\n",
    "\n",
    "\n",
    "# Find when target pos changes\n",
    "has_change = nwb_dataset.data.target_pos.fillna(-1000).diff(axis=0).any(axis=1) # filling NaNs with arbitrary scalar to treat as one block\n",
    "# Find if target pos change corresponds to NaN-padded gap between files\n",
    "change_nan = nwb_dataset.data[has_change].isna().any(axis=1)\n",
    "# Drop trials containing the gap and immediately before and after, as those trials may be cut short\n",
    "drop_trial = (change_nan | change_nan.shift(1, fill_value=True) | change_nan.shift(-1, fill_value=True))[:-1]\n",
    "# Add start and end times to trial info\n",
    "change_times = nwb_dataset.data.index[has_change]\n",
    "start_times = change_times[:-1][~drop_trial]\n",
    "end_times = change_times[1:][~drop_trial]\n",
    "# Get target position per trial\n",
    "start_pos = nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy().tolist()\n",
    "target_pos = nwb_dataset.data.target_pos.loc[start_times].to_numpy().tolist()\n",
    "# Compute reach distance and angle\n",
    "reach_dist = nwb_dataset.data.target_pos.loc[end_times - pd.Timedelta(1, 'ms')].to_numpy() - nwb_dataset.data.target_pos.loc[start_times - pd.Timedelta(1, 'ms')].to_numpy()\n",
    "reach_angle = np.arctan2(reach_dist[:, 1], reach_dist[:, 0]) / np.pi * 180\n",
    "# Create trial info\n",
    "nwb_dataset.trial_info = pd.DataFrame({\n",
    "    'trial_id': np.arange(len(start_times)),\n",
    "    'start_time': start_times,\n",
    "    'end_time': end_times,\n",
    "    'duration': (end_times - start_times).total_seconds(),\n",
    "    'start_pos': start_pos,\n",
    "    'target_pos': target_pos,\n",
    "    'reach_dist_x': reach_dist[:, 0],\n",
    "    'reach_dist_y': reach_dist[:, 1],\n",
    "    'reach_angle': reach_angle,\n",
    "})\n",
    "nwb_dataset.resample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df43f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_dataset.trial_info.to_csv('/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17bca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys \n",
    "\n",
    "DTYPE_FLOAT = np.float32 \n",
    "def pad_collate(batch, fixed_seq_len=None):\n",
    "    # Assume batch is a list of dicts with keys: 'neural_input', 'behavior_input', etc.\n",
    "    # Each 'neural_input' is a tensor of shape (timesteps, units)\n",
    "    neural_inputs = [item['neural_input'] for item in batch if item is not None]  # (timesteps, units)\n",
    "    behavioral_inputs = [item['behavior_input'] for item in batch if item is not None]\n",
    "    \n",
    "    # Determine the fixed sequence length\n",
    "    if fixed_seq_len is None:\n",
    "        max_len = max(x.shape[0] for x in neural_inputs)\n",
    "    else:\n",
    "        max_len = fixed_seq_len\n",
    "\n",
    "    # Pad or truncate each sequence to fixed length\n",
    "    def pad_or_truncate(tensor, max_len):\n",
    "        seq_len = tensor.shape[0]\n",
    "        if seq_len == max_len:\n",
    "            return tensor\n",
    "        elif seq_len > max_len:\n",
    "            return tensor[:max_len]\n",
    "        else:\n",
    "            pad_shape = (max_len - seq_len,) + tensor.shape[1:]\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            return torch.cat([tensor, pad_tensor], dim=0)\n",
    "\n",
    "    padded_neural = torch.stack([pad_or_truncate(x, max_len) for x in neural_inputs if x is not None])  # (batch, max_len, units)\n",
    "    padded_behavior = torch.stack([pad_or_truncate(x, max_len) for x in behavioral_inputs if x is not None])\n",
    "\n",
    "    # Create mask: 1 for real data, 0 for padding\n",
    "    lengths = [x.shape[0] for x in neural_inputs]\n",
    "    mask = torch.zeros((len(batch), max_len), dtype=torch.bool)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, :min(l, max_len)] = 1\n",
    "\n",
    "    # Stack other fields (e.g., dataset_group_idx)\n",
    "    dataset_group_idx = torch.stack([item['dataset_group_idx'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'neural_input': padded_neural,\n",
    "        'behavior_input': padded_behavior,\n",
    "        'mask': mask,\n",
    "        'dataset_group_idx': dataset_group_idx,\n",
    "        # add other fields as needed\n",
    "    }\n",
    "    \n",
    "batch = []\n",
    "max_len = 0\n",
    "for i, row in nwb_dataset.trial_info.iterrows():\n",
    "    start = row['start_time']\n",
    "    end = row['end_time']\n",
    "    # Load data slices. This is where memory efficiency of nwb_dataset is key.\n",
    "    behavior_data = nwb_dataset.data.finger_vel[start:end].to_numpy()\n",
    "    neural_data = nwb_dataset.data.spikes[start:end].to_numpy(dtype=np.int8)\n",
    "    cursor_pos = nwb_dataset.data.cursor_pos[start:end].to_numpy()\n",
    "\n",
    "    # Convert to torch tensors with desired dtype immediately.\n",
    "    batch.append({\n",
    "        'behavior_input': torch.from_numpy(behavior_data),\n",
    "        'neural_input': torch.from_numpy(neural_data),\n",
    "        'cursor_pos': torch.from_numpy(cursor_pos),\n",
    "        'dataset_group_idx': torch.tensor(9, dtype=torch.int8) # Ensure it's a torch.Tensor\n",
    "    })\n",
    "    if max_len < behavior_data.shape[0]:\n",
    "        max_len = behavior_data.shape[0]\n",
    "    \n",
    "    # break\n",
    "batch = pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch \n",
    "def dict_to_h5(tensor_dict, output_h5_file):\n",
    "    with h5py.File(output_h5_file, 'w') as f:\n",
    "        # Iterate through the items in your 'batch' dictionary\n",
    "        for key, value in tensor_dict.items():\n",
    "            # Convert PyTorch tensor to NumPy array before saving\n",
    "            # Ensure data type consistency for saving\n",
    "            if torch.is_tensor(value):\n",
    "                # For boolean tensors, convert to int8 if you want them to take less space in HDF5\n",
    "                # (np.bool_ usually takes 1 byte, but some systems/HDF5 viewers prefer integer)\n",
    "                if value.dtype == torch.bool:\n",
    "                    data_to_save = value.to(torch.int8).numpy()\n",
    "                else:\n",
    "                    data_to_save = value.numpy()\n",
    "            else:\n",
    "                data_to_save = value # If any value isn't a tensor (e.g., a simple scalar or list)\n",
    "\n",
    "            f.create_dataset(key, data=data_to_save)\n",
    "            print(f\"  - Saved '{key}' with shape {data_to_save.shape} and dtype {data_to_save.dtype}\")\n",
    "\n",
    "        print(f\"Successfully saved all data from 'batch' to {output_h5_file}\")\n",
    "\n",
    "output_h5_file = '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_trialized.h5'\n",
    "dict_to_h5(batch, output_h5_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3668124",
   "metadata": {},
   "source": [
    "# Original Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Saved 'train_spikes_heldin' with shape (810, 120, 98) and dtype float16\n",
      "  - Saved 'train_spikes_heldout' with shape (810, 120, 32) and dtype float16\n",
      "  - Saved 'train_behavior' with shape (810, 120, 2) and dtype float64\n",
      "  - Saved 'train_spikes_heldin_forward' with shape (810, 40, 98) and dtype float16\n",
      "  - Saved 'train_spikes_heldout_forward' with shape (810, 40, 32) and dtype float16\n",
      "  - Saved 'neural_input' with shape (810, 120, 130) and dtype float16\n",
      "  - Saved 'behavior_input' with shape (810, 120, 2) and dtype float64\n",
      "  - Saved 'mask' with shape (810, 120) and dtype float64\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_not_trialized_train.h5\n",
      "  - Saved 'train_spikes_heldin' with shape (270, 120, 98) and dtype float16\n",
      "  - Saved 'train_spikes_heldout' with shape (270, 120, 32) and dtype float16\n",
      "  - Saved 'train_behavior' with shape (270, 120, 2) and dtype float64\n",
      "  - Saved 'train_spikes_heldin_forward' with shape (270, 40, 98) and dtype float16\n",
      "  - Saved 'train_spikes_heldout_forward' with shape (270, 40, 32) and dtype float16\n",
      "  - Saved 'neural_input' with shape (270, 120, 130) and dtype float16\n",
      "  - Saved 'behavior_input' with shape (270, 120, 2) and dtype float64\n",
      "  - Saved 'mask' with shape (270, 120) and dtype float64\n",
      "Successfully saved all data from 'batch' to /cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_not_trialized_val.h5\n"
     ]
    }
   ],
   "source": [
    "d = datasets[1]\n",
    "dataset_name = d['name']\n",
    "dataset_subpath = d['subpath']\n",
    "train_trial_split = ['train', 'val']\n",
    "\n",
    "raw_data_path = os.path.join(dataset_folder, 'raw', 'dandi', dataset_subpath) \n",
    "\n",
    "nwb_dataset = NWBDataset(raw_data_path)\n",
    "bin_width = 5\n",
    "nwb_dataset.resample(bin_width)\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'\n",
    "\n",
    "train_dataset_dict = make_train_input_tensors(nwb_dataset, dataset_name=dataset_name, trial_split='train', save_file=True, save_path=processed_data_path, include_forward_pred=True, include_behavior=True)\n",
    "val_dataset_dict = make_train_input_tensors(nwb_dataset, dataset_name=dataset_name, trial_split='val', save_file=True, save_path=processed_data_path, include_forward_pred=True, include_behavior=True)\n",
    "\n",
    "train_dataset_dict['neural_input'] = np.concatenate([train_dataset_dict['train_spikes_heldin'], train_dataset_dict['train_spikes_heldout']], axis=2)\n",
    "train_dataset_dict['behavior_input'] = train_dataset_dict['train_behavior']\n",
    "train_dataset_dict['mask'] = np.ones(train_dataset_dict['neural_input'].shape[:2])\n",
    "\n",
    "val_dataset_dict['neural_input'] = np.concatenate([val_dataset_dict['train_spikes_heldin'], val_dataset_dict['train_spikes_heldout']], axis=2)\n",
    "val_dataset_dict['behavior_input'] = val_dataset_dict['train_behavior']\n",
    "val_dataset_dict['mask'] = np.ones(val_dataset_dict['neural_input'].shape[:2])\n",
    "\n",
    "dict_to_h5(train_dataset_dict, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_not_trialized_train.h5')\n",
    "dict_to_h5(val_dataset_dict, \n",
    "           '/cs/student/projects1/ml/2024/mlaimon/data/foundational_ssm/processed/nlb/mc_rtt_not_trialized_val.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a18533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
